{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLQLStKwyzgs"
   },
   "source": [
    "# **METAMODELING WITH ARTIFICIAL NEURAL NETWORK**\n",
    "\n",
    "In this notebook, we will use the results of Abaqus analyses in order to build an Artificial Neural Network (ANN) of the Finite Element (FE) analysis solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Orwif6dIvF4t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bwj567NSvHv1",
    "outputId": "e91823c3-87c5-47fe-cca9-cd505741e7ff"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Matplotlib spec\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']}) # Palatino font\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jqtrSiFqdBTe",
    "outputId": "1cd691d5-fc75-4420-b6e5-d331a6ee9c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMkgB8U2dS1A"
   },
   "source": [
    "When this notebook has been generated the result of the previous line of code is: _'1.5.1'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZDSnO7jyzg8"
   },
   "source": [
    "We fix the seed in order to obtain reproducible results.\n",
    "\n",
    "__N.B.__ : Reproducible results are obtained every time the runtime is restarded and runned. If you run multiple time the same cell the results will not be reporducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SytMvTAE22lL"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed=seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Last two lines just when using GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxRgA_ioyzhA"
   },
   "source": [
    "## **Data preprocessing**\n",
    "\n",
    "We start by importing some information about the model used to generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "0FZ6R8aeyzhB",
    "outputId": "e6f59f12-68cb-4eb0-dd90-990424b0c8c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Radius</th>\n",
       "      <th>MaxCurvature</th>\n",
       "      <th>MeshSize</th>\n",
       "      <th>Plies</th>\n",
       "      <th>EffectivePlies</th>\n",
       "      <th>Symmetric</th>\n",
       "      <th>Balanced</th>\n",
       "      <th>AnglesFunction</th>\n",
       "      <th>LoadCase</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>705</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>harmlin</td>\n",
       "      <td>torsion</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Height  Radius  MaxCurvature  MeshSize  Plies  EffectivePlies  \\\n",
       "Value     705     300      0.001575        10      8               2   \n",
       "\n",
       "       Symmetric  Balanced AnglesFunction LoadCase  Train  Test  \n",
       "Value       True      True        harmlin  torsion    512   128  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify parameter to choose the output folder to consider\n",
    "load_case = 'torsion'\n",
    "stacking_sequence = 'symmetric_balanced'\n",
    "data_set = '8x'\n",
    "fiber_path = 'harmlin'\n",
    "\n",
    "# Check if notebook running in Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# Model info folder\n",
    "if is_colab:\n",
    "    input_folder = './'\n",
    "else:\n",
    "    input_folder = load_case + '/' + stacking_sequence + '/' + data_set + '/' + fiber_path + '/'\n",
    "\n",
    "info = pd.read_csv(input_folder + 'model_info.csv', sep=\",\")\n",
    "info.index = ['Value']\n",
    "eff_plies = int(info['EffectivePlies'].values)\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active sets: Train, Test\n"
     ]
    }
   ],
   "source": [
    "sets = ['Train', 'Val', 'Test']\n",
    "for set in sets:\n",
    "    if set not in info.keys():\n",
    "        sets.remove(set)\n",
    "print(\"Active sets: {}, {}\".format(*sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIBWzRwLyzhG"
   },
   "source": [
    "At this point we have to import the data set containing the input and output of the FE analysis. The data is stored in a dataframe in which the upper part is associated to the training set and the lower part to the test set. The precise number of upper row belonging to the train set is indicated in the info above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "niD2Q60oyzhG",
    "outputId": "6046f3fb-ed80-4099-aa94-7eb2dd87de49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude1</th>\n",
       "      <th>PhaseShift1</th>\n",
       "      <th>Omega1</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Amplitude2</th>\n",
       "      <th>PhaseShift2</th>\n",
       "      <th>Omega2</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Buckling</th>\n",
       "      <th>Stiffness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>6.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.633786</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>-1.716353</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>625.422031</td>\n",
       "      <td>6.900489e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>82.809519</td>\n",
       "      <td>52.005238</td>\n",
       "      <td>0.577781</td>\n",
       "      <td>51.996747</td>\n",
       "      <td>82.051847</td>\n",
       "      <td>52.004818</td>\n",
       "      <td>0.577854</td>\n",
       "      <td>52.003605</td>\n",
       "      <td>23019.537491</td>\n",
       "      <td>2.624252e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-195.753000</td>\n",
       "      <td>-89.808000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-89.749000</td>\n",
       "      <td>-199.306000</td>\n",
       "      <td>-89.694000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-89.677000</td>\n",
       "      <td>-37397.900000</td>\n",
       "      <td>1.580429e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-45.722000</td>\n",
       "      <td>-45.037500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-44.877250</td>\n",
       "      <td>-44.977000</td>\n",
       "      <td>-44.972000</td>\n",
       "      <td>0.499750</td>\n",
       "      <td>-44.976250</td>\n",
       "      <td>-21553.325000</td>\n",
       "      <td>4.951265e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>11844.100000</td>\n",
       "      <td>6.874971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.232750</td>\n",
       "      <td>44.813500</td>\n",
       "      <td>1.498250</td>\n",
       "      <td>44.808250</td>\n",
       "      <td>38.815750</td>\n",
       "      <td>44.992000</td>\n",
       "      <td>1.499500</td>\n",
       "      <td>44.966500</td>\n",
       "      <td>21768.725000</td>\n",
       "      <td>8.746724e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.303000</td>\n",
       "      <td>89.756000</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>89.750000</td>\n",
       "      <td>196.889000</td>\n",
       "      <td>89.809000</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>89.943000</td>\n",
       "      <td>36818.400000</td>\n",
       "      <td>1.244372e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amplitude1  PhaseShift1      Omega1       Beta1  Amplitude2  \\\n",
       "count  640.000000   640.000000  640.000000  640.000000  640.000000   \n",
       "mean    -0.633786     0.005536    1.000078   -0.002559   -1.716353   \n",
       "std     82.809519    52.005238    0.577781   51.996747   82.051847   \n",
       "min   -195.753000   -89.808000    0.004000  -89.749000 -199.306000   \n",
       "25%    -45.722000   -45.037500    0.500000  -44.877250  -44.977000   \n",
       "50%      0.191500     0.025000    0.998500    0.051000    0.036000   \n",
       "75%     41.232750    44.813500    1.498250   44.808250   38.815750   \n",
       "max    199.303000    89.756000    1.999000   89.750000  196.889000   \n",
       "\n",
       "       PhaseShift2      Omega2       Beta2      Buckling     Stiffness  \n",
       "count   640.000000  640.000000  640.000000    640.000000  6.400000e+02  \n",
       "mean      0.012386    0.999973    0.005630    625.422031  6.900489e+06  \n",
       "std      52.004818    0.577854   52.003605  23019.537491  2.624252e+06  \n",
       "min     -89.694000    0.001000  -89.677000 -37397.900000  1.580429e+06  \n",
       "25%     -44.972000    0.499750  -44.976250 -21553.325000  4.951265e+06  \n",
       "50%       0.070000    0.998500    0.029500  11844.100000  6.874971e+06  \n",
       "75%      44.992000    1.499500   44.966500  21768.725000  8.746724e+06  \n",
       "max      89.809000    1.999000   89.943000  36818.400000  1.244372e+07  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data sets\n",
    "data_orig = pd.read_csv(input_folder + '/data.csv', sep=',')\n",
    "data_orig.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9U4RPrnyzhP"
   },
   "source": [
    "The most important step to perform before training our model is the normalization of the variables. Different strategies are possible at this end, among which 2 are the most used:\n",
    "\n",
    "* Range normalization: converts all the values to the range $[0, 1]$\n",
    "\n",
    "* Standard score normalization: forces the variables to have $0$ mean and $1$ standard deviation\n",
    "\n",
    "We will try both to see the effect on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZVjkHziyzhQ"
   },
   "outputs": [],
   "source": [
    "def range_norm(x, x_min=None, x_max=None):\n",
    "    \"\"\" Normalization in range [0, 1] \"\"\"\n",
    "    if x_min is None and x_max is None:\n",
    "        x_min = np.min(x, axis=0)\n",
    "        x_max = np.max(x, axis=0)\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    return x_norm, x_min, x_max\n",
    "\n",
    "def std_norm(x, m=None, s=None):\n",
    "    \"\"\" Normalization with zero mean and unitary standard deviation \"\"\"\n",
    "    if m is None and s is None:\n",
    "        m = np.mean(x, axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "    x_norm = (x - m) / s\n",
    "    \n",
    "    return x_norm, m, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL3mX63XyzhT"
   },
   "source": [
    "Now we can split the data into training and test set. The two sets have been generate independently during the DOE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "uy149NwNyzhT",
    "outputId": "224f9a13-bed4-4c54-f05e-a4eee762b812"
   },
   "outputs": [],
   "source": [
    "X = data_orig.drop(['Buckling', 'Stiffness'], axis=1).values\n",
    "Y = data_orig[['Buckling','Stiffness']].values\n",
    "\n",
    "# Train set\n",
    "train_smp = int(info['Train'].values)\n",
    "_X_train = X[:train_smp, :]\n",
    "_Y_train = Y[:train_smp]\n",
    "last = np.copy(train_smp)\n",
    "\n",
    "# Validation set\n",
    "if 'Val' in sets:\n",
    "    val_smp = int(info['Val'].values)\n",
    "    _X_val = X[last:last+val_smp, :]\n",
    "    _Y_val = Y[last:last+val_smp]\n",
    "    last += val_smp\n",
    "\n",
    "# Test set\n",
    "if 'Test' in sets:\n",
    "    test_smp = int(info['Test'].values)\n",
    "    _X_test = X[last:last+test_smp, :]\n",
    "    _Y_test = Y[last:last+test_smp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If validation set is not present we will generate it from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Data sets info: \n",
      "\n",
      "X_train : (409, 8)  |  Y_train : (409, 2)\n",
      "X_val   : (103, 8)  |  Y_val   : (103, 2)\n",
      "X_test  : (128, 8)  |  Y_test  : (128, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_X_train, _X_val, _Y_train, _Y_val = train_test_split(_X_train, _Y_train, test_size=0.2, random_state=seed)\n",
    "print('            Data sets info: \\n')\n",
    "print(\"X_train : {}  |  Y_train : {}\".format(_X_train.shape, _Y_train.shape))\n",
    "print(\"X_val   : {}  |  Y_val   : {}\".format(_X_val.shape, _Y_val.shape))\n",
    "print(\"X_test  : {}  |  Y_test  : {} \\n\".format(_X_test.shape, _Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can generate the iterable data sets for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, Y):\n",
    "    \"\"\" Random shuffle of samples in X and y \"\"\"\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    return X[idx], Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "ORbqtnh-yzhR",
    "outputId": "d18315cf-cbc9-40f5-f987-26f6485bba5b"
   },
   "outputs": [],
   "source": [
    "# Normalization training set\n",
    "X_train, x_min, x_max = range_norm(_X_train)\n",
    "Y_train, y_min, y_max = range_norm(_Y_train)\n",
    "\n",
    "# Shuffle training set\n",
    "X_train, Y_train = shuffle_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save the normalization values since we will need them during the optimization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate how many free variables are in the model\n",
    "if info['AnglesFunction'].values == 'harmlin':\n",
    "    col_num = int(4 * info['EffectivePlies'].values)\n",
    "    X_bounds = pd.DataFrame({'x_min': x_min, 'x_max': x_max}, index=data_orig.columns[:col_num]).T\n",
    "    Y_bounds = pd.DataFrame({'y_min':y_min, 'y_max':y_max}, index=data_orig.columns[col_num:]).T\n",
    "    X_bounds.to_csv(input_folder + '/X_bounds.csv', index=True, float_format='%.3f')\n",
    "    Y_bounds.to_csv(input_folder + '/Y_bounds.csv', index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization validation set\n",
    "X_val, _, _ = range_norm(_X_val, x_min=x_min, x_max=x_max)\n",
    "Y_val, _, _ = range_norm(_Y_val, x_min=y_min, x_max=y_max)\n",
    "\n",
    "# Normalization testing set\n",
    "X_test, _, _ = range_norm(_X_test, x_min=x_min, x_max=x_max)\n",
    "Y_test, _, _ = range_norm(_Y_test, x_min=y_min, x_max=y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(_X, _Y, batch_size):\n",
    "    \"\"\" Split the data into k batches \"\"\"\n",
    "\n",
    "    n_samples = _X.shape[0]\n",
    "    leftovers = {}\n",
    "    n_leftovers = n_samples % batch_size\n",
    "    \n",
    "    # Case with all batches of equal size\n",
    "    if n_leftovers != 0:\n",
    "        leftovers[\"X\"] = _X[-n_leftovers:]\n",
    "        leftovers[\"Y\"] = _Y[-n_leftovers:]\n",
    "        _X = _X[:-n_leftovers]\n",
    "        _Y = _Y[:-n_leftovers]\n",
    "\n",
    "    k = np.int(_X.shape[0] / batch_size)\n",
    "        \n",
    "    X_split = np.split(_X, k)\n",
    "    Y_split = np.split(_Y, k)\n",
    "    \n",
    "    # Add leftover samples as last batch\n",
    "    if n_leftovers != 0:\n",
    "        X_split.append(leftovers[\"X\"])\n",
    "        Y_split.append(leftovers[\"Y\"])\n",
    "\n",
    "    return X_split, Y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "X_train_b, Y_train_b = create_batches(X_train, Y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches dimensions: \n",
      "\n",
      "Batch 0 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 1 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 2 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 3 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 4 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 5 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 6 : input (25, 8)  ,  output : (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Batches dimensions: \\n\")\n",
    "for i in range(len(X_train_b)):\n",
    "    print(\"Batch {} : input {}  ,  output : {}\".format(i, X_train_b[i].shape, Y_train_b[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Cyx2vNti_z4"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\" Stop network training if validation loss increases for a certain time.\n",
    "        The code is base on https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\"\"\"\n",
    "    def __init__(self, patience=30, path=input_folder + '/weights_NN'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        if self.val_loss_min > val_loss:\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7O7AIIoyzhU"
   },
   "source": [
    "## **Neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vyx3cytYfs7E"
   },
   "source": [
    "First define network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    \"\"\" Implementation of FeedForward Neural Network \"\"\"\n",
    "    def __init__(self, D_in, H, D_out, p):\n",
    "        super(FFNN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(H, D_in)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Second hidden layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Third hidden layer\n",
    "        #self.W_3 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        #self.b_3 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Output layer\n",
    "        self.W_4 = Parameter(init.xavier_normal_(torch.Tensor(D_out, H)))\n",
    "        self.b_4 = Parameter(init.constant_(torch.Tensor(D_out), 0))\n",
    "        \n",
    "        # define activation function in constructor\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        #self.activation_3 = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.activation_2(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = F.linear(x, self.W_3, self.b_3)\n",
    "        #x = self.activation_3(x)\n",
    "        #x = self.dropout(x)\n",
    "        pred = F.linear(x, self.W_4, self.b_4)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ibkHXkYKmXs6",
    "outputId": "3a2927df-9996-474a-eee5-3ec0d6f12263",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0000 | Train loss : 0.77409 | Val loss : 0.48309\n",
      "Iteration : 0100 | Train loss : 0.09403 | Val loss : 0.07564\n",
      "Iteration : 0200 | Train loss : 0.08073 | Val loss : 0.06375\n",
      "Iteration : 0300 | Train loss : 0.07158 | Val loss : 0.05997\n",
      "Iteration : 0400 | Train loss : 0.06552 | Val loss : 0.05783\n",
      "Iteration : 0500 | Train loss : 0.05637 | Val loss : 0.05731\n",
      "Iteration : 0600 | Train loss : 0.05793 | Val loss : 0.05658\n",
      "Iteration : 0700 | Train loss : 0.05307 | Val loss : 0.05611\n",
      "Iteration : 0800 | Train loss : 0.05198 | Val loss : 0.05488\n",
      "Iteration : 0900 | Train loss : 0.04742 | Val loss : 0.05368\n",
      "Iteration : 1000 | Train loss : 0.04548 | Val loss : 0.05465\n",
      "Iteration : 1100 | Train loss : 0.04343 | Val loss : 0.05267\n",
      "Iteration : 1200 | Train loss : 0.04143 | Val loss : 0.05041\n",
      "Iteration : 1300 | Train loss : 0.04515 | Val loss : 0.05110\n",
      "Iteration : 1400 | Train loss : 0.03964 | Val loss : 0.05131\n",
      "Iteration : 1500 | Train loss : 0.04262 | Val loss : 0.05284\n",
      "Iteration : 1600 | Train loss : 0.03987 | Val loss : 0.05193\n",
      "Iteration : 1700 | Train loss : 0.04090 | Val loss : 0.05003\n",
      "Iteration : 1800 | Train loss : 0.03857 | Val loss : 0.05084\n",
      "Iteration : 1900 | Train loss : 0.03862 | Val loss : 0.04946\n",
      "Iteration : 2000 | Train loss : 0.03841 | Val loss : 0.04789\n",
      "Iteration : 2100 | Train loss : 0.03550 | Val loss : 0.04856\n",
      "Iteration : 2200 | Train loss : 0.03718 | Val loss : 0.04753\n",
      "Iteration : 2300 | Train loss : 0.03560 | Val loss : 0.04804\n",
      "Iteration : 2400 | Train loss : 0.03340 | Val loss : 0.04702\n",
      "Iteration : 2500 | Train loss : 0.03666 | Val loss : 0.04731\n",
      "Iteration : 2600 | Train loss : 0.03136 | Val loss : 0.04476\n",
      "Iteration : 2700 | Train loss : 0.03542 | Val loss : 0.04567\n",
      "Iteration : 2800 | Train loss : 0.03252 | Val loss : 0.04826\n",
      "Iteration : 2900 | Train loss : 0.03465 | Val loss : 0.04530\n",
      "Iteration : 3000 | Train loss : 0.03415 | Val loss : 0.04473\n",
      "Iteration : 3100 | Train loss : 0.03136 | Val loss : 0.04465\n",
      "Iteration : 3200 | Train loss : 0.03094 | Val loss : 0.04126\n",
      "Iteration : 3300 | Train loss : 0.03072 | Val loss : 0.04281\n",
      "Iteration : 3400 | Train loss : 0.03137 | Val loss : 0.04403\n",
      "Iteration : 3500 | Train loss : 0.02855 | Val loss : 0.04137\n",
      "Iteration : 3600 | Train loss : 0.03061 | Val loss : 0.04064\n",
      "Iteration : 3700 | Train loss : 0.02935 | Val loss : 0.04143\n",
      "Iteration : 3800 | Train loss : 0.02780 | Val loss : 0.04182\n",
      "Iteration : 3900 | Train loss : 0.02845 | Val loss : 0.04392\n",
      "Iteration : 4000 | Train loss : 0.03064 | Val loss : 0.03965\n",
      "Iteration : 4100 | Train loss : 0.03006 | Val loss : 0.04243\n",
      "Iteration : 4200 | Train loss : 0.02925 | Val loss : 0.04013\n",
      "Iteration : 4300 | Train loss : 0.03154 | Val loss : 0.04152\n",
      "Iteration : 4400 | Train loss : 0.02922 | Val loss : 0.04055\n",
      "Iteration : 4500 | Train loss : 0.02751 | Val loss : 0.04215\n",
      "Iteration : 4600 | Train loss : 0.02758 | Val loss : 0.03842\n",
      "Iteration : 4700 | Train loss : 0.02670 | Val loss : 0.03803\n",
      "Iteration : 4800 | Train loss : 0.03263 | Val loss : 0.03843\n",
      "Iteration : 4900 | Train loss : 0.02751 | Val loss : 0.03788\n",
      "Iteration : 5000 | Train loss : 0.02822 | Val loss : 0.03725\n",
      "Iteration : 5100 | Train loss : 0.02742 | Val loss : 0.03719\n",
      "Iteration : 5200 | Train loss : 0.02755 | Val loss : 0.03702\n",
      "Iteration : 5300 | Train loss : 0.02843 | Val loss : 0.03794\n",
      "Iteration : 5400 | Train loss : 0.03163 | Val loss : 0.03746\n",
      "Iteration : 5500 | Train loss : 0.02464 | Val loss : 0.03829\n",
      "Iteration : 5600 | Train loss : 0.02671 | Val loss : 0.03669\n",
      "Iteration : 5700 | Train loss : 0.02670 | Val loss : 0.03705\n",
      "Iteration : 5800 | Train loss : 0.03196 | Val loss : 0.03726\n",
      "Iteration : 5900 | Train loss : 0.02674 | Val loss : 0.03595\n",
      "Iteration : 6000 | Train loss : 0.02616 | Val loss : 0.03719\n",
      "Iteration : 6100 | Train loss : 0.02759 | Val loss : 0.03545\n",
      "Iteration : 6200 | Train loss : 0.02811 | Val loss : 0.03697\n",
      "Iteration : 6300 | Train loss : 0.02822 | Val loss : 0.03824\n",
      "Iteration : 6400 | Train loss : 0.02822 | Val loss : 0.03759\n",
      "Iteration : 6500 | Train loss : 0.02816 | Val loss : 0.03661\n",
      "Iteration : 6600 | Train loss : 0.02687 | Val loss : 0.03575\n",
      "Iteration : 6700 | Train loss : 0.02943 | Val loss : 0.03583\n",
      "Iteration : 6800 | Train loss : 0.02759 | Val loss : 0.03385\n",
      "Iteration : 6900 | Train loss : 0.02762 | Val loss : 0.03535\n",
      "Iteration : 7000 | Train loss : 0.02789 | Val loss : 0.03405\n",
      "Iteration : 7100 | Train loss : 0.02685 | Val loss : 0.03433\n",
      "Iteration : 7200 | Train loss : 0.02762 | Val loss : 0.03315\n",
      "Iteration : 7300 | Train loss : 0.02748 | Val loss : 0.03349\n",
      "Iteration : 7400 | Train loss : 0.02501 | Val loss : 0.03344\n",
      "Iteration : 7500 | Train loss : 0.02848 | Val loss : 0.03334\n",
      "Iteration : 7600 | Train loss : 0.02531 | Val loss : 0.03189\n",
      "Iteration : 7700 | Train loss : 0.02661 | Val loss : 0.03340\n",
      "Iteration : 7800 | Train loss : 0.02503 | Val loss : 0.03220\n",
      "Iteration : 7900 | Train loss : 0.02762 | Val loss : 0.03168\n",
      "Iteration : 8000 | Train loss : 0.02699 | Val loss : 0.03429\n",
      "Iteration : 8100 | Train loss : 0.02546 | Val loss : 0.03200\n",
      "Iteration : 8200 | Train loss : 0.02369 | Val loss : 0.03164\n",
      "Iteration : 8300 | Train loss : 0.02628 | Val loss : 0.03093\n",
      "Iteration : 8400 | Train loss : 0.02602 | Val loss : 0.03091\n",
      "Iteration : 8500 | Train loss : 0.02555 | Val loss : 0.03066\n",
      "Iteration : 8600 | Train loss : 0.02815 | Val loss : 0.03162\n",
      "Iteration : 8700 | Train loss : 0.02458 | Val loss : 0.02962\n",
      "Iteration : 8800 | Train loss : 0.02497 | Val loss : 0.02863\n",
      "Iteration : 8900 | Train loss : 0.02371 | Val loss : 0.03093\n",
      "Iteration : 9000 | Train loss : 0.02694 | Val loss : 0.03080\n",
      "Iteration : 9100 | Train loss : 0.02649 | Val loss : 0.02909\n",
      "Iteration : 9200 | Train loss : 0.02491 | Val loss : 0.02933\n",
      "Iteration : 9300 | Train loss : 0.02695 | Val loss : 0.03043\n",
      "Iteration : 9400 | Train loss : 0.03033 | Val loss : 0.02952\n",
      "Iteration : 9500 | Train loss : 0.02510 | Val loss : 0.02950\n",
      "Iteration : 9600 | Train loss : 0.02583 | Val loss : 0.03047\n",
      "Iteration : 9700 | Train loss : 0.02554 | Val loss : 0.02965\n",
      "Iteration : 9800 | Train loss : 0.02426 | Val loss : 0.02916\n",
      "Iteration : 9900 | Train loss : 0.02395 | Val loss : 0.02919\n",
      "Iteration : 10000 | Train loss : 0.02661 | Val loss : 0.02989\n",
      "Iteration : 10100 | Train loss : 0.02518 | Val loss : 0.02983\n",
      "Iteration : 10200 | Train loss : 0.02503 | Val loss : 0.02901\n",
      "Iteration : 10300 | Train loss : 0.02593 | Val loss : 0.02857\n",
      "Iteration : 10400 | Train loss : 0.02649 | Val loss : 0.02845\n",
      "Iteration : 10500 | Train loss : 0.02419 | Val loss : 0.02809\n",
      "Iteration : 10600 | Train loss : 0.02332 | Val loss : 0.02721\n",
      "Iteration : 10700 | Train loss : 0.02399 | Val loss : 0.02903\n",
      "Iteration : 10800 | Train loss : 0.02378 | Val loss : 0.02801\n",
      "Iteration : 10900 | Train loss : 0.02473 | Val loss : 0.02877\n",
      "Iteration : 11000 | Train loss : 0.02851 | Val loss : 0.02913\n",
      "Iteration : 11100 | Train loss : 0.02408 | Val loss : 0.02724\n",
      "Iteration : 11200 | Train loss : 0.02626 | Val loss : 0.02844\n",
      "Iteration : 11300 | Train loss : 0.02392 | Val loss : 0.02695\n",
      "Iteration : 11400 | Train loss : 0.02585 | Val loss : 0.02684\n",
      "Iteration : 11500 | Train loss : 0.02547 | Val loss : 0.02752\n",
      "Iteration : 11600 | Train loss : 0.02460 | Val loss : 0.02807\n",
      "Iteration : 11700 | Train loss : 0.02625 | Val loss : 0.02735\n",
      "Iteration : 11800 | Train loss : 0.02553 | Val loss : 0.02741\n",
      "Iteration : 11900 | Train loss : 0.02503 | Val loss : 0.02602\n",
      "Iteration : 12000 | Train loss : 0.02593 | Val loss : 0.02624\n",
      "Iteration : 12100 | Train loss : 0.02723 | Val loss : 0.02623\n",
      "Iteration : 12200 | Train loss : 0.02612 | Val loss : 0.02693\n",
      "Iteration : 12300 | Train loss : 0.02694 | Val loss : 0.02810\n",
      "Iteration : 12400 | Train loss : 0.02497 | Val loss : 0.02755\n",
      "Iteration : 12500 | Train loss : 0.02521 | Val loss : 0.02828\n",
      "Iteration : 12600 | Train loss : 0.02433 | Val loss : 0.02700\n",
      "Iteration : 12700 | Train loss : 0.02416 | Val loss : 0.02761\n",
      "Iteration : 12800 | Train loss : 0.02430 | Val loss : 0.02612\n",
      "Iteration : 12900 | Train loss : 0.02487 | Val loss : 0.02761\n",
      "Iteration : 13000 | Train loss : 0.02496 | Val loss : 0.02655\n",
      "Iteration : 13100 | Train loss : 0.02427 | Val loss : 0.02730\n",
      "Iteration : 13200 | Train loss : 0.02517 | Val loss : 0.02711\n",
      "Iteration : 13300 | Train loss : 0.02339 | Val loss : 0.02578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 13400 | Train loss : 0.02559 | Val loss : 0.02626\n",
      "Iteration : 13500 | Train loss : 0.02383 | Val loss : 0.02801\n",
      "Iteration : 13600 | Train loss : 0.02307 | Val loss : 0.02566\n",
      "Iteration : 13700 | Train loss : 0.02491 | Val loss : 0.02622\n",
      "Iteration : 13800 | Train loss : 0.02230 | Val loss : 0.02616\n",
      "Iteration : 13900 | Train loss : 0.02707 | Val loss : 0.02450\n",
      "Iteration : 14000 | Train loss : 0.02323 | Val loss : 0.02620\n",
      "Iteration : 14100 | Train loss : 0.02252 | Val loss : 0.02475\n",
      "Iteration : 14200 | Train loss : 0.02422 | Val loss : 0.02740\n",
      "Iteration : 14300 | Train loss : 0.02430 | Val loss : 0.02622\n",
      "Iteration : 14400 | Train loss : 0.02543 | Val loss : 0.02655\n",
      "Iteration : 14500 | Train loss : 0.02447 | Val loss : 0.02568\n",
      "Iteration : 14600 | Train loss : 0.02557 | Val loss : 0.02714\n",
      "Iteration : 14700 | Train loss : 0.02478 | Val loss : 0.02680\n",
      "Iteration : 14800 | Train loss : 0.02473 | Val loss : 0.02658\n",
      "Iteration : 14900 | Train loss : 0.02333 | Val loss : 0.02660\n",
      "Iteration : 15000 | Train loss : 0.02295 | Val loss : 0.02643\n",
      "Iteration : 15100 | Train loss : 0.02549 | Val loss : 0.02543\n",
      "Iteration : 15200 | Train loss : 0.02522 | Val loss : 0.02590\n",
      "Iteration : 15300 | Train loss : 0.02595 | Val loss : 0.02562\n",
      "Iteration : 15400 | Train loss : 0.02523 | Val loss : 0.02619\n",
      "Iteration : 15500 | Train loss : 0.02454 | Val loss : 0.02810\n",
      "Iteration : 15600 | Train loss : 0.02564 | Val loss : 0.02471\n",
      "Iteration : 15700 | Train loss : 0.02470 | Val loss : 0.02624\n",
      "Iteration : 15800 | Train loss : 0.02624 | Val loss : 0.02631\n",
      "Iteration : 15900 | Train loss : 0.02427 | Val loss : 0.02724\n",
      "Iteration : 16000 | Train loss : 0.02341 | Val loss : 0.02717\n",
      "Iteration : 16100 | Train loss : 0.02366 | Val loss : 0.02710\n",
      "Iteration : 16200 | Train loss : 0.02464 | Val loss : 0.02605\n",
      "Iteration : 16300 | Train loss : 0.02536 | Val loss : 0.02635\n",
      "Iteration : 16400 | Train loss : 0.02358 | Val loss : 0.02503\n",
      "Iteration : 16500 | Train loss : 0.02375 | Val loss : 0.02615\n",
      "Iteration : 16600 | Train loss : 0.02300 | Val loss : 0.02629\n",
      "Iteration : 16700 | Train loss : 0.02522 | Val loss : 0.02573\n",
      "Iteration : 16800 | Train loss : 0.02353 | Val loss : 0.02703\n",
      "Iteration : 16900 | Train loss : 0.02553 | Val loss : 0.02611\n",
      "Iteration : 17000 | Train loss : 0.02362 | Val loss : 0.02484\n",
      "Iteration : 17100 | Train loss : 0.02407 | Val loss : 0.02572\n",
      "Iteration : 17200 | Train loss : 0.02463 | Val loss : 0.02594\n",
      "Iteration : 17300 | Train loss : 0.02373 | Val loss : 0.02538\n",
      "Iteration : 17400 | Train loss : 0.02380 | Val loss : 0.02564\n",
      "Iteration : 17500 | Train loss : 0.02626 | Val loss : 0.02655\n",
      "Iteration : 17600 | Train loss : 0.02295 | Val loss : 0.02765\n",
      "Iteration : 17700 | Train loss : 0.02308 | Val loss : 0.02728\n",
      "Iteration : 17800 | Train loss : 0.02421 | Val loss : 0.02604\n",
      "Iteration : 17900 | Train loss : 0.02461 | Val loss : 0.02607\n",
      "Iteration : 18000 | Train loss : 0.02331 | Val loss : 0.02541\n",
      "Iteration : 18100 | Train loss : 0.02557 | Val loss : 0.02620\n",
      "Iteration : 18200 | Train loss : 0.02335 | Val loss : 0.02527\n",
      "Iteration : 18300 | Train loss : 0.02137 | Val loss : 0.02795\n",
      "Iteration : 18400 | Train loss : 0.02245 | Val loss : 0.02594\n",
      "Iteration : 18500 | Train loss : 0.02755 | Val loss : 0.02557\n",
      "Iteration : 18600 | Train loss : 0.02499 | Val loss : 0.02498\n",
      "Iteration : 18700 | Train loss : 0.02374 | Val loss : 0.02573\n",
      "Iteration : 18800 | Train loss : 0.02632 | Val loss : 0.02637\n",
      "Iteration : 18900 | Train loss : 0.02452 | Val loss : 0.02502\n",
      "Iteration : 19000 | Train loss : 0.02577 | Val loss : 0.02628\n",
      "Iteration : 19100 | Train loss : 0.02444 | Val loss : 0.02670\n",
      "Iteration : 19200 | Train loss : 0.02574 | Val loss : 0.02530\n",
      "Iteration : 19300 | Train loss : 0.02390 | Val loss : 0.02585\n",
      "Iteration : 19400 | Train loss : 0.02241 | Val loss : 0.02678\n",
      "Iteration : 19500 | Train loss : 0.02232 | Val loss : 0.02526\n",
      "Iteration : 19600 | Train loss : 0.02246 | Val loss : 0.02536\n",
      "Iteration : 19700 | Train loss : 0.02354 | Val loss : 0.02703\n",
      "Iteration : 19800 | Train loss : 0.02515 | Val loss : 0.02597\n",
      "Iteration : 19900 | Train loss : 0.02302 | Val loss : 0.02535\n",
      "Iteration : 20000 | Train loss : 0.02302 | Val loss : 0.02523\n",
      "Iteration : 20100 | Train loss : 0.02390 | Val loss : 0.02751\n",
      "Iteration : 20200 | Train loss : 0.02316 | Val loss : 0.02626\n",
      "Iteration : 20300 | Train loss : 0.02284 | Val loss : 0.02573\n",
      "Iteration : 20400 | Train loss : 0.02310 | Val loss : 0.02630\n",
      "Iteration : 20500 | Train loss : 0.02384 | Val loss : 0.02580\n",
      "Iteration : 20600 | Train loss : 0.02381 | Val loss : 0.02566\n",
      "Iteration : 20700 | Train loss : 0.02476 | Val loss : 0.02640\n",
      "Iteration : 20800 | Train loss : 0.02427 | Val loss : 0.02485\n",
      "Iteration : 20900 | Train loss : 0.02296 | Val loss : 0.02546\n",
      "Iteration : 21000 | Train loss : 0.02380 | Val loss : 0.02550\n",
      "Iteration : 21100 | Train loss : 0.02397 | Val loss : 0.02453\n",
      "Iteration : 21200 | Train loss : 0.02334 | Val loss : 0.02685\n",
      "Iteration : 21300 | Train loss : 0.02476 | Val loss : 0.02556\n",
      "Iteration : 21400 | Train loss : 0.02401 | Val loss : 0.02584\n",
      "Iteration : 21500 | Train loss : 0.02567 | Val loss : 0.02627\n",
      "Iteration : 21600 | Train loss : 0.02416 | Val loss : 0.02682\n",
      "Iteration : 21700 | Train loss : 0.02265 | Val loss : 0.02517\n",
      "Iteration : 21800 | Train loss : 0.02371 | Val loss : 0.02573\n",
      "Iteration : 21900 | Train loss : 0.02450 | Val loss : 0.02593\n",
      "Iteration : 22000 | Train loss : 0.02071 | Val loss : 0.02642\n",
      "Iteration : 22100 | Train loss : 0.02281 | Val loss : 0.02644\n",
      "Iteration : 22200 | Train loss : 0.02291 | Val loss : 0.02658\n",
      "Iteration : 22300 | Train loss : 0.02270 | Val loss : 0.02517\n",
      "Iteration : 22400 | Train loss : 0.02306 | Val loss : 0.02682\n",
      "Iteration : 22500 | Train loss : 0.02394 | Val loss : 0.02594\n",
      "Iteration : 22600 | Train loss : 0.02329 | Val loss : 0.02512\n",
      "Iteration : 22700 | Train loss : 0.02451 | Val loss : 0.02645\n",
      "Iteration : 22800 | Train loss : 0.02296 | Val loss : 0.02516\n",
      "Iteration : 22900 | Train loss : 0.02427 | Val loss : 0.02592\n",
      "Iteration : 23000 | Train loss : 0.02346 | Val loss : 0.02615\n",
      "Iteration : 23100 | Train loss : 0.02374 | Val loss : 0.02728\n",
      "Iteration : 23200 | Train loss : 0.02462 | Val loss : 0.02808\n",
      "\n",
      "Early stopping - Minimum loss : 0.02312\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[1]\n",
    "n_y = Y_train.shape[1]\n",
    "D_in, H, D_out = n_x, 64, n_y\n",
    "\n",
    "epochs = 50000\n",
    "lr = 1e-3\n",
    "weight_decay = 0.\n",
    "lambda_1 = 1e-3\n",
    "lambda_2 = 1e-3\n",
    "p = 0.2\n",
    "patience = 3000\n",
    "\n",
    "model = FFNN(D_in, H, D_out, p)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "# Flag True if you are in the network optimization process.\n",
    "is_optimizing = True\n",
    "\n",
    "# If weight_NN exists and we are not in optimization mode just load\n",
    "# network weights and evaluate the model.\n",
    "if(os.path.isfile(input_folder + '/weights_NN') and is_optimizing==False):\n",
    "    model.load_state_dict(torch.load(input_folder + '/weights_NN'))\n",
    "    print(model.eval())\n",
    "else:\n",
    "    criterion = nn.MSELoss(reduction='mean') \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, amsgrad=True)\n",
    "    \n",
    "    # Initialize losses lists.\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    losses = []\n",
    "    W_1_hist = []\n",
    "    W_2_hist = []\n",
    "    W_3_hist = []\n",
    "    \n",
    "    idx = np.arange(len(X_train_b))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        curr_loss = 0\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        model.train()\n",
    "        for batch_num in idx:\n",
    "            \n",
    "            # Create torch variables, required dtype 'float32' no 'float64'\n",
    "            batch_x = Variable(torch.from_numpy(X_train_b[batch_num].astype('float32')))\n",
    "            batch_y = Variable(torch.from_numpy(Y_train_b[batch_num].astype('float32')))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(batch_x)\n",
    "            \n",
    "            # L1 and L2 regularization\n",
    "            l1_reg = None\n",
    "            l2_reg = None\n",
    "            for W in model.parameters():\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = W.norm(1)\n",
    "                    l2_reg = 0.5* W.norm(2) ** 2\n",
    "                else:\n",
    "                    l1_reg = l1_reg + W.norm(1)\n",
    "                    l2_reg = l2_reg + 0.5 * W.norm(2) ** 2\n",
    "                \n",
    "            # Compute loss\n",
    "            batch_loss = 1 / batch_x.shape[0] * ((y_pred - batch_y).pow(2).sum() + l1_reg * lambda_1 + l2_reg * lambda_2)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += batch_loss\n",
    "        train_loss.append(curr_loss.item() / len(idx))\n",
    "        \n",
    "        # Save norm of the weights\n",
    "        W_1_hist.append(np.linalg.norm(model.W_1.detach().numpy()))\n",
    "        W_2_hist.append(np.linalg.norm(model.W_2.detach().numpy()))\n",
    "        #W_3_hist.append(np.linalg.norm(model.W_3.detach().numpy()))\n",
    "        \n",
    "        model.eval()\n",
    "        val_x = Variable(torch.from_numpy(X_val.astype('float32')))\n",
    "        val_y = Variable(torch.from_numpy(Y_val.astype('float32')))\n",
    "        val_pred = model(val_x)\n",
    "        loss_val = 1 / val_x.shape[0] * ((val_y - val_pred).pow(2).sum() + l1_reg * lambda_1 + l2_reg * lambda_2)\n",
    "            \n",
    "        val_loss.append(loss_val.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Iteration : {:04d} | Train loss : {:.5f} | Val loss : {:.5f}\".format(epoch, train_loss[epoch], val_loss[epoch]))\n",
    "        \n",
    "        early_stopping(val_loss[epoch], model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"\\nEarly stopping - Minimum loss : {:.5f}\".format(early_stopping.val_loss_min))\n",
    "            break\n",
    "    \n",
    "    if not early_stopping.early_stop:\n",
    "        torch.save(model.state_dict(), input_folder + '/weights_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Epoch'), Text(0, 0.5, 'Weights norm'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.arange(len(W_1_hist)), W_1_hist, 'r',\n",
    "         np.arange(len(W_2_hist)), W_2_hist, 'b',\n",
    "         np.arange(len(W_3_hist)), W_3_hist, 'k')\n",
    "plt.legend(['W_1','W_2', 'W_3'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Weights norm')\n",
    "#plt.ylim(-.0, .06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Epoch'), Text(0, 0.5, 'Loss'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, 'r', np.arange(len(val_loss)), val_loss, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "#plt.ylim(-.0,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to go back from normalization in order to have a better feeling of the results and avoid division for very low numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_range_norm(x_norm, x_min, x_max):\n",
    "    \"\"\" Inverse of normalization in range [0, 1] \"\"\"\n",
    "    x = x_min +  x_norm * (x_max - x_min)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of accuracy\n",
    "\n",
    "We need to define some metrics to evaluate the goodness of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pe(pred, true):\n",
    "    \"\"\" Calculate percent error\"\"\"\n",
    "    return abs((pred - true) / true) * 100\n",
    "\n",
    "def r2(pred, true):\n",
    "    \"\"\" Calculate the coefficient of determination, i.e. R^2\"\"\"\n",
    "    # https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "    ss_tot = np.sum((true - np.mean(true, axis=0)) ** 2, axis=0)  \n",
    "    ss_res = np.sum((true - pred) ** 2, axis=0)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "def raae(pred, true):\n",
    "    \"\"\" Calculate the relative averaged absolute error\"\"\"\n",
    "    sigma = np.std(true, axis=0)\n",
    "    num = np.sum(abs(true - pred), axis=0)\n",
    "    return num / (true.shape[0] * sigma)\n",
    "\n",
    "def rmae(pred, true):\n",
    "    \"\"\" Calculate the relative maximum absolute error\"\"\"\n",
    "    sigma = np.std(true, axis=0)\n",
    "    num = np.max(abs(pred - true), axis=0)\n",
    "    return num / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the network's weights saved. This is required since, if the training is stopped by the early stopping class, the actual weights are associated to the overfitted network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(input_folder + '/weights_NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 13.30%\n",
      "\n",
      "The coefficient of determination is: 0.934\n",
      "\n",
      "The relative averaged absolute error is: 0.176\n",
      "\n",
      "The relative maximum absolute error is: 1.009\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "model.eval()\n",
    "train_x = Variable(torch.from_numpy(X_train.astype('float32')))\n",
    "train_y = Variable(torch.from_numpy(Y_train.astype('float32')))\n",
    "\n",
    "train_pred = model(train_x)\n",
    "\n",
    "train_y = reverse_range_norm(train_y.detach().numpy(), y_min, y_max)\n",
    "train_pred = reverse_range_norm(train_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "err_perc = pe(train_pred, train_y)\n",
    "r_2 = r2(train_pred, train_y)\n",
    "raae_score = raae(train_pred, train_y)\n",
    "rmae_score = rmae(train_pred, train_y)\n",
    "\n",
    "if flag_list:\n",
    "    for i in range(train_y.shape[0]):\n",
    "        print(\"True : {:+010.2f}, {:+013.2f}  |  Prediction : {:+010.2f}, {:+013.2f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "              format(*train_y[i, :], *train_pred[i, :], *err_perc[i, :]))\n",
    "    \n",
    "print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 17.74%\n",
      "\n",
      "The coefficient of determination is: 0.849\n",
      "\n",
      "The relative averaged absolute error is: 0.263\n",
      "\n",
      "The relative maximum absolute error is: 1.437\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "model.eval()\n",
    "val_x = Variable(torch.from_numpy(X_val.astype('float32')))\n",
    "val_y = Variable(torch.from_numpy(Y_val.astype('float32')))\n",
    "\n",
    "val_pred = model(val_x)\n",
    "\n",
    "val_y = reverse_range_norm(val_y.detach().numpy(), y_min, y_max)\n",
    "val_pred = reverse_range_norm(val_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "err_perc = pe(val_pred, val_y)\n",
    "r_2 = r2(val_pred, val_y)\n",
    "raae_score = raae(val_pred, val_y)\n",
    "rmae_score = rmae(val_pred, val_y)\n",
    "    \n",
    "if flag_list:\n",
    "    for i in range(val_y.shape[0]):\n",
    "        print(\"True : {:+010.2f}, {:+013.2f}  |  Prediction : {:+010.2f}, {:+013.2f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "              format(*val_y[i, :], *val_pred[i, :], *err_perc[i, :]))\n",
    "print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.10745195e+04,  1.02491841e+07],\n",
       "       [ 1.47482685e+04,  6.02893727e+06],\n",
       "       [-2.02699113e+04,  5.90630742e+06],\n",
       "       [-1.46408796e+04,  7.46748831e+06],\n",
       "       [ 1.43751348e+04,  6.84606530e+06],\n",
       "       [-1.88639249e+04,  5.80937825e+06],\n",
       "       [ 1.93901758e+04,  7.06653601e+06],\n",
       "       [-1.75381224e+04,  6.09729669e+06],\n",
       "       [ 3.02620669e+04,  7.62876045e+06],\n",
       "       [-2.29853272e+04,  7.90409987e+06],\n",
       "       [-2.67643047e+04,  7.82701341e+06],\n",
       "       [-2.82876148e+04,  5.35188085e+06],\n",
       "       [ 1.90749652e+04,  6.48099953e+06],\n",
       "       [ 1.29674365e+04,  6.15040808e+06],\n",
       "       [-1.53037482e+04,  8.53720447e+06],\n",
       "       [-5.21561046e+03,  5.68647451e+06],\n",
       "       [ 2.52804863e+04,  1.07758212e+07],\n",
       "       [ 2.27468044e+04,  6.35759398e+06],\n",
       "       [-2.02036154e+04,  1.03510376e+07],\n",
       "       [-1.35893901e+04,  6.92442217e+06],\n",
       "       [ 3.02762844e+04,  5.60993033e+06],\n",
       "       [-2.54327006e+04,  1.10213736e+07],\n",
       "       [-2.58664238e+04,  7.00709269e+06],\n",
       "       [-1.41501900e+04,  3.83352561e+06],\n",
       "       [-1.74704452e+04,  1.07490120e+07],\n",
       "       [ 2.45523292e+04,  8.70671543e+06],\n",
       "       [-1.75744007e+04,  8.01649141e+06],\n",
       "       [-1.95911054e+04,  6.82702614e+06],\n",
       "       [ 2.59637788e+04,  5.42211512e+06],\n",
       "       [-1.91833523e+04,  6.86505882e+06],\n",
       "       [ 3.06280077e+04,  7.58729375e+06],\n",
       "       [ 2.20535100e+04,  7.61267326e+06],\n",
       "       [ 2.22216569e+04,  9.53532499e+06],\n",
       "       [-2.44933857e+04,  1.05448972e+07],\n",
       "       [ 2.34244789e+04,  7.36437484e+06],\n",
       "       [-1.75487878e+04,  6.57572558e+06],\n",
       "       [-2.01677308e+04,  1.06469592e+07],\n",
       "       [ 3.13439290e+04,  6.13277044e+06],\n",
       "       [-2.00135826e+04,  6.93443320e+06],\n",
       "       [-2.50957943e+04,  9.64661447e+06],\n",
       "       [-1.86480614e+04,  1.01268489e+07],\n",
       "       [ 2.98338943e+04,  6.89880794e+06],\n",
       "       [-2.43931063e+04,  7.49327186e+06],\n",
       "       [-1.82855002e+04,  8.02577076e+06],\n",
       "       [ 2.98424142e+04,  7.96254020e+06],\n",
       "       [ 1.09251056e+04,  5.29675508e+06],\n",
       "       [-1.88026675e+04,  7.06513934e+06],\n",
       "       [-1.37685584e+04,  3.48870222e+06],\n",
       "       [ 1.74744671e+04,  7.30583608e+06],\n",
       "       [-2.53630770e+04,  9.95264486e+06],\n",
       "       [ 2.91474079e+04,  7.04472035e+06],\n",
       "       [ 1.72420328e+03,  5.14666821e+06],\n",
       "       [ 1.75935690e+04,  6.08653973e+06],\n",
       "       [ 3.05095339e+04,  7.53151530e+06],\n",
       "       [ 2.45454018e+04,  6.55900933e+06],\n",
       "       [ 2.17526099e+04,  9.35568514e+06],\n",
       "       [ 1.78950222e+04,  5.39809925e+06],\n",
       "       [-2.14428196e+04,  7.78744842e+06],\n",
       "       [-1.90804132e+04,  1.06624358e+07],\n",
       "       [ 1.08012261e+04,  3.05591675e+06],\n",
       "       [ 2.38933047e+04,  9.32015279e+06],\n",
       "       [ 1.59510375e+04,  6.62947638e+06],\n",
       "       [ 2.28763373e+04,  9.70708084e+06],\n",
       "       [ 2.18420735e+04,  9.37104649e+06],\n",
       "       [-2.98304541e+04,  4.55650630e+06],\n",
       "       [-2.12624768e+04,  1.10733434e+07],\n",
       "       [-2.43134853e+04,  6.28416525e+06],\n",
       "       [ 3.33486944e+04,  5.55698696e+06],\n",
       "       [-2.32078815e+04,  9.59945946e+06],\n",
       "       [-2.53665650e+04,  1.03391358e+07],\n",
       "       [-2.94247624e+04,  6.53780136e+06],\n",
       "       [-1.10130445e+04,  6.45439948e+06],\n",
       "       [ 2.83058818e+04,  5.68818068e+06],\n",
       "       [ 1.04975169e+04,  4.03205213e+06],\n",
       "       [ 1.29128311e+04,  6.40744034e+06],\n",
       "       [ 1.93077281e+04,  4.79834815e+06],\n",
       "       [-2.75934357e+04,  6.42044672e+06],\n",
       "       [ 8.36786342e+03,  7.84399869e+06],\n",
       "       [ 2.55425248e+04,  1.04302712e+07],\n",
       "       [-2.80095848e+04,  9.43339709e+06],\n",
       "       [ 1.45549026e+04,  4.98222914e+06],\n",
       "       [ 1.95141571e+04,  6.49239104e+06],\n",
       "       [-2.58560293e+04,  1.02615450e+07],\n",
       "       [ 2.03383822e+04,  1.06573606e+07],\n",
       "       [-1.68955141e+04,  6.88479275e+06],\n",
       "       [-2.37879374e+04,  1.04295460e+07],\n",
       "       [-1.35523112e+04,  5.80074898e+06],\n",
       "       [-2.34579562e+04,  7.52667327e+06],\n",
       "       [-2.39847228e+04,  7.42078331e+06],\n",
       "       [ 1.52853258e+03,  6.77254852e+06],\n",
       "       [ 2.27255002e+04,  6.04642954e+06],\n",
       "       [ 1.40049827e+04,  6.87030715e+06],\n",
       "       [ 8.16097880e+03,  5.90631390e+06],\n",
       "       [-2.05830351e+04,  7.28056018e+06],\n",
       "       [-2.33694006e+04,  5.21001921e+06],\n",
       "       [-2.18469011e+04,  1.00323977e+07],\n",
       "       [ 1.42336581e+04,  7.03144202e+06],\n",
       "       [ 2.01385046e+04,  7.73750590e+06],\n",
       "       [-1.67514893e+04,  7.16506646e+06],\n",
       "       [ 2.72069356e+04,  7.84442734e+06],\n",
       "       [ 1.70409785e+03,  6.71566154e+06],\n",
       "       [-2.68931221e+04,  7.22667308e+06],\n",
       "       [ 3.12354084e+04,  6.82308803e+06]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 23.85%\n",
      "\n",
      "The coefficient of determination is: 0.734\n",
      "\n",
      "The relative averaged absolute error is: 0.342\n",
      "\n",
      "The relative maximum absolute error is: 2.068\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "testing = True\n",
    "\n",
    "if testing:\n",
    "    model.eval()\n",
    "    test_x = Variable(torch.from_numpy(X_test.astype('float32')))\n",
    "    test_y = Variable(torch.from_numpy(Y_test.astype('float32')))\n",
    "\n",
    "    test_pred = model(test_x)\n",
    "    \n",
    "    test_y = reverse_range_norm(test_y.detach().numpy(), y_min, y_max)\n",
    "    test_pred = reverse_range_norm(test_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "    err_perc = pe(test_pred, test_y)\n",
    "    r_2 = r2(test_pred, test_y)\n",
    "    raae_score = raae(test_pred, test_y)\n",
    "    rmae_score = rmae(test_pred, test_y)\n",
    "\n",
    "    if flag_list:\n",
    "        for i in range(test_y.shape[0]):\n",
    "            print(\"True : {:.4f}, {:.4f}  |  Prediction : {:.4f}, {:.4f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "                  format(*test_y[i, :], *test_pred[i, :], *err_perc[i, :]))\n",
    "    print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "    print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "    print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "    print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 Parameter containing:\n",
      "tensor([[-1.0844e-01, -4.9057e-01, -4.6628e-01,  6.9210e-01, -2.3332e-01,\n",
      "          4.2882e-01, -4.1004e-01, -1.8674e-01],\n",
      "        [-1.4891e-01, -3.9336e-01, -9.4734e-02,  5.2422e-02,  1.7628e-01,\n",
      "          2.3287e-01, -8.9333e-03,  3.8331e-01],\n",
      "        [-1.8496e-01,  2.4584e-02,  2.1688e-02,  3.7966e-02,  5.9815e-02,\n",
      "         -5.8522e-01, -5.1259e-01,  1.1237e-01],\n",
      "        [ 1.1920e-03, -5.3885e-03,  5.2022e-03, -2.2404e-03,  2.4978e-02,\n",
      "         -2.0544e-02,  2.3576e-02, -6.3236e-01],\n",
      "        [ 7.2022e-02, -5.1027e-02, -2.5911e-02,  5.7195e-01, -5.0238e-02,\n",
      "          6.3549e-03,  6.9255e-03, -9.6424e-03],\n",
      "        [-8.7163e-02,  1.0295e-01, -1.1280e-01, -2.4809e-01,  6.9280e-02,\n",
      "          3.9446e-01, -5.7644e-01, -1.3134e-01],\n",
      "        [-5.4915e-01,  1.8107e-01,  3.1066e-01, -4.0122e-02,  3.0949e-02,\n",
      "         -7.8460e-02,  1.0198e-02, -1.1879e-01],\n",
      "        [ 1.3140e-02,  5.5321e-03,  1.2650e-02,  7.3111e-01, -3.6382e-02,\n",
      "         -2.8615e-02, -1.0237e-02, -2.4332e-02],\n",
      "        [ 2.7867e-02,  4.3903e-02,  5.0828e-03,  3.1196e-01, -1.5185e-01,\n",
      "          1.3220e-01,  9.0371e-02,  2.8901e-01],\n",
      "        [-2.3288e-01, -1.5710e-01, -2.6328e-01, -2.2125e-01, -1.4394e-01,\n",
      "          5.7349e-01, -3.9494e-01,  5.7261e-01],\n",
      "        [-1.2140e-02, -5.0333e-04,  8.5482e-03, -6.9222e-01,  7.8181e-02,\n",
      "          1.0093e-02, -9.4183e-03,  1.4308e-02],\n",
      "        [ 3.0464e-01,  5.2449e-01, -7.4451e-02, -1.5828e-01, -1.2839e-01,\n",
      "         -2.1625e-01,  7.8812e-02,  4.2669e-02],\n",
      "        [ 2.6085e-01, -1.8634e-01, -1.3076e-01,  1.7110e-01, -7.7619e-01,\n",
      "         -2.2135e-01, -1.7079e-03,  2.9317e-01],\n",
      "        [-1.0480e-02, -3.2760e-02, -4.5917e-03, -1.9695e-03, -8.1825e-03,\n",
      "         -2.2551e-02, -9.3070e-03, -7.8803e-01],\n",
      "        [ 5.1877e-01, -5.1291e-01,  2.3426e-01,  1.1469e-01, -1.7116e-01,\n",
      "          5.6418e-02, -1.8112e-01, -2.4057e-03],\n",
      "        [ 1.5666e-02, -1.8349e-03,  2.1933e-02, -8.1866e-03,  8.8873e-03,\n",
      "         -3.8231e-02, -1.7846e-02,  6.1761e-01],\n",
      "        [-2.4814e-01,  1.0727e-01,  3.6851e-02,  1.5521e-01,  7.0094e-02,\n",
      "         -4.3083e-02,  3.8171e-01, -2.8680e-03],\n",
      "        [ 1.7535e-02, -1.1089e-02, -1.2469e-02,  7.2036e-01,  2.1808e-02,\n",
      "         -4.3915e-03, -3.3395e-02, -2.1680e-02],\n",
      "        [ 4.4016e-02, -9.6052e-03,  4.1551e-03,  8.8652e-01, -1.1954e-02,\n",
      "         -1.9361e-02, -2.2673e-02, -6.6510e-04],\n",
      "        [-1.7163e-01, -2.4287e-02, -7.4443e-01,  2.3707e-02, -4.6964e-02,\n",
      "          1.4123e-01, -1.6278e-01, -3.3166e-03],\n",
      "        [ 1.2856e-01, -1.3241e-01,  1.7343e-01, -4.4937e-01,  5.0444e-01,\n",
      "         -3.5566e-01,  3.8307e-02, -1.9195e-01],\n",
      "        [-5.3341e-03,  1.4626e-02, -2.4461e-02,  4.6804e-02,  1.8781e-02,\n",
      "         -4.5524e-02,  6.2318e-02, -1.7648e+00],\n",
      "        [-7.3661e-02, -1.4683e-01, -5.2092e-01,  3.4310e-02, -4.1813e-03,\n",
      "          1.7898e-01,  2.5288e-01,  3.1542e-01],\n",
      "        [-7.0800e-01, -1.8801e-01,  3.3306e-01,  2.6298e-02, -1.8875e-01,\n",
      "         -2.2749e-03,  4.2050e-02, -1.2990e-01],\n",
      "        [ 6.7781e-03,  2.1143e-03,  1.5522e-02, -1.0262e-02,  2.0047e-02,\n",
      "         -4.0300e-02, -1.5258e-02,  7.2497e-01],\n",
      "        [ 3.5062e-04, -7.4771e-03,  1.3226e-02,  8.7713e-01,  4.4254e-02,\n",
      "          1.3752e-02, -3.5190e-03, -2.0116e-01],\n",
      "        [ 2.7619e-01, -1.4189e-01,  2.5657e-01,  2.7069e-01, -4.8554e-01,\n",
      "          1.8988e-01, -5.6985e-02,  1.7531e-01],\n",
      "        [ 3.5023e-02,  8.9313e-02, -1.2721e-01,  1.3398e-01,  1.8522e-01,\n",
      "         -1.7377e-01, -3.2339e-01,  2.6656e-01],\n",
      "        [-1.7246e-02, -9.1714e-03,  7.8256e-03, -5.8404e-02,  5.5100e-03,\n",
      "          1.3484e-03, -2.5005e-03, -1.3705e+00],\n",
      "        [ 2.4685e-01, -2.0182e-02, -1.3812e-01,  1.3410e-01, -8.5874e-01,\n",
      "         -1.6122e-03,  3.3256e-01, -1.5802e-01],\n",
      "        [-5.7846e-01, -3.5988e-01,  3.7208e-02, -7.2953e-01, -6.3505e-02,\n",
      "         -1.1108e-03, -1.5298e-01, -2.0105e-01],\n",
      "        [-7.1637e-02,  4.2091e-02,  2.5132e-02, -5.2019e-01,  8.1150e-02,\n",
      "          1.0033e-03,  2.4940e-02,  6.1611e-01],\n",
      "        [ 4.2385e-05,  2.6666e-05,  3.4214e-05, -1.9625e-05, -3.6069e-05,\n",
      "         -2.6943e-05,  2.3318e-05, -2.1698e-05],\n",
      "        [-5.6526e-02,  3.4733e-02,  6.7802e-03, -1.8144e-01, -1.2912e-02,\n",
      "         -2.5097e-02,  2.9677e-02,  9.8578e-01],\n",
      "        [-7.7475e-06,  5.9301e-06, -1.8844e-06,  1.0501e-05,  2.2919e-05,\n",
      "          1.1559e-05,  4.7148e-06, -1.2189e-05],\n",
      "        [-1.4510e-01,  9.7375e-02,  1.7491e-01, -1.0011e-01, -1.0493e+00,\n",
      "          8.9976e-02,  4.2141e-01,  6.3543e-02],\n",
      "        [ 2.3625e-01,  2.0107e-01,  1.5814e-02, -8.8970e-01,  1.8992e-02,\n",
      "         -1.4729e-02,  2.4035e-02, -4.9901e-01],\n",
      "        [ 1.3358e-02, -1.0489e-02, -4.4734e-03, -1.0966e-02, -1.2194e-02,\n",
      "         -1.4186e-02,  7.7175e-03, -6.7784e-01],\n",
      "        [ 4.4299e-02, -1.2579e-01, -8.2714e-02,  1.1046e+00, -9.9921e-02,\n",
      "         -1.1693e-02, -1.0037e-01, -9.6645e-02],\n",
      "        [-3.9751e-01,  3.4425e-01, -1.5810e-01, -9.1850e-01,  6.2328e-01,\n",
      "          1.5106e-04, -3.1949e-01,  9.9856e-02],\n",
      "        [-1.0390e-01, -1.1792e-03, -1.9893e-01, -7.0709e-02,  5.8567e-02,\n",
      "          3.2614e-01,  2.0391e-01, -8.0878e-02],\n",
      "        [-3.3268e-01,  1.2249e-01, -2.7809e-02,  1.1352e-01,  3.5619e-01,\n",
      "         -1.6145e-01,  1.4630e-01, -5.9135e-03],\n",
      "        [ 2.2202e-01,  3.6265e-01, -1.2008e-01, -2.1518e-01, -2.5427e-01,\n",
      "         -5.1642e-02, -1.2405e-01,  3.8521e-01],\n",
      "        [ 1.1141e-01, -2.4619e-01, -8.2764e-01, -2.4855e-01,  4.2161e-01,\n",
      "          3.6868e-01, -3.6010e-02, -3.3215e-01],\n",
      "        [ 5.1238e-02, -2.5498e-02, -3.8415e-02,  9.7346e-01, -4.9586e-02,\n",
      "         -4.3489e-02, -4.6563e-02,  5.6522e-02],\n",
      "        [ 1.0806e-01, -4.6786e-02,  1.1394e-02, -8.7110e-01,  1.6838e-02,\n",
      "         -2.7872e-02, -4.1876e-02, -4.1077e-01],\n",
      "        [ 4.8652e-03,  4.2394e-03,  8.6441e-03, -7.9896e-03,  8.6882e-03,\n",
      "         -2.8322e-02, -4.9992e-03,  8.3308e-01],\n",
      "        [-2.9328e-02, -2.9241e-02,  7.1670e-03, -1.9381e+00,  4.9141e-02,\n",
      "          2.9948e-02,  2.4500e-02,  1.0333e-01],\n",
      "        [ 1.6407e-02,  3.6978e-02,  4.1935e-02,  6.0886e-01,  4.6733e-02,\n",
      "         -6.5504e-02,  4.5734e-02, -4.7528e-02],\n",
      "        [ 7.3859e-02,  4.3628e-03,  1.3842e-02, -6.3777e-01,  1.2565e-02,\n",
      "         -1.0053e-02, -7.6583e-03, -1.3661e-02],\n",
      "        [ 3.4180e-02,  1.2163e-03,  3.8031e-02, -1.2720e+00,  4.3714e-04,\n",
      "         -1.4935e-01, -4.1294e-02, -9.7191e-02],\n",
      "        [ 4.8948e-01, -2.5928e-01, -1.0032e-01,  2.0506e-01,  1.0472e-02,\n",
      "         -1.2401e-01,  1.9718e-01,  1.8678e-01],\n",
      "        [-1.2365e-01,  5.6998e-02,  2.2850e-02,  6.7332e-01,  2.1596e-02,\n",
      "          7.4934e-02, -4.2624e-04, -1.4507e-02],\n",
      "        [-5.5676e-02, -4.0404e-02,  7.1155e-02,  2.6401e-01,  1.0655e-01,\n",
      "         -5.5856e-03,  1.0473e-02, -1.3387e-01],\n",
      "        [ 8.2770e-02,  1.1406e-01, -3.0749e-01,  1.2977e-01,  2.2413e-02,\n",
      "          3.0460e-01, -2.0829e-01, -2.1471e-02],\n",
      "        [ 2.4317e-01, -2.6288e-02,  9.2921e-02, -1.0394e+00,  2.3064e-01,\n",
      "         -5.1541e-02,  2.9187e-01, -3.3564e-01],\n",
      "        [-2.2131e-01,  8.3428e-02,  2.0617e-02, -2.7853e-01,  5.1261e-01,\n",
      "         -2.2280e-01, -1.4498e-01,  2.5384e-01],\n",
      "        [ 2.5073e-02, -1.4383e-03,  3.0116e-02,  6.9193e-02,  7.6774e-02,\n",
      "          6.4241e-03, -2.0786e-02,  6.8348e-01],\n",
      "        [-4.3917e-01, -6.0662e-01, -2.1586e-01,  1.6908e-02,  2.9456e-01,\n",
      "          3.1355e-02,  2.7211e-02,  3.6422e-02],\n",
      "        [-1.6864e-01,  2.4105e-01,  1.1299e-01,  1.4254e-02,  1.2980e-01,\n",
      "         -1.9424e-01,  1.9773e-01, -4.3516e-01],\n",
      "        [ 4.6528e-02,  1.5668e-03,  2.5372e-03, -6.7346e-01, -3.3359e-02,\n",
      "          1.2720e-02, -1.2200e-02, -4.1914e-02],\n",
      "        [-5.9338e-02,  2.4411e-03, -2.7623e-03, -1.1176e-02,  1.0936e-02,\n",
      "         -9.7964e-03,  4.2503e-02,  1.0177e+00],\n",
      "        [ 2.7732e-05, -6.4002e-05, -2.5877e-05,  1.0272e-05, -7.6049e-06,\n",
      "          3.9102e-05, -2.6984e-05, -2.0533e-05],\n",
      "        [ 4.9562e-02,  1.0627e-02,  2.5869e-03, -1.5897e+00, -8.3313e-02,\n",
      "          2.2313e-02, -4.3083e-03,  5.4991e-02]], requires_grad=True)\n",
      "b_1 Parameter containing:\n",
      "tensor([-2.9830e-01, -2.3101e-01,  1.6092e-01,  4.0474e-01, -1.5960e-01,\n",
      "         6.2052e-02,  1.4837e-03, -3.8434e-01, -3.3192e-01, -1.6254e-01,\n",
      "         4.1213e-01, -4.0923e-01,  1.7230e-02,  5.5169e-01, -2.7098e-01,\n",
      "        -1.9397e-01, -2.4090e-01, -2.0561e-01, -2.8640e-01,  2.4910e-01,\n",
      "         4.4970e-02,  2.0080e-01, -1.1952e-01,  1.9962e-01, -2.3408e-01,\n",
      "        -5.5205e-01, -3.6108e-01, -1.2196e-01,  2.6996e-01,  9.7083e-02,\n",
      "         8.5317e-01, -1.8584e-01, -7.3052e-06, -5.5606e-01,  1.9652e-06,\n",
      "         1.0020e-03,  3.8815e-01,  4.5674e-01, -8.6975e-01, -6.8611e-02,\n",
      "        -1.4665e-01, -1.4835e-01, -3.4344e-01,  6.8277e-02, -7.3664e-01,\n",
      "         6.5048e-01, -2.6737e-01,  1.7589e-01, -4.2642e-01,  4.0296e-01,\n",
      "         6.3933e-01, -3.1673e-01, -2.5484e-01, -5.8375e-02, -1.0743e-01,\n",
      "         2.1586e-01, -1.3436e-01, -4.6901e-01,  1.1224e-01, -1.9496e-01,\n",
      "         4.6646e-01, -8.1822e-01, -3.6351e-05,  2.3458e-01],\n",
      "       requires_grad=True)\n",
      "W_2 Parameter containing:\n",
      "tensor([[-1.6548e-01, -6.6943e-03, -4.8915e-03,  ..., -2.8771e-02,\n",
      "         -6.9771e-05, -1.5098e-01],\n",
      "        [-3.8894e-03, -1.0169e-04, -1.8631e-01,  ...,  2.3320e-02,\n",
      "          1.1817e-04, -5.7920e-03],\n",
      "        [-2.0205e-01,  1.6208e-02,  6.2865e-04,  ..., -4.2851e-02,\n",
      "         -1.5836e-04, -2.9667e-06],\n",
      "        ...,\n",
      "        [-2.6094e-05,  5.4215e-02,  3.3715e-05,  ..., -6.0024e-02,\n",
      "          8.5578e-05,  7.4026e-02],\n",
      "        [-2.0628e-01,  5.4677e-03,  4.8458e-03,  ..., -1.0183e-02,\n",
      "          1.4416e-04, -5.7903e-04],\n",
      "        [-1.5925e-01,  3.8437e-04, -5.5289e-04,  ..., -2.9123e-03,\n",
      "         -8.7245e-06, -2.7485e-02]], requires_grad=True)\n",
      "b_2 Parameter containing:\n",
      "tensor([ 2.2950e-02, -2.6891e-02,  4.0276e-02,  4.8653e-02,  4.3052e-02,\n",
      "        -2.9746e-01, -2.7227e-01,  4.2470e-02,  3.6495e-02,  3.5921e-02,\n",
      "         5.1349e-02, -9.1147e-02,  3.7366e-02,  5.4128e-02,  3.2487e-02,\n",
      "         1.1712e-02,  7.1657e-02,  5.3352e-02,  5.0999e-02,  4.1305e-02,\n",
      "         5.3762e-02,  4.0014e-02,  5.1557e-02,  3.4891e-02,  4.6329e-02,\n",
      "        -2.6054e-06,  3.8427e-02,  4.3749e-02, -7.3502e-02,  5.3622e-02,\n",
      "         2.8461e-02, -3.9258e-01,  2.8655e-02,  3.4366e-02, -1.1554e-01,\n",
      "         6.7380e-02,  6.4993e-02,  1.5199e-02,  4.0237e-02,  7.9818e-02,\n",
      "         2.7146e-02,  5.6265e-02,  2.7220e-02,  2.0307e-02,  4.4149e-02,\n",
      "        -3.5721e-01, -4.5842e-03, -4.3727e-02, -3.3942e-01, -3.3610e-01,\n",
      "         5.1826e-02, -7.5576e-02,  1.5953e-02,  6.0863e-02, -2.3765e-05,\n",
      "         5.4804e-02,  1.8235e-02,  3.6759e-02,  6.5866e-02,  4.3525e-02,\n",
      "         3.5013e-02, -7.7510e-02,  2.7580e-02,  2.4215e-02],\n",
      "       requires_grad=True)\n",
      "W_4 Parameter containing:\n",
      "tensor([[ 1.1174e+00, -1.2804e+00,  7.1869e-01,  1.5016e-03,  1.5423e-01,\n",
      "         -3.4153e-02,  6.0086e-01,  4.5325e-01,  9.6072e-01,  1.0047e+00,\n",
      "         -2.5054e-01, -9.8434e-01, -1.7475e-01, -2.7004e-01,  6.2269e-01,\n",
      "         -5.4778e-01,  1.7729e-01, -2.4910e-01,  8.9739e-04,  6.2594e-01,\n",
      "         -1.2208e-04,  4.1719e-01, -1.9787e-01, -7.1724e-01, -8.0016e-03,\n",
      "          4.2922e-07,  4.9071e-03,  1.2287e-03,  1.6404e+00, -5.3671e-01,\n",
      "          6.8886e-01, -1.6415e-02,  7.7774e-01,  6.6143e-01, -4.6174e-01,\n",
      "         -2.9532e-01, -2.3051e-01,  2.1284e-01, -3.6043e-01,  8.3158e-04,\n",
      "         -1.2151e+00, -7.2907e-03, -9.5414e-01,  2.7816e-01,  4.5871e-01,\n",
      "         -1.3690e-01,  8.2323e-01,  1.0333e+00,  4.8539e-04, -2.8621e-04,\n",
      "          5.6992e-04, -2.2515e-01,  3.3342e-02, -3.6171e-02,  4.7728e-05,\n",
      "         -3.8187e-01,  7.8315e-01,  1.1392e+00,  1.2305e-05,  3.9273e-01,\n",
      "          3.0737e-01,  1.5044e+00,  6.8008e-01,  3.3979e-01],\n",
      "        [-8.6497e-02,  1.5491e-04, -2.1583e-03,  1.1183e+00,  1.0600e+00,\n",
      "         -1.0954e+00, -1.0768e+00,  7.8895e-01, -2.2402e-03, -3.2689e-03,\n",
      "          7.4318e-01, -3.3273e-02,  5.0663e-01,  2.8413e-01, -7.5255e-03,\n",
      "          2.9968e-04, -1.0261e+00,  1.1233e+00,  7.5640e-01, -4.4834e-04,\n",
      "          8.5600e-01,  4.9844e-01,  8.0127e-01, -2.7428e-01,  1.1274e+00,\n",
      "          2.9058e-05,  6.2457e-01,  1.3109e+00, -4.7677e-03,  5.5979e-01,\n",
      "         -2.8742e-02, -7.4169e-01, -5.1534e-03, -7.3787e-02, -9.1456e-01,\n",
      "         -8.1353e-01,  1.2102e+00,  9.3138e-01, -1.3036e+00, -1.3388e+00,\n",
      "          1.4447e-01,  8.2264e-01, -1.6829e-01, -1.8200e-03,  9.2742e-01,\n",
      "         -6.5874e-01, -1.7550e-02, -1.4267e-01, -9.5550e-01, -6.9321e-01,\n",
      "          7.4830e-01, -1.4162e-01,  1.1841e+00,  1.1724e+00, -7.5009e-05,\n",
      "          3.6151e-01,  1.2875e-01, -3.5416e-01,  1.3502e+00,  7.5137e-01,\n",
      "         -1.8742e-04, -2.8613e-03, -2.2592e-02, -2.4821e-02]],\n",
      "       requires_grad=True)\n",
      "b_4 Parameter containing:\n",
      "tensor([0.3689, 0.3456], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
