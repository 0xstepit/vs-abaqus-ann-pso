{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLQLStKwyzgs"
   },
   "source": [
    "# **METAMODELING WITH ARTIFICIAL NEURAL NETWORK**\n",
    "\n",
    "In this notebook, we will use the results of Abaqus analyses in order to build an Artificial Neural Network (ANN) of the Finite Element (FE) analysis solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Orwif6dIvF4t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bwj567NSvHv1",
    "outputId": "e91823c3-87c5-47fe-cca9-cd505741e7ff"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Matplotlib spec\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']}) # Palatino font\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jqtrSiFqdBTe",
    "outputId": "1cd691d5-fc75-4420-b6e5-d331a6ee9c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMkgB8U2dS1A"
   },
   "source": [
    "When this notebook has been generated the result of the previous line of code is: _'1.5.1'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZDSnO7jyzg8"
   },
   "source": [
    "We fix the seed in order to obtain reproducible results.\n",
    "\n",
    "__N.B.__ : Reproducible results are obtained every time the runtime is restarded and runned. If you run multiple time the same cell the results will not be reporducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SytMvTAE22lL"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed=seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Last two lines just when using GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxRgA_ioyzhA"
   },
   "source": [
    "## **Data preprocessing**\n",
    "\n",
    "We start by importing some information about the model used to generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "0FZ6R8aeyzhB",
    "outputId": "e6f59f12-68cb-4eb0-dd90-990424b0c8c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Radius</th>\n",
       "      <th>MaxCurvature</th>\n",
       "      <th>MeshSize</th>\n",
       "      <th>Plies</th>\n",
       "      <th>EffectivePlies</th>\n",
       "      <th>Symmetric</th>\n",
       "      <th>Balanced</th>\n",
       "      <th>AnglesFunction</th>\n",
       "      <th>LoadCase</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>300</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>harmlin</td>\n",
       "      <td>torsion</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Radius  MaxCurvature  MeshSize  Plies  EffectivePlies  Symmetric  \\\n",
       "Value     300      0.001575        10      8               2       True   \n",
       "\n",
       "       Balanced AnglesFunction LoadCase  Train  Test  \n",
       "Value      True        harmlin  torsion    512   128  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify parameter to choose the output folder to consider\n",
    "load_case = 'torsion'\n",
    "stacking_sequence = 'symmetric_balanced'\n",
    "data_set = '8x'\n",
    "fiber_path = 'harmlin'\n",
    "\n",
    "# Check if notebook running in Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# Model info folder\n",
    "if is_colab:\n",
    "    input_folder = './'\n",
    "else:\n",
    "    input_folder = load_case + '/' + stacking_sequence + '/' + data_set + '/' + fiber_path + '/'\n",
    "\n",
    "info = pd.read_csv(input_folder + 'model_info.csv', sep=\",\")\n",
    "info.index = ['Value']\n",
    "eff_plies = int(info['EffectivePlies'].values)\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active sets: Train, Test\n"
     ]
    }
   ],
   "source": [
    "sets = ['Train', 'Val', 'Test']\n",
    "for set in sets:\n",
    "    if set not in info.keys():\n",
    "        sets.remove(set)\n",
    "print(\"Active sets: {}, {}\".format(*sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIBWzRwLyzhG"
   },
   "source": [
    "At this point we have to import the data set containing the input and output of the FE analysis. The data is stored in a dataframe in which the upper part is associated to the training set and the lower part to the test set. The precise number of upper row belonging to the train set is indicated in the info above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "niD2Q60oyzhG",
    "outputId": "6046f3fb-ed80-4099-aa94-7eb2dd87de49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude1</th>\n",
       "      <th>PhaseShift1</th>\n",
       "      <th>Omega1</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Amplitude2</th>\n",
       "      <th>PhaseShift2</th>\n",
       "      <th>Omega2</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Buckling</th>\n",
       "      <th>Stiffness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>6.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.633786</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>-1.716353</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>625.422031</td>\n",
       "      <td>6.900489e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>82.809519</td>\n",
       "      <td>52.005238</td>\n",
       "      <td>0.577781</td>\n",
       "      <td>51.996747</td>\n",
       "      <td>82.051847</td>\n",
       "      <td>52.004818</td>\n",
       "      <td>0.577854</td>\n",
       "      <td>52.003605</td>\n",
       "      <td>23019.537491</td>\n",
       "      <td>2.624252e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-195.753000</td>\n",
       "      <td>-89.808000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>-89.749000</td>\n",
       "      <td>-199.306000</td>\n",
       "      <td>-89.694000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-89.677000</td>\n",
       "      <td>-37397.900000</td>\n",
       "      <td>1.580429e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-45.722000</td>\n",
       "      <td>-45.037500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-44.877250</td>\n",
       "      <td>-44.977000</td>\n",
       "      <td>-44.972000</td>\n",
       "      <td>0.499750</td>\n",
       "      <td>-44.976250</td>\n",
       "      <td>-21553.325000</td>\n",
       "      <td>4.951265e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>11844.100000</td>\n",
       "      <td>6.874971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.232750</td>\n",
       "      <td>44.813500</td>\n",
       "      <td>1.498250</td>\n",
       "      <td>44.808250</td>\n",
       "      <td>38.815750</td>\n",
       "      <td>44.992000</td>\n",
       "      <td>1.499500</td>\n",
       "      <td>44.966500</td>\n",
       "      <td>21768.725000</td>\n",
       "      <td>8.746724e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>199.303000</td>\n",
       "      <td>89.756000</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>89.750000</td>\n",
       "      <td>196.889000</td>\n",
       "      <td>89.809000</td>\n",
       "      <td>1.999000</td>\n",
       "      <td>89.943000</td>\n",
       "      <td>36818.400000</td>\n",
       "      <td>1.244372e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amplitude1  PhaseShift1      Omega1       Beta1  Amplitude2  \\\n",
       "count  640.000000   640.000000  640.000000  640.000000  640.000000   \n",
       "mean    -0.633786     0.005536    1.000078   -0.002559   -1.716353   \n",
       "std     82.809519    52.005238    0.577781   51.996747   82.051847   \n",
       "min   -195.753000   -89.808000    0.004000  -89.749000 -199.306000   \n",
       "25%    -45.722000   -45.037500    0.500000  -44.877250  -44.977000   \n",
       "50%      0.191500     0.025000    0.998500    0.051000    0.036000   \n",
       "75%     41.232750    44.813500    1.498250   44.808250   38.815750   \n",
       "max    199.303000    89.756000    1.999000   89.750000  196.889000   \n",
       "\n",
       "       PhaseShift2      Omega2       Beta2      Buckling     Stiffness  \n",
       "count   640.000000  640.000000  640.000000    640.000000  6.400000e+02  \n",
       "mean      0.012386    0.999973    0.005630    625.422031  6.900489e+06  \n",
       "std      52.004818    0.577854   52.003605  23019.537491  2.624252e+06  \n",
       "min     -89.694000    0.001000  -89.677000 -37397.900000  1.580429e+06  \n",
       "25%     -44.972000    0.499750  -44.976250 -21553.325000  4.951265e+06  \n",
       "50%       0.070000    0.998500    0.029500  11844.100000  6.874971e+06  \n",
       "75%      44.992000    1.499500   44.966500  21768.725000  8.746724e+06  \n",
       "max      89.809000    1.999000   89.943000  36818.400000  1.244372e+07  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data sets\n",
    "data_orig = pd.read_csv(input_folder + '/data.csv', sep=',')\n",
    "data_orig.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9U4RPrnyzhP"
   },
   "source": [
    "The most important step to perform before training our model is the normalization of the variables. Different strategies are possible at this end, among which 2 are the most used:\n",
    "\n",
    "* Range normalization: converts all the values to the range $[0, 1]$\n",
    "\n",
    "* Standard score normalization: forces the variables to have $0$ mean and $1$ standard deviation\n",
    "\n",
    "We will try both to see the effect on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZVjkHziyzhQ"
   },
   "outputs": [],
   "source": [
    "def range_norm(x, x_min=None, x_max=None):\n",
    "    \"\"\" Normalization in range [0, 1] \"\"\"\n",
    "    if x_min is None and x_max is None:\n",
    "        x_min = np.min(x, axis=0)\n",
    "        x_max = np.max(x, axis=0)\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    return x_norm, x_min, x_max\n",
    "\n",
    "def std_norm(x, m=None, s=None):\n",
    "    \"\"\" Normalization with zero mean and unitary standard deviation \"\"\"\n",
    "    if m is None and s is None:\n",
    "        m = np.mean(x, axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "    x_norm = (x - m) / s\n",
    "    \n",
    "    return x_norm, m, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL3mX63XyzhT"
   },
   "source": [
    "Now we can split the data into training and test set. The two sets have been generate independently during the DOE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "uy149NwNyzhT",
    "outputId": "224f9a13-bed4-4c54-f05e-a4eee762b812"
   },
   "outputs": [],
   "source": [
    "X = data_orig.drop(['Buckling', 'Stiffness'], axis=1).values\n",
    "Y = data_orig[['Buckling','Stiffness']].values\n",
    "\n",
    "# Train set\n",
    "train_smp = int(info['Train'].values)\n",
    "_X_train = X[:train_smp, :]\n",
    "_Y_train = Y[:train_smp]\n",
    "last = np.copy(train_smp)\n",
    "\n",
    "# Validation set\n",
    "if 'Val' in sets:\n",
    "    val_smp = int(info['Val'].values)\n",
    "    _X_val = X[last:last+val_smp, :]\n",
    "    _Y_val = Y[last:last+val_smp]\n",
    "    last += val_smp\n",
    "\n",
    "# Test set\n",
    "if 'Test' in sets:\n",
    "    test_smp = int(info['Test'].values)\n",
    "    _X_test = X[last:last+test_smp, :]\n",
    "    _Y_test = Y[last:last+test_smp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If validation set is not present we will generate it from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Data sets info: \n",
      "\n",
      "X_train : (409, 8)  |  Y_train : (409, 2)\n",
      "X_val   : (103, 8)  |  Y_val   : (103, 2)\n",
      "X_test  : (128, 8)  |  Y_test  : (128, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_X_train, _X_val, _Y_train, _Y_val = train_test_split(_X_train, _Y_train, test_size=0.2, random_state=seed)\n",
    "print('            Data sets info: \\n')\n",
    "print(\"X_train : {}  |  Y_train : {}\".format(_X_train.shape, _Y_train.shape))\n",
    "print(\"X_val   : {}  |  Y_val   : {}\".format(_X_val.shape, _Y_val.shape))\n",
    "print(\"X_test  : {}  |  Y_test  : {} \\n\".format(_X_test.shape, _Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can generate the iterable data sets for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, Y):\n",
    "    \"\"\" Random shuffle of samples in X and y \"\"\"\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    return X[idx], Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "ORbqtnh-yzhR",
    "outputId": "d18315cf-cbc9-40f5-f987-26f6485bba5b"
   },
   "outputs": [],
   "source": [
    "# Normalization training set\n",
    "X_train, x_min, x_max = range_norm(_X_train)\n",
    "Y_train, y_min, y_max = range_norm(_Y_train)\n",
    "\n",
    "# Shuffle training set\n",
    "X_train, Y_train = shuffle_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save the normalization values since we will need them during the optimization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate how many free variables are in the model\n",
    "if info['AnglesFunction'].values == 'harmlin':\n",
    "    col_num = int(4 * info['EffectivePlies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bounds = pd.DataFrame({'x_min': x_min, 'x_max': x_max}, index=data_orig.columns[:col_num]).T\n",
    "Y_bounds = pd.DataFrame({'y_min':y_min, 'y_max':y_max}, index=data_orig.columns[col_num:]).T\n",
    "X_bounds.to_csv(input_folder + '/X_bounds.csv', index=True, float_format='%.3f')\n",
    "Y_bounds.to_csv(input_folder'/Y_bounds.csv', index=True, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization validation set\n",
    "X_val, _, _ = range_norm(_X_val, x_min=x_min, x_max=x_max)\n",
    "Y_val, _, _ = range_norm(_Y_val, x_min=y_min, x_max=y_max)\n",
    "\n",
    "# Normalization testing set\n",
    "X_test, _, _ = range_norm(_X_test, x_min=x_min, x_max=x_max)\n",
    "Y_test, _, _ = range_norm(_Y_test, x_min=y_min, x_max=y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(_X, _Y, batch_size):\n",
    "    \"\"\" Split the data into k batches \"\"\"\n",
    "\n",
    "    n_samples = _X.shape[0]\n",
    "    leftovers = {}\n",
    "    n_leftovers = n_samples % batch_size\n",
    "    \n",
    "    # Case with all batches of equal size\n",
    "    if n_leftovers != 0:\n",
    "        leftovers[\"X\"] = _X[-n_leftovers:]\n",
    "        leftovers[\"Y\"] = _Y[-n_leftovers:]\n",
    "        _X = _X[:-n_leftovers]\n",
    "        _Y = _Y[:-n_leftovers]\n",
    "\n",
    "    k = np.int(_X.shape[0] / batch_size)\n",
    "        \n",
    "    X_split = np.split(_X, k)\n",
    "    Y_split = np.split(_Y, k)\n",
    "    \n",
    "    # Add leftover samples as last batch\n",
    "    if n_leftovers != 0:\n",
    "        X_split.append(leftovers[\"X\"])\n",
    "        Y_split.append(leftovers[\"Y\"])\n",
    "\n",
    "    return X_split, Y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "X_train_b, Y_train_b = create_batches(X_train, Y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches dimensions: \n",
      "\n",
      "Batch 0 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 1 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 2 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 3 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 4 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 5 : input (64, 8)  ,  output : (64, 2)\n",
      "Batch 6 : input (25, 8)  ,  output : (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Batches dimensions: \\n\")\n",
    "for i in range(len(X_train_b)):\n",
    "    print(\"Batch {} : input {}  ,  output : {}\".format(i, X_train_b[i].shape, Y_train_b[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Cyx2vNti_z4"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\" Stop network training if validation loss increases for a certain time.\n",
    "        The code is base on https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\"\"\"\n",
    "    def __init__(self, patience=30, path=input_folder + '/weights_NN_'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        if self.val_loss_min > val_loss:\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7O7AIIoyzhU"
   },
   "source": [
    "## **Neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vyx3cytYfs7E"
   },
   "source": [
    "First define network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aMiO2DUfrIK"
   },
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    \"\"\" Implementation of FeedForward Neural Network \"\"\"\n",
    "    def __init__(self, D_in, H, D_out, p):\n",
    "        super(FFNN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(H, D_in)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Second hidden layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Third hidden layer\n",
    "        #self.W_3 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        #self.b_3 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Output layer\n",
    "        self.W_4 = Parameter(init.xavier_normal_(torch.Tensor(D_out, H)))\n",
    "        self.b_4 = Parameter(init.constant_(torch.Tensor(D_out), 0))\n",
    "        \n",
    "        # define activation function in constructor\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        #self.activation_3 = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.activation_2(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = F.linear(x, self.W_3, self.b_3)\n",
    "        #x = self.activation_3(x)\n",
    "        #x = self.dropout(x)\n",
    "        pred = F.linear(x, self.W_4, self.b_4)\n",
    "        return pred\n",
    "\n",
    "p=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(torch.nn.Module):\n",
    "    \"\"\" Implementation of FeedForward Neural Network \"\"\"\n",
    "    def __init__(self, D_in, H, D_out, p):\n",
    "        super(FFNN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(H, D_in)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Second hidden layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Third hidden layer\n",
    "        #self.W_3 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        #self.b_3 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Output layer\n",
    "        self.W_4 = Parameter(init.xavier_normal_(torch.Tensor(D_out, H)))\n",
    "        self.b_4 = Parameter(init.constant_(torch.Tensor(D_out), 0))\n",
    "        \n",
    "        # define activation function in constructor\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        #self.activation_3 = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.activation_2(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = F.linear(x, self.W_3, self.b_3)\n",
    "        #x = self.activation_3(x)\n",
    "        #x = self.dropout(x)\n",
    "        pred = F.linear(x, self.W_4, self.b_4)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ibkHXkYKmXs6",
    "outputId": "3a2927df-9996-474a-eee5-3ec0d6f12263",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0000 | Train loss : 0.77409 | Val loss : 0.48309\n",
      "Iteration : 0100 | Train loss : 0.09403 | Val loss : 0.07564\n",
      "Iteration : 0200 | Train loss : 0.08073 | Val loss : 0.06375\n",
      "Iteration : 0300 | Train loss : 0.07158 | Val loss : 0.05997\n",
      "Iteration : 0400 | Train loss : 0.06552 | Val loss : 0.05783\n",
      "Iteration : 0500 | Train loss : 0.05637 | Val loss : 0.05731\n",
      "Iteration : 0600 | Train loss : 0.05793 | Val loss : 0.05658\n",
      "Iteration : 0700 | Train loss : 0.05307 | Val loss : 0.05611\n",
      "Iteration : 0800 | Train loss : 0.05198 | Val loss : 0.05488\n",
      "Iteration : 0900 | Train loss : 0.04742 | Val loss : 0.05368\n",
      "Iteration : 1000 | Train loss : 0.04548 | Val loss : 0.05465\n",
      "Iteration : 1100 | Train loss : 0.04343 | Val loss : 0.05267\n",
      "Iteration : 1200 | Train loss : 0.04143 | Val loss : 0.05041\n",
      "Iteration : 1300 | Train loss : 0.04515 | Val loss : 0.05110\n",
      "Iteration : 1400 | Train loss : 0.03964 | Val loss : 0.05131\n",
      "Iteration : 1500 | Train loss : 0.04262 | Val loss : 0.05284\n",
      "Iteration : 1600 | Train loss : 0.03987 | Val loss : 0.05193\n",
      "Iteration : 1700 | Train loss : 0.04090 | Val loss : 0.05003\n",
      "Iteration : 1800 | Train loss : 0.03857 | Val loss : 0.05084\n",
      "Iteration : 1900 | Train loss : 0.03862 | Val loss : 0.04946\n",
      "Iteration : 2000 | Train loss : 0.03841 | Val loss : 0.04789\n",
      "Iteration : 2100 | Train loss : 0.03550 | Val loss : 0.04856\n",
      "Iteration : 2200 | Train loss : 0.03718 | Val loss : 0.04753\n",
      "Iteration : 2300 | Train loss : 0.03560 | Val loss : 0.04804\n",
      "Iteration : 2400 | Train loss : 0.03340 | Val loss : 0.04702\n",
      "Iteration : 2500 | Train loss : 0.03666 | Val loss : 0.04731\n",
      "Iteration : 2600 | Train loss : 0.03136 | Val loss : 0.04476\n",
      "Iteration : 2700 | Train loss : 0.03542 | Val loss : 0.04567\n",
      "Iteration : 2800 | Train loss : 0.03252 | Val loss : 0.04826\n",
      "Iteration : 2900 | Train loss : 0.03465 | Val loss : 0.04530\n",
      "Iteration : 3000 | Train loss : 0.03415 | Val loss : 0.04473\n",
      "Iteration : 3100 | Train loss : 0.03136 | Val loss : 0.04465\n",
      "Iteration : 3200 | Train loss : 0.03094 | Val loss : 0.04126\n",
      "Iteration : 3300 | Train loss : 0.03072 | Val loss : 0.04281\n",
      "Iteration : 3400 | Train loss : 0.03137 | Val loss : 0.04403\n",
      "Iteration : 3500 | Train loss : 0.02855 | Val loss : 0.04137\n",
      "Iteration : 3600 | Train loss : 0.03061 | Val loss : 0.04064\n",
      "Iteration : 3700 | Train loss : 0.02935 | Val loss : 0.04143\n",
      "Iteration : 3800 | Train loss : 0.02780 | Val loss : 0.04182\n",
      "Iteration : 3900 | Train loss : 0.02845 | Val loss : 0.04392\n",
      "Iteration : 4000 | Train loss : 0.03064 | Val loss : 0.03965\n",
      "Iteration : 4100 | Train loss : 0.03006 | Val loss : 0.04243\n",
      "Iteration : 4200 | Train loss : 0.02925 | Val loss : 0.04013\n",
      "Iteration : 4300 | Train loss : 0.03154 | Val loss : 0.04152\n",
      "Iteration : 4400 | Train loss : 0.02922 | Val loss : 0.04055\n",
      "Iteration : 4500 | Train loss : 0.02751 | Val loss : 0.04215\n",
      "Iteration : 4600 | Train loss : 0.02758 | Val loss : 0.03842\n",
      "Iteration : 4700 | Train loss : 0.02670 | Val loss : 0.03803\n",
      "Iteration : 4800 | Train loss : 0.03263 | Val loss : 0.03843\n",
      "Iteration : 4900 | Train loss : 0.02751 | Val loss : 0.03788\n",
      "Iteration : 5000 | Train loss : 0.02822 | Val loss : 0.03725\n",
      "Iteration : 5100 | Train loss : 0.02742 | Val loss : 0.03719\n",
      "Iteration : 5200 | Train loss : 0.02755 | Val loss : 0.03702\n",
      "Iteration : 5300 | Train loss : 0.02843 | Val loss : 0.03794\n",
      "Iteration : 5400 | Train loss : 0.03163 | Val loss : 0.03746\n",
      "Iteration : 5500 | Train loss : 0.02464 | Val loss : 0.03829\n",
      "Iteration : 5600 | Train loss : 0.02671 | Val loss : 0.03669\n",
      "Iteration : 5700 | Train loss : 0.02670 | Val loss : 0.03705\n",
      "Iteration : 5800 | Train loss : 0.03196 | Val loss : 0.03726\n",
      "Iteration : 5900 | Train loss : 0.02674 | Val loss : 0.03595\n",
      "Iteration : 6000 | Train loss : 0.02616 | Val loss : 0.03719\n",
      "Iteration : 6100 | Train loss : 0.02759 | Val loss : 0.03545\n",
      "Iteration : 6200 | Train loss : 0.02811 | Val loss : 0.03697\n",
      "Iteration : 6300 | Train loss : 0.02822 | Val loss : 0.03824\n",
      "Iteration : 6400 | Train loss : 0.02822 | Val loss : 0.03759\n",
      "Iteration : 6500 | Train loss : 0.02816 | Val loss : 0.03661\n",
      "Iteration : 6600 | Train loss : 0.02687 | Val loss : 0.03575\n",
      "Iteration : 6700 | Train loss : 0.02943 | Val loss : 0.03583\n",
      "Iteration : 6800 | Train loss : 0.02759 | Val loss : 0.03385\n",
      "Iteration : 6900 | Train loss : 0.02762 | Val loss : 0.03535\n",
      "Iteration : 7000 | Train loss : 0.02789 | Val loss : 0.03405\n",
      "Iteration : 7100 | Train loss : 0.02685 | Val loss : 0.03433\n",
      "Iteration : 7200 | Train loss : 0.02762 | Val loss : 0.03315\n",
      "Iteration : 7300 | Train loss : 0.02748 | Val loss : 0.03349\n",
      "Iteration : 7400 | Train loss : 0.02501 | Val loss : 0.03344\n",
      "Iteration : 7500 | Train loss : 0.02848 | Val loss : 0.03334\n",
      "Iteration : 7600 | Train loss : 0.02531 | Val loss : 0.03189\n",
      "Iteration : 7700 | Train loss : 0.02661 | Val loss : 0.03340\n",
      "Iteration : 7800 | Train loss : 0.02503 | Val loss : 0.03220\n",
      "Iteration : 7900 | Train loss : 0.02762 | Val loss : 0.03168\n",
      "Iteration : 8000 | Train loss : 0.02699 | Val loss : 0.03429\n",
      "Iteration : 8100 | Train loss : 0.02546 | Val loss : 0.03200\n",
      "Iteration : 8200 | Train loss : 0.02369 | Val loss : 0.03164\n",
      "Iteration : 8300 | Train loss : 0.02628 | Val loss : 0.03093\n",
      "Iteration : 8400 | Train loss : 0.02602 | Val loss : 0.03091\n",
      "Iteration : 8500 | Train loss : 0.02555 | Val loss : 0.03066\n",
      "Iteration : 8600 | Train loss : 0.02815 | Val loss : 0.03162\n",
      "Iteration : 8700 | Train loss : 0.02458 | Val loss : 0.02962\n",
      "Iteration : 8800 | Train loss : 0.02497 | Val loss : 0.02863\n",
      "Iteration : 8900 | Train loss : 0.02371 | Val loss : 0.03093\n",
      "Iteration : 9000 | Train loss : 0.02694 | Val loss : 0.03080\n",
      "Iteration : 9100 | Train loss : 0.02649 | Val loss : 0.02909\n",
      "Iteration : 9200 | Train loss : 0.02491 | Val loss : 0.02933\n",
      "Iteration : 9300 | Train loss : 0.02695 | Val loss : 0.03043\n",
      "Iteration : 9400 | Train loss : 0.03033 | Val loss : 0.02952\n",
      "Iteration : 9500 | Train loss : 0.02510 | Val loss : 0.02950\n",
      "Iteration : 9600 | Train loss : 0.02583 | Val loss : 0.03047\n",
      "Iteration : 9700 | Train loss : 0.02554 | Val loss : 0.02965\n",
      "Iteration : 9800 | Train loss : 0.02426 | Val loss : 0.02916\n",
      "Iteration : 9900 | Train loss : 0.02395 | Val loss : 0.02919\n",
      "Iteration : 10000 | Train loss : 0.02661 | Val loss : 0.02989\n",
      "Iteration : 10100 | Train loss : 0.02518 | Val loss : 0.02983\n",
      "Iteration : 10200 | Train loss : 0.02503 | Val loss : 0.02901\n",
      "Iteration : 10300 | Train loss : 0.02593 | Val loss : 0.02857\n",
      "Iteration : 10400 | Train loss : 0.02649 | Val loss : 0.02845\n",
      "Iteration : 10500 | Train loss : 0.02419 | Val loss : 0.02809\n",
      "Iteration : 10600 | Train loss : 0.02332 | Val loss : 0.02721\n",
      "Iteration : 10700 | Train loss : 0.02399 | Val loss : 0.02903\n",
      "Iteration : 10800 | Train loss : 0.02378 | Val loss : 0.02801\n",
      "Iteration : 10900 | Train loss : 0.02473 | Val loss : 0.02877\n",
      "Iteration : 11000 | Train loss : 0.02851 | Val loss : 0.02913\n",
      "Iteration : 11100 | Train loss : 0.02408 | Val loss : 0.02724\n",
      "Iteration : 11200 | Train loss : 0.02626 | Val loss : 0.02844\n",
      "Iteration : 11300 | Train loss : 0.02392 | Val loss : 0.02695\n",
      "Iteration : 11400 | Train loss : 0.02585 | Val loss : 0.02684\n",
      "Iteration : 11500 | Train loss : 0.02547 | Val loss : 0.02752\n",
      "Iteration : 11600 | Train loss : 0.02460 | Val loss : 0.02807\n",
      "Iteration : 11700 | Train loss : 0.02625 | Val loss : 0.02735\n",
      "Iteration : 11800 | Train loss : 0.02553 | Val loss : 0.02741\n",
      "Iteration : 11900 | Train loss : 0.02503 | Val loss : 0.02602\n",
      "Iteration : 12000 | Train loss : 0.02593 | Val loss : 0.02624\n",
      "Iteration : 12100 | Train loss : 0.02723 | Val loss : 0.02623\n",
      "Iteration : 12200 | Train loss : 0.02612 | Val loss : 0.02693\n",
      "Iteration : 12300 | Train loss : 0.02694 | Val loss : 0.02810\n",
      "Iteration : 12400 | Train loss : 0.02497 | Val loss : 0.02755\n",
      "Iteration : 12500 | Train loss : 0.02521 | Val loss : 0.02828\n",
      "Iteration : 12600 | Train loss : 0.02433 | Val loss : 0.02700\n",
      "Iteration : 12700 | Train loss : 0.02416 | Val loss : 0.02761\n",
      "Iteration : 12800 | Train loss : 0.02430 | Val loss : 0.02612\n",
      "Iteration : 12900 | Train loss : 0.02487 | Val loss : 0.02761\n",
      "Iteration : 13000 | Train loss : 0.02496 | Val loss : 0.02655\n",
      "Iteration : 13100 | Train loss : 0.02427 | Val loss : 0.02730\n",
      "Iteration : 13200 | Train loss : 0.02517 | Val loss : 0.02711\n",
      "Iteration : 13300 | Train loss : 0.02339 | Val loss : 0.02578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 13400 | Train loss : 0.02559 | Val loss : 0.02626\n",
      "Iteration : 13500 | Train loss : 0.02383 | Val loss : 0.02801\n",
      "Iteration : 13600 | Train loss : 0.02307 | Val loss : 0.02566\n",
      "Iteration : 13700 | Train loss : 0.02491 | Val loss : 0.02622\n",
      "Iteration : 13800 | Train loss : 0.02230 | Val loss : 0.02616\n",
      "Iteration : 13900 | Train loss : 0.02707 | Val loss : 0.02450\n",
      "Iteration : 14000 | Train loss : 0.02323 | Val loss : 0.02620\n",
      "Iteration : 14100 | Train loss : 0.02252 | Val loss : 0.02475\n",
      "Iteration : 14200 | Train loss : 0.02422 | Val loss : 0.02740\n",
      "Iteration : 14300 | Train loss : 0.02430 | Val loss : 0.02622\n",
      "Iteration : 14400 | Train loss : 0.02543 | Val loss : 0.02655\n",
      "Iteration : 14500 | Train loss : 0.02447 | Val loss : 0.02568\n",
      "Iteration : 14600 | Train loss : 0.02557 | Val loss : 0.02714\n",
      "Iteration : 14700 | Train loss : 0.02478 | Val loss : 0.02680\n",
      "Iteration : 14800 | Train loss : 0.02473 | Val loss : 0.02658\n",
      "Iteration : 14900 | Train loss : 0.02333 | Val loss : 0.02660\n",
      "Iteration : 15000 | Train loss : 0.02295 | Val loss : 0.02643\n",
      "Iteration : 15100 | Train loss : 0.02549 | Val loss : 0.02543\n",
      "Iteration : 15200 | Train loss : 0.02522 | Val loss : 0.02590\n",
      "Iteration : 15300 | Train loss : 0.02595 | Val loss : 0.02562\n",
      "Iteration : 15400 | Train loss : 0.02523 | Val loss : 0.02619\n",
      "Iteration : 15500 | Train loss : 0.02454 | Val loss : 0.02810\n",
      "Iteration : 15600 | Train loss : 0.02564 | Val loss : 0.02471\n",
      "Iteration : 15700 | Train loss : 0.02470 | Val loss : 0.02624\n",
      "Iteration : 15800 | Train loss : 0.02624 | Val loss : 0.02631\n",
      "Iteration : 15900 | Train loss : 0.02427 | Val loss : 0.02724\n",
      "Iteration : 16000 | Train loss : 0.02341 | Val loss : 0.02717\n",
      "Iteration : 16100 | Train loss : 0.02366 | Val loss : 0.02710\n",
      "Iteration : 16200 | Train loss : 0.02464 | Val loss : 0.02605\n",
      "Iteration : 16300 | Train loss : 0.02536 | Val loss : 0.02635\n",
      "Iteration : 16400 | Train loss : 0.02358 | Val loss : 0.02503\n",
      "Iteration : 16500 | Train loss : 0.02375 | Val loss : 0.02615\n",
      "Iteration : 16600 | Train loss : 0.02300 | Val loss : 0.02629\n",
      "Iteration : 16700 | Train loss : 0.02522 | Val loss : 0.02573\n",
      "Iteration : 16800 | Train loss : 0.02353 | Val loss : 0.02703\n",
      "Iteration : 16900 | Train loss : 0.02553 | Val loss : 0.02611\n",
      "Iteration : 17000 | Train loss : 0.02362 | Val loss : 0.02484\n",
      "Iteration : 17100 | Train loss : 0.02407 | Val loss : 0.02572\n",
      "Iteration : 17200 | Train loss : 0.02463 | Val loss : 0.02594\n",
      "Iteration : 17300 | Train loss : 0.02373 | Val loss : 0.02538\n",
      "Iteration : 17400 | Train loss : 0.02380 | Val loss : 0.02564\n",
      "Iteration : 17500 | Train loss : 0.02626 | Val loss : 0.02655\n",
      "Iteration : 17600 | Train loss : 0.02295 | Val loss : 0.02765\n",
      "Iteration : 17700 | Train loss : 0.02308 | Val loss : 0.02728\n",
      "Iteration : 17800 | Train loss : 0.02421 | Val loss : 0.02604\n",
      "Iteration : 17900 | Train loss : 0.02461 | Val loss : 0.02607\n",
      "Iteration : 18000 | Train loss : 0.02331 | Val loss : 0.02541\n",
      "Iteration : 18100 | Train loss : 0.02557 | Val loss : 0.02620\n",
      "Iteration : 18200 | Train loss : 0.02335 | Val loss : 0.02527\n",
      "Iteration : 18300 | Train loss : 0.02137 | Val loss : 0.02795\n",
      "Iteration : 18400 | Train loss : 0.02245 | Val loss : 0.02594\n",
      "Iteration : 18500 | Train loss : 0.02755 | Val loss : 0.02557\n",
      "Iteration : 18600 | Train loss : 0.02499 | Val loss : 0.02498\n",
      "Iteration : 18700 | Train loss : 0.02374 | Val loss : 0.02573\n",
      "Iteration : 18800 | Train loss : 0.02632 | Val loss : 0.02637\n",
      "Iteration : 18900 | Train loss : 0.02452 | Val loss : 0.02502\n",
      "Iteration : 19000 | Train loss : 0.02577 | Val loss : 0.02628\n",
      "Iteration : 19100 | Train loss : 0.02444 | Val loss : 0.02670\n",
      "Iteration : 19200 | Train loss : 0.02574 | Val loss : 0.02530\n",
      "Iteration : 19300 | Train loss : 0.02390 | Val loss : 0.02585\n",
      "Iteration : 19400 | Train loss : 0.02241 | Val loss : 0.02678\n",
      "Iteration : 19500 | Train loss : 0.02232 | Val loss : 0.02526\n",
      "Iteration : 19600 | Train loss : 0.02246 | Val loss : 0.02536\n",
      "Iteration : 19700 | Train loss : 0.02354 | Val loss : 0.02703\n",
      "Iteration : 19800 | Train loss : 0.02515 | Val loss : 0.02597\n",
      "Iteration : 19900 | Train loss : 0.02302 | Val loss : 0.02535\n",
      "Iteration : 20000 | Train loss : 0.02302 | Val loss : 0.02523\n",
      "Iteration : 20100 | Train loss : 0.02390 | Val loss : 0.02751\n",
      "Iteration : 20200 | Train loss : 0.02316 | Val loss : 0.02626\n",
      "Iteration : 20300 | Train loss : 0.02284 | Val loss : 0.02573\n",
      "Iteration : 20400 | Train loss : 0.02310 | Val loss : 0.02630\n",
      "Iteration : 20500 | Train loss : 0.02384 | Val loss : 0.02580\n",
      "Iteration : 20600 | Train loss : 0.02381 | Val loss : 0.02566\n",
      "Iteration : 20700 | Train loss : 0.02476 | Val loss : 0.02640\n",
      "Iteration : 20800 | Train loss : 0.02427 | Val loss : 0.02485\n",
      "Iteration : 20900 | Train loss : 0.02296 | Val loss : 0.02546\n",
      "Iteration : 21000 | Train loss : 0.02380 | Val loss : 0.02550\n",
      "Iteration : 21100 | Train loss : 0.02397 | Val loss : 0.02453\n",
      "Iteration : 21200 | Train loss : 0.02334 | Val loss : 0.02685\n",
      "Iteration : 21300 | Train loss : 0.02476 | Val loss : 0.02556\n",
      "Iteration : 21400 | Train loss : 0.02401 | Val loss : 0.02584\n",
      "Iteration : 21500 | Train loss : 0.02567 | Val loss : 0.02627\n",
      "Iteration : 21600 | Train loss : 0.02416 | Val loss : 0.02682\n",
      "Iteration : 21700 | Train loss : 0.02265 | Val loss : 0.02517\n",
      "Iteration : 21800 | Train loss : 0.02371 | Val loss : 0.02573\n",
      "Iteration : 21900 | Train loss : 0.02450 | Val loss : 0.02593\n",
      "Iteration : 22000 | Train loss : 0.02071 | Val loss : 0.02642\n",
      "Iteration : 22100 | Train loss : 0.02281 | Val loss : 0.02644\n",
      "Iteration : 22200 | Train loss : 0.02291 | Val loss : 0.02658\n",
      "Iteration : 22300 | Train loss : 0.02270 | Val loss : 0.02517\n",
      "Iteration : 22400 | Train loss : 0.02306 | Val loss : 0.02682\n",
      "Iteration : 22500 | Train loss : 0.02394 | Val loss : 0.02594\n",
      "Iteration : 22600 | Train loss : 0.02329 | Val loss : 0.02512\n",
      "Iteration : 22700 | Train loss : 0.02451 | Val loss : 0.02645\n",
      "Iteration : 22800 | Train loss : 0.02296 | Val loss : 0.02516\n",
      "Iteration : 22900 | Train loss : 0.02427 | Val loss : 0.02592\n",
      "Iteration : 23000 | Train loss : 0.02346 | Val loss : 0.02615\n",
      "Iteration : 23100 | Train loss : 0.02374 | Val loss : 0.02728\n",
      "Iteration : 23200 | Train loss : 0.02462 | Val loss : 0.02808\n",
      "\n",
      "Early stopping - Minimum loss : 0.02312\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[1]\n",
    "n_y = Y_train.shape[1]\n",
    "D_in, H, D_out = n_x, 64, n_y\n",
    "\n",
    "epochs = 50000\n",
    "lr = 1e-3\n",
    "weight_decay = 0.\n",
    "lambda_1 = 1e-3\n",
    "lambda_2 = 1e-3\n",
    "p = 0.2\n",
    "patience = 3000\n",
    "\n",
    "model = FFNN(D_in, H, D_out, p)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "# Flag True if you are in the network optimization process.\n",
    "is_optimizing = True\n",
    "\n",
    "# If weight_NN exists and we are not in optimization mode just load\n",
    "# network weights and evaluate the model.\n",
    "if(os.path.isfile(input_folder + '/weights_NN') and is_optimizing==False):\n",
    "    model.load_state_dict(torch.load(input_folder + '/weights_NN'))\n",
    "    print(model.eval())\n",
    "else:\n",
    "    criterion = nn.MSELoss(reduction='mean') \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, amsgrad=True)\n",
    "    \n",
    "    # Initialize losses lists.\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    losses = []\n",
    "    W_1_hist = []\n",
    "    W_2_hist = []\n",
    "    W_3_hist = []\n",
    "    \n",
    "    idx = np.arange(len(X_train_b))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        curr_loss = 0\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        model.train()\n",
    "        for batch_num in idx:\n",
    "            \n",
    "            # Create torch variables, required dtype 'float32' no 'float64'\n",
    "            batch_x = Variable(torch.from_numpy(X_train_b[batch_num].astype('float32')))\n",
    "            batch_y = Variable(torch.from_numpy(Y_train_b[batch_num].astype('float32')))\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(batch_x)\n",
    "            \n",
    "            # L1 and L2 regularization\n",
    "            l1_reg = None\n",
    "            l2_reg = None\n",
    "            for W in model.parameters():\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = W.norm(1)\n",
    "                    l2_reg = 0.5* W.norm(2) ** 2\n",
    "                else:\n",
    "                    l1_reg = l1_reg + W.norm(1)\n",
    "                    l2_reg = l2_reg + 0.5 * W.norm(2) ** 2\n",
    "                \n",
    "            # Compute loss\n",
    "            batch_loss = 1 / batch_x.shape[0] * ((y_pred - batch_y).pow(2).sum() + l1_reg * lambda_1 + l2_reg * lambda_2)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += batch_loss\n",
    "        train_loss.append(curr_loss.item() / len(idx))\n",
    "        \n",
    "        # Save norm of the weights\n",
    "        W_1_hist.append(np.linalg.norm(model.W_1.detach().numpy()))\n",
    "        W_2_hist.append(np.linalg.norm(model.W_2.detach().numpy()))\n",
    "        #W_3_hist.append(np.linalg.norm(model.W_3.detach().numpy()))\n",
    "        \n",
    "        model.eval()\n",
    "        val_x = Variable(torch.from_numpy(X_val.astype('float32')))\n",
    "        val_y = Variable(torch.from_numpy(Y_val.astype('float32')))\n",
    "        val_pred = model(val_x)\n",
    "        loss_val = 1 / val_x.shape[0] * ((val_y - val_pred).pow(2).sum() + l1_reg * lambda_1 + l2_reg * lambda_2)\n",
    "            \n",
    "        val_loss.append(loss_val.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Iteration : {:04d} | Train loss : {:.5f} | Val loss : {:.5f}\".format(epoch, train_loss[epoch], val_loss[epoch]))\n",
    "        \n",
    "        early_stopping(val_loss[epoch], model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"\\nEarly stopping - Minimum loss : {:.5f}\".format(early_stopping.val_loss_min))\n",
    "            break\n",
    "    \n",
    "    if not early_stopping.early_stop:\n",
    "        torch.save(model.state_dict(), input_folder + '/weights_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text(0.5, 0, 'Epoch'), Text(0, 0.5, 'Weights norm'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU1d3H8e9JwhZWWQQEkR0RERBEBap1q63boz5a3Ip1w621Vq2tWJdWxbaPWq3WFbVV1GrduqhYrFiqoAICIio7CAKyyb5kO88fJ+MkZBImydw5d+79vF+vvO6SydxfroPfnHvPPcdYawUAAMIjz3cBAACgMsIZAICQIZwBAAgZwhkAgJAhnAEACBnCGQCAkCnwXUBC27ZtbdeuXX2XAQBA1syYMWOdtbbd7vtDE85du3bV9OnTfZcBAEDWGGOWpdrPZW0AAEKGcAYAIGQIZwAAQiY095wBAPFWXFysFStWaOfOnb5LybjGjRurc+fOatCgQVqvJ5wBAKGwYsUKNW/eXF27dpUxxnc5GWOt1fr167VixQp169YtrZ/hsjYAIBR27typNm3aRCqYJckYozZt2tTqigDhDAAIjagFc0Jtfy/CGQCADFi4cKFKSkoy8l6EMwAAkh544AF16NBBM2bMkCTNmjVLY8eOlSStXbtWo0aN0kMPPVTl5z7//HPdfffdGjp0qLZu3ZqRWghnAAAkXXHFFZKkAQMGSJJeffVVPf/885Kkdu3aqWPHjrrsssuq/Fy3bt107bXXqkWLFhmrhd7aAIDwufpqadaszL7nwIHSvfdW++28vDwNGjRIH3/8sfr166fWrVurRYsWmjNnjvr376+WLVumvHfcqFGjzNYpwhkAgG+MGDFCU6dO1YIFC3T66aercePGGj9+vEaNGqXBgwdnrQ7CGQAQPjW0cIM0YsQIPfrooxowYIBGjhyp73//+xoyZIg6deqkSy65JGt1cM8ZAIByhxxyiN5880316tVLktSqVSsNHDhQU6dOVZMmTbJWB+EMAEC5wsJC9e/fXyeeeOI3+8466ywNGjSo2p8pLS3Viy++qPXr1+v555/Xli1b6l2HsdbW+00yYciQIZb5nAEgvj777DP17dvXdxmBSfX7GWNmWGuH7P5a7jkDAJCmiy++uMq+cePGZfw4hHMdzZ8v9ekjHXKI9Oc/S1OmSBdeKEV05DkAgIIJ4lS451wH06e7YJakadOkAw6QLr5YysuTQnKXAACQwwjnNBUVSc88Iy1a5FrL1fnpT7NXEwAgmgjnNDzxhNSokXTeeVLPnsn927dL11wjlZRIc+a4fffdJ40c6adOAEA0BBrOxphWxphbjDGjjDH3GmMaBHm8IHz5pXTRRVX3T54sNWki3X23lJ8vHXig9IMfuO+98II0c2Z26wQAREfQLefrJb1trX1K0peSzg34eBm1bp3UuXNye/RoaetWd1/5W9+q+vqnnpKOP96tX3dddmoEAERP0OE8UNL68vUvJFW6W2uMGW2MmW6Mmb527dqAS6m9du2S62Vl0iOPSE2b1vwzEya45dtvS199FVxtAIDoCjqcl0pKDLMyTFKlvszW2kettUOstUPaVUzCEBg/Prm+dWvdHpHq0CFz9QAAglXX+Zzff/99DR8+XH379tWVV16psrKyetcSdDj/WtKhxpg/SmotaU7Ax8uIsrLk/eMJE/bcWt5dcXFyneeeASA31HU+55kzZ+qdd97RrFmz9O677+rdd9+tdy2BDkJirV0t6QxjTJ6kNyQ9H+TxMuWcc5LriXvItVFQII0dK40Z47bff1867LDM1AYAceBhOuc6z+d84YUXqkED19+5R48eatmyZb1rDXyEMGNMgaSbJN1hrd0Y9PHq66mnpPI/lPTGG3V/nxtukA4+WPrud6Urr5TKr5IAAEKsLvM5N2rUSJK0YsUKtWzZ8puWd30EGs7GmPaSTpc0zlq7PMhjZcr55yfXv/vd+r1XotX90UfuUnkeT5UDQFo8Tedc5/mcd+7cqQcffFAPPvhgRuoINC6stV9Zax/KlWAePjy5nqlhOI8+2i2HDcvM+wEAglOX+ZyLi4v1hz/8QTfeeGPG5nymLVfBlCluuWRJ5t7zb39zyw8+kFatytz7AgAyry7zOY8ePVr33XefBgwYoO7du9fYwk4Xs1KV27bNLYcOlbp2zdz7NmsmXXqpe0b6uuvc+NwAgPCaNGlSpe0zzjijxtc/+eSTlbZtBi69Es7lJk50y8svz/x7P/ywC+dnn02O0w0AyD3pzOecqkd3bRHO5W6/3S0rPkYVhC5dGDkMAHIV8zlnUWlp8lGnhg2DOcbG8ofI1qyRduwI5hgAgGggnCUtXeqWdRlwJF0Vn0lv2za44wAAch/hLOmzz9zy5puDPc6HH7rl9u3JDmgAAOyOcJZ08slu2bdvsMc55JDkPe1mzTL3LDUAIFpiH86lpcn1vfYK/ngVB49JMbkJAACEc2LAkVNOyc7xWraUbr3VrV95Ja1nAEBVsQ/nFSvc8qqrsnfMW25Jrt9zT/aOCwCoXl3nc169evU3Y3B///vfV0lJSb1riX04L1rklpkcFSwdCxa45W9+k93jAgBSq+t8zlOnTtWTTz6padOmafXq1Zo9e3a9a4n9ICTz5rlnm7Mdzj17SoMHu+erR4+WHn00u8cHgDC7+uqrNSvDEzoPHDhQ99Yw3VVd53M+7bTTvlnv3r27+vXrV+9aYx/OixZJ3btL+fnZP/ZTT0n9+kmPPUY4A0AY1GU+Z0maMmWKXnvtNe3atUsbNmzQPvvsU686Yh/OX34pde7s59gHHJBcb9zYPf/MnM8AoBpbuEGq63zOw4YN07Bhw3Tffffprrvu0j317FAU+yj48kupUyd/x090SNu1SzrzTH91AADqNp/zW2+99c16aWmpBg4cWO86Yt1yLi11cyz7DOdOnaRXX5VOPVV6+WV3iX3xYn/1AECcVTef8+Ia/sf82Wef6fXXX1ePHj3UvHlzjRo1qt51xDqcn3/eBbSP+80V/c//SPfdJ/3kJ+656/nzpd69/dYEAHFV2/mcf/zjH2e8hliH86uvumW2BiCpyVVXueefN26U+vRhcBIACKN05nPOhFiH87p10uGHS0OG+K7EWb8+2YofN05K8RkAAHjEfM5ZsGCBVH7PPxTy8qSXXnLrl1ziwhoAED+xDecdO1xP6Z49fVdS2emnu2CWmPcZQPzYiN7Tq+3vFdtwTnS8C1PLOeGRR5Lrn3/urw4AyKbGjRtr/fr1kQtoa63Wr1+vxo0bp/0zsb3n/MUXbtmli986UjFGWrjQter79nUB3aeP76oAIFidO3fWihUrtHbtWt+lZFzjxo3VuRYjXsU2nFetcst6jrAWmB49kuv77y+VlPh/5AsAgtSgQQN169bNdxmhENvL2olw7tDBbx012bQpuV5QIF1/vb9aAADZE9twXrlS2msvN6Z1WLVokZzSUpL+7/+km27yVw8AIDtiG86rVkkdO/quYs+6d0+Ovy1Jt98uffWVv3oAAMGLbTivXBne+82769TJjRiWeOyrQweegQaAKIttOK9Y4W+qyLpasCC53rZt5UeuAADREctwLilxl7VzLZwl6V//Sq5fdpl77CqCTx0AQKzFMpxXr5bKynIznI87ruqkGHvv7acWAEAwYhnOS5e6ZS6Gc8LureVzz/VTBwAg82Idzrnc4mzb1rX+581z288+K23Z4vYBAHJbLMN582a33Hdfv3XUlzFS797J7RYt3Chi69b5qwkAUH+xDOfEY0itW/utI1OmTau8nQvPbwMAqhfbcG7eXGrY0HclmTFkiLu8/Y9/uO2SEmngQL81AQDqLpbhvHJl9FqXvXtLJ53kRhCTpNmzpYce8lsTAKBuYhnOX36Z2z21a3LjjdItt7j1K66QHn/cbz0AgNqLZTivWye1a+e7iuDceqs0aJBbv/hi6eCDqz4bDQAIr1iG8/r1Ups2vqsI1kcfSRdc4NZnzpTy8lzvbkIaAMIvduFcWipt2BD9cJakJ56Q3n238r68PAIaAMIuduG8caMLp7ZtfVeSHcOHu3vsZ5yR3JeX53qrP/aYv7oAANWLXTgnBuiIQ8s5YZ99pL/+VfrnP5P7tm6VRo+Wfv1rf3UBAFIr8F1AtiUGIIlLy7miE090c0GvXp3cd8st0s6d0tixbnvqVDeC2tSp0po10oMP+qkVAOIstuEcp5ZzRatWuXvuLVtKBeX/9e+8032l8tBD3KMGgGzjsnYMtW7txuC2VhozZs+vN8Z9TZgQfG0AgBiGc5wva6dyxx3S3//u1r/1LdebvbjYBffs2ZVf+73vSdu2Zb9GAIib2IXzunVSgwZSs2a+KwmPk092YTx5suvJnbjcfdBB0scfS9dem3ztT37ip0YAiJPYhXNiABJjfFeSG/r3l+66S3rzTbf9+OOupzcAIDixDGcuadfed74jnXuuW2/ePDnBBgAg82IXzuvWxbszWH089VRy/aab3NWH733PXz0AEFWxC2daznWXGPqz4oxeEya4iTYAAJkTu3Cm5Vx/y5dLa9cmt3/1K9eKfvVVfzUBQJTEKpytjc+kF0Fr21YqKqq877TTpMWL/dQDAFESq3DeskUqKXGDcKD+GjRwf/BUDOQePVwrescOf3UBQK6LVThv2OCWtJwzq1s3N3BJRYWF0tKlXsoBgJwXy3Cm5Zx5BQVSWZm7tJ3QrZtrRTM2NwDUDuGMjDFGevll6fPPK+/v399PPQCQqwhnZFyfPm4UsUQreu5cNy0lACA9hDMC0bSpa0U/9pjbbtLETaoBANgzwhmBuuCC5Pqf/+yvDgDIJbEL58JCqVEj35XER36+NG+eW7/ooqq9ugEAVQUazsaYFsaYXxpjzjHG3GOM8RqLX38t7bWXzwriqXdvd5lbkho2lKZMcT27AQCpBd1yPl/SR9baZyUtk9Q34OPViHD2Z82a5Prw4a5FbYz7Wr7cX10AEEZBh/M8SdcaY/aVtEvSxwEfr0YbN0qtWvmsIL4KC6Vt21J/r0uX7NYCAGEXdDhPkvSOpF9KOlFSYcVvGmNGG2OmG2Omr604k0JAaDn7VVjoBiRZtcrNDlbRuHF+agKAMAo6nG+TNN5ae6mkKZIuqPhNa+2j1toh1toh7dq1C7gUF860nP3r0MH1mLdWmjnT7bvkEmn+fL91AUBYBB3OPSVtLV9fLGlVwMer0YYNtJzDZuBA6cIL3XqfPn5rAYCwCDqcfyPpZmPMDyU1s9a+GPDxqlVW5malouUcPo8/nlwfMKDqVJQAEDcFQb65tXa6pOlBHiNdic5ILVr4rQOprVkj7b239PHH7jn00lIpL1ZP4QNAUmz+97d5s1s2b+63DqTWrp0L5oT8fGnsWCkL/QQBIHRiE85btrglLefw6t9f2rQpuX3jja41bYz00EP+6gKAbItNONNyzg0tWkgzZlTdf8UVzAsNID5iF860nMPv4INdEFvrpp5MyMtzreiSEn+1AUA2xCacuaydm5o2lWbPrryvQQNp+3amoAQQXbEJZy5r566DDnKDlbz1VnJf06ZSQYE0daq/ugAgKIE+ShUmtJxz28CBbllS4kI5Ydgwt5w7V2rZUurUKfu1AUCmRbLl/PXXUq9e0lNPVd4nEc65Lj/fPRN97rmV9/frJ3Xu7L4PALkukuFsrbRwoZuFKmHDBndJu2FDf3UhM9q1k8aPd6O+3X9/5e+VlSX/EAOAXBXJcE6MLFVWlty3YYObbAHRYYz0ox+5P8aWLpXuusvt79BBKi72WhoA1EtswpnpIqNtv/2kq65y60VF7grJc8/5rQkA6opwRmQ0aFC59/Y551T+DABArohNOG/cyIxUcXDYYdL69cnt/Hzps8/81QMAdUE4I3Jat67cGfCAA6RJk/zVAwC1FZtw5rJ2vLRsKb3ySnL76KOlJk0qT6wBAGEVi3AuLnbzOdNyjpdTT608WcbOndL55/urBwDSFYtwTrSWCOd4qvhY1d/+5h7BMqbypBoAECaRDGdj3DIRzolBKbisHU8FBa4FvfujVc2bu8/Kzp1+6gKA6kQ2nI1JhnOicxAt53g76yxp2jT3THRFTZowVzSAcIlkOEvu0vbuLWfCGUOGuNHEdp8TOi9PmjzZS0kAUEUswnnDBrdk+E4k5Oe71vKqVcl9Rx4pvf66v5oAICFW4dymjb96EE4dOkjvvpvcvvNOf7UAQEKswpkOYUhl+PDkZ+Tdd6UDD3TjcxcVSTffzBCgALIvFuG8caNUWMh0kajeXntJd9zh1ufOlRo1cl+33eYugc+Z47c+APESi3Devl1q2tRvPQi/MWOk3/8+9fcOOsg9AbBwYXZrAhBPsQnnwkK/9SA3XH216yj20EPSoYdWnRe6Vy/pwgu51A0gWLEI523bCGfUzmWXSe+/nxzA5O23k9978kl3qbvi2N0AkEmxCGdazqivo45yIT1sWHLf6aczeAmAYEQ6nEtL3TrhjEx57z33lZCXlxyNrqSEsAaQGZEO54qXtekQhkwZNqzqvej8fKlBAzd3NAENoL5iEc5btxLOyKyCAumLL1wP74o+/9x99hJDxgJAXUQ2nPPzKz/nzAAkyLR993XPRlvrpiU99tjk94YMSU5VCgC1Fdlwrthy3rRJatnSbz2IthYtpIkTpTVr3PbixW6ilY8/9lsXgNwU+XAuLXUdwlq08F0R4qBdO+mCC5LbAwZIb73lrx4AuSny4bxli9tu3txvPYiPxx+XZs6UGjd228cd57ceALknNuFMyxnZYow0cKC0Y0dy3+23S/PmJR/vA4CaRD6cN29227Sc4UNiZLGbbpL239/18j7mGL81AQi/yIczLWf4dNRR0rnnVt739tvS00/7qQdAbohNODdr5rcexNf48e5xq+JiqUcPt2/UKHf52xjpo4+k5csZvARAUuTDeetWt81lbfhWUJB6ysnBg6UuXdxntn//7NcFIHxiE860nBEW1rrP5q5dVb/3ySeuNc3z0UC8Ec6AB8ZIDRu6z+jXX7vA/t73kt8fMMC9Zv58fzUC8IdwBjwyxo0kJkmvv+5CuaI+fdxrnnwy+7UB8CcW4WyM1KSJ74qAPZs1y7WilyypvP/CC6XXXpN++UupqMhPbQCyJxbh3LSp2wZyRdeuLqRXr07uO+kkN9FGo0bS9OneSgOQBZGNrIrhzCVt5Kr27VM/YnXIIdJ112W/HgDZQTgDOSDRwzsx4p0k3X23u2VTXOyvLgDBiEU4N23quxqg/oxxz+tbK91wQ3J/w4bMHQ1EDeEM5KCxY6U5c5LbrVpJEya4AP/Tn7yVBSBDIh/O27ZxWRvRdOCB0oYNye3Ec9IXXJAcGtQYac0aP/UBqLtYhDMtZ0TVXntJJSU1v6Z9+8r3qgGEH+EM5Lj8/GSHMWulF16Q9tlHOvvs5Gtatky2pF980V+tANIT6XAuLeWeM+LDGLc880zpyy+lZ59N3ZP7zDOTs7XNm5ccRQ9AeEQ2nBs0cP9jouWMOCsocGN3jx9feX+LFi7M99/f9QC/4w4/9QFILdLhvGuXtH074Yx4a9VKOvdcd8l7wYLUr/nlL11YN2okrV2b3foAVBXZcG7YMNkJht7agNOzpwvp115z22+8Ufn7RUXS3ntX7u39xhvS0qVZLxWItciGc4MG0saNbp2WM1DZCSe4kP7ud91y5cqaX9utmwvqW29N7i8rC7xMILYiHc6JTi+EM1Czjh1dSCd6fb/3nvTTn1Z93a9+lWxR5+cn17/+OvX7FhVJnTsnX7d9e7C/BxAVkQ7nBMIZSJ8x0rBh0j33JAP73Xdr/pnWraWnn5b+8hc3lGhpqfu5Ro1cz/GEpk1dwO/p2Wwg7iIbzg0bJtcJZ6B+hg93YVtxtLE33nCTbySMGuWerW7VyvUSrzhN68iRyfVbb5VOPVX66KPKl8atdX8YjBoV2K8B5IzIhjMtZyDz2rVLtqa/+13pmmuku+6q+WcWLnQt6qKi5L7XXpMGD05eGt+xQzr/fPe9p5+WHn1U+t//rTw8qRcbNyYfFi8ulk47zRV8wAFSjx6Ve861aiVNmuR+ZtMm90u99Zb0+uvS++9Xfm3Frx//WBowQBo0qPrXpPr61reS65Mmub+c7rorue/SS91fQLfcUvP7tGmTXO/Rw3VA2LBB+r//k1askJYsSV4+STwUv3WrdOONyZ87/3zpwQddq+iYY6Sf/Uz697/dfYyf/MR97x//cD0L58yRXnlFmjhRWrTInatVq1L/x965M7adG4xNNVmsB0OGDLHTMziD/PXXu8+W5CamHzw4Y28NoAalpdKdd0o33STNmCEdfHDye9ZKl18uPfJI/Y5RUuKCvZLEXw2LFrm/yNu2dSGycqUb51RyN9MrNuOBAw+UPvvMfXDTMXWqdNhhGTu8MWaGtXbI7vsj23LmsjbgR36+e27a2srBLLlG1sMPS++8I02ZIh1xROXvX3ppescoKCjvYGYKZROtt7w8d/DevaVOndwN7zZtpP79Xa+0zp2DCeaTT07/tT/9qWvdvvmmdNZZqV/Tpo1rmf/wh9K4ce6vmT/9SbrwQtdarahPn9TvkSo8pk51xx050rVmJTe9WYsWbv1nP0v/94iSTz5JP5glN1h9NlhrQ/E1ePBgm0m33pr4M9raL77I6FsDSFi2zNri4sr7ysoqb5eWWltUZO3q1dYuWGDt5MnWLllibUmJtb//vS2VsXNO+oW1ffsm/9FKtkyyH2qIfVmnfrN7jG6v+JJvvs7U83aXGnyzfbL+Zqfq0EqveUwX2U+1v9t4+WVr33/f2mOPtfYXv7D2sMOsbd/efe/JJ61dvjxbZxAV7f7Z2f17q1dbu2OH++wUFbnlnt5j505rN2506yUl7vtbtlg7dqy1+fnuv/nZZ1v73HPue7t2WbtokbULF1r75ZfutStWWDtnTs311ZGk6TZFJkb2svbYsck/Mtevd71JAVSQyK28FBfQEv9fWL9e6tcvVPNOblRL7aWN9XqPt9+Wjjyy8q9eXJzsqzJrlnu2u2XLeh0G2CMvl7WNMQXGJIbjz64mTZLrXNZGaFnrrvE++KC75PjII9KECdIf/1i5087bb7t955wjzZ/vLtWeeWayU9JJJ0mHHy5ddJE0enRy/+23SyNGuJ5cXbtWfs/EZeBUnYTy8txXu3bZDebEQN9FRW5A8MQzWRW+WtmN3wycctBBe/73/Yc/VN139NHuV09cdu/Y0d0KS/z6gwa5/l1PP1355z7/3PVrimkfJWRRoC1nY8wtks6XlPgob7PWDkj12ky3nMeNky65xK2XlSVn7AGyKjGP4y235PbsEjfd5HofX3SRu6e7c6fUuHHqVrfkevl+8IHrcp0Fn37qbim3aOE6Ax90kNu/ZUty+N7iYtdXrG/f2r33f//rft0pU9x/xoSdO91tbcl10G7SJLkNpKu6lnNBwMddbK3tXl5Af0n7BXy8bzRvnlwnmBG4KVPcw8C54N57pf/5H9eSltwfEGVlyVZ0OgoLa/5+ogNWlhxwQHK9f//kVfmKGjRws3BZK61bJ51yiusjlfDGG9LcuS6MBw50g6VI7omlVBo3rrrvrrtcv6pf/MLdWttd4kJAQdD/50XOC7rl3NRau618/dfW2pure22mW85vvOHGBJZS/0MF6mzZMvesUG2fBxoxQnr11eSNzUQvWXjzox+5uwXvvedGRauoqKhqS7hHD/fUTcWnQdL1hz9IV11Ved+aNe7OAeLLyz3nCsF8mqR3UhQ12hgz3RgzfW2G56mr2HIG6mTHDmnmTPeXXsX7sV27pg7moUNdk2nXrlQdil2TrE0bF8oEcyg88ID7T7N7MEsugK11l6xfecVdXFi40P1t9cEH7jWJVvW0aXs+1u7BLLkZwJo1k15+ufL4IcZUvle+c6d0/PHShx/W/ndEbgq8t7YxpomkR621P6jpdZluOX/8sRt0R6LljFoqKZG6d5eWL9/zaxnhBrtJDEN65JHu6v9vfuMukyds3er66f3mN3V7/9/+1g2yVFrq+gfOnevus8+YIR1yiBuU7Jhjqv5caan7aHNfPFyqazlnI5xvk/Qfa+1bNb0u0+G8ZIn7/2ufPq6HJVBFUZFrARcWShdfvOfXH3aY9MQTte9RBKRQ3VNsY8dKY8bU//137HD3xd97z413smJF8ntnny0995xbnz072YGuNhYvdh3xEj3beSqmbnw9SrW/pBP2FMxB6NJFuuIK6Zlnsn1khIa10uTJrqnQvXvVx4UaNXLXGqsL5hdfrHxZeupUghkZY0zyozVpkrsIU1ws3XCD2zdrlgu/0aPdvWlrXWf5dDVp4oJ3xIjKwSwlg1lyVxgr/rOYONEtW7RIDhM+f37ln1+82N1/P/lk1++vWbPkz2/blvy9PvnEvTbx6Jm1bjvh44+lefNqd97iIugOYab8GHt8KjDTLWdEUOJ64fjxrslx7rmZP8Zll0m33ebGZQZC6J//dPe+27SRTjzRTRJyww0uFAsL3fwUqXqYDx8uXXedazXv3Fn348+d68alqY+rr3ZDtSb+1i0sdJf7031YIPG/goq+/rrqYFNFRe4hhNLSZD/MNWvcdseO6R0rMe9JxcmUMqm6lrP3YTsTX5kevhM5ats2NyTkBx9Ye/zxqbpV1e/rd79zw/Nt2uT7NwUC8/DDlT/2O3akfl1ZmbVHH21t167WPvFE7f85bd/uRrdcssTap57KzD/RV191td1wg9u+7DJ3jJIS97X76//977od58gj3eidbdpU/d6117rl3LnJfRddZG3v3tauX5/Z/1aK2/CdyCHr1rlus//6V+UHT9PRrJl07bXuQdcBA9wAGTzYDqioyM3Y2KpV+j+zaZO7T92okbR6tXtsrFMn90/qzTfd7JYJ1UVHUZEb/OWyy1yXjnnz3EhsX3zhOqS98IKb0zthzJjUz4TXx6uvujnDg/DOO66zX6bQcka4XHppen/ejhlj7XvvWfvVV24CBQDelJVZe9BBrpVcHyUl1r79dnL7gQdq1+qdONHaLl0q77v66soXxMrKrL3ySmv//ndrV61yr9l//+QFtLq0tkeMyPzcF6LlDKdBvvcAABkxSURBVC+Ki92f0tOnu7EP//Of6l/bvr17XRZHlgIQHta6C2Bz5rjtbdukr75y03G3bl39yGuZOvbuF93+9S83ZP327W7soaFDM39cX8N3Iq6ee849hLkn996bnFsWQKwZ43pwV9Stm1sGPdlIqrth3/mOWzZvnr1pnBMIZ2SOtdL551edyqeil1+WTjstezUBQA4inFF/y5YlJ1GoaMQIN2QlAKBWAh2EBBE3a1ZyrOmKTj7ZjS9NMANAndByRu299JJ0xhmV9zVrJm3ezGNMAJABdQpnY8x+1tplmS4GIZcqeE84wQ1ZRCgDQMakFc7GmO9I+qGkRnKXwjtJCqBTOULpqadcR6/dzZ8v9eqV/XoAIOLSbTnfL+lmSV9JCseD0QietW7C2XXrkvuWLEnd+QsAkDHpdgibYK193lr7jrX2P5KWBFkUQuCOO9zkEolgPvtsF9YEMwAELt2W83+NMf+VtLZ8u7O4rB1NGze64XgqmjBBOv54P/UAQAylG86/lHSnpNXl2yOCKQdeFRVVDuY773Tj5QEAsirdcP63tfb5xIYxZmFA9cCXLl2k5cuT2wsXutnUAQBZl244dzTGTJC0Sq5DWDdJRwVWFbLnrbek445Lbg8Y4AYXAQB4k244z5X0boVtLmvnOmtdh6+KJk6Ujj3WTz0AgG+kG86Fkr6w1i6RJGPM5OBKQuDKyqT8/Kr7GEgEAEIh3UepuktaWWF7cAC1IBt27qwczGvWpJ7IFADgTbrhbCTNM8a8bYyZJOmpAGtCUG69VWrSJLm9ebPUrp23cgAAqaXdW1vSzytsfyeAWhCk3VvGXMYGgNBKq+VsrX1M0r6SjpNUWL6NXFBSUjmEL72Uy9gAEHLpTnxxk9yjUx9JGmqM+Ze19sVAK0P9WSs1aJDc3r698mVtAEAopXtZu9Rae3RiwxgzMqB6kClFRVKjRsnt0tKqj04BAEIp3f9b7z5388GZLgQZtHFj5WDesYNgBoAckm7LWcaYZyTtkHSIpFcDqwj1U1ZWeXzsTZukxo391QMAqLW0wtla+4wx5kNJAyTdq6otaYRBSUnle8yWqbcBIBel2yGsvaRvS2osaR9JgyRdEFxZqLXiYqlhw+R2aam/WgAA9ZLuZe1/SJqh5JSRSwOpBnVjrdS5c3Kbe8wAkNPSDef51trLA60EdVcxiIuKKl/aBgDknHTD+WNjzFlKjq89wlo7NqCaUBsDBybX588nmAEgAtIN56GS+lbY7iqJcPbtBz+QZs9265s3S82b+60HAJAR6Ybz9dbaxYkNY0znml6MLPjjH6Xx49368uUEMwBESLpjay/ebXtFMOUgLb/+tfSjH7n1W26p3BkMAJDz0h6EBCGxcaMLZEl67DHp4ov91gMAyLi0Ws7GmL7GmI7GmMONMXcZY7oEXRhSKClJjv71u98RzAAQUek+DHuqpI2S7pf0d0knBlYRqlexJ/bPfuavDgBAoNK9rD1HUmtJRdbaycaYlgHWhFSuuSa5zuhfABBp6bacO0p6SdL9xphDJJ0UXEmoYuZM6fe/d+uzZzP6FwBEXLot50XW2sMSG8YYmm7ZUlYmHVw+Q+e4cdJBB/mtBwAQuBrD2RgzWFJzSd83xpSU786XdLuk4QHXBknKz0+uX3SRvzoAAFmzp5Zze0lnSuojqVH5Pivp+SCLQrmbb06ul5X5qwMAkFU1hrO19nVJrxtj2ltrv0rsN8a0CryyuFuxQrrtNre+YYNkjN96AABZk+495ybGmJ8r2XruJ2lkMCVBkrTvvm55/fXJZ5sBALGQbrffFyW1lLRJ0mZJswOrCJVbyb/9rb86AABepNtynm2tHZPYMMYw7GdQ1q9Prn/1VfWvAwBEVsqQNcY0lXRMhV3LjDFXSVpdvj1U0nUB1xZPbdu65dNPS3vv7bcWAIAX1bWAd0kaI+mzar7fPZhyYu6ee5Lr553nrw4AgFcpw9laW2KMOc9au1CSjDE9rLWLEt83xnQu77G9yVprs1RrtJWVSdde69a3bvVbCwDAq2o7hCWCudyPjTGnGWM6lG+fL+mfkp4zxgwIssDYOKb8LsLjj0tNm/qtBQDgVbodu46WC/LhxpjxkoZJOspaW1x+L5re2/WxebP0zjtu/eyzvZYCAPAv3XC+31r7mCQZY86VlGetLS7/3tpAKouTwYPd8vzzpSZN/NYCAPAu3XBuZoy5XFIXSZ0ldTPGHC1puqQBkp4LqL7oe+staWH5HYQnn/RbCwAgFNIdhOQBucFHlkq6SG7Si66SnhDjbNddaal03HFufeJEhugEAEhKs+Vcfgn7mcS2MaadtfYJuXBGXRVUOP3HHuuvDgBAqOxpysiXJF0g6XpJJyR2S2otab9gS4u4Tz5Jrm/e7K8OAEDo7KnlPMZau9kYM1fSnyQlOoEdGWhVcTBsmFt+/rnUvLnfWgAAoVLjPWdr7bzy1ZclHS6pn7V2maRJQRcWaf/9r7Rli3TUUVKfPr6rAQCETLodwh6R1FvSAeXbZwVTTkwccYRbPvus3zoAAKGUbjj/y1p7k6T55du9Aqon+l56yS3PPlvq0KHm1wIAYind55z3McYcL2l/Y8w+ojNY3VgrnXGGW7/7br+1AABCK92W86OSvl3+1U/SqIDqibbEZewjjpA6dvRbCwAgtPb0KNWl1tpHrLWbJd2QpZqiKzEN5Msv+60DABBqe7qsPcwY01XSJkkzJE221u6q7UGMMd3kHr96uTzo4+eFF5Lrbdr4qwMAEHp7CudrrbXrJMkYc4CkS4wxh0uaaa29K50DGGOOkHSMpFtjO/dzaak0cqRbnz+/5tcCAGKvxnCuEMz9JP1A0v9KmiVpcTpvboxpJmmMpBNiG8yS9OKLbjlwoNSLju4AgJrt6Z7zDZK+LzfpxdOShlhrN9Xi/UdJWiPpbmNMZ0nXWGuX17XYnHXxxVKrVtK0ab4rAQDkgD311n5d0quSXpT0ei2DWZIOlPSltfankqZIuqXiN40xo40x040x09eujei00I88Im3d6u4zF6T75BoAIM72dFl7tqTZxpiGkk41xnSRtEDSa9bakjTev1iuI5kkTZR01G7v/6jcY1oaMmRINC97X3aZW77yit86AAA5o8aWszFmkDEmT27YzraSBkoaJ+nNNN9/mqQe5esdJL1fxzpz0+LyW/MjR0r9+/utBQCQM9K5rL1C0h2SWsm1crtYa49J8/2fl9TeGHO+pEMl3VvXQnNSj/K/SxgNDABQC3u6CXq7pAfr2tPaWlss6Zq6/GzOW7cuud6pk786AAA5Z0/3nP+YrUIiJzHz1KxZfusAAOScdMfWRm1YK332mVsfMMBvLQCAnEM4B+Gf/3TLxFjaAADUAuGcadZKp5zi1seN81sLACAnEc6Z9t//uuXQoVKjRn5rAQDkJMI50/7yF7d87TW/dQAAchbhnEklJdJLL7lBR9q29V0NACBHEc6ZNHmytGaNdOaZvisBAOQwwjmTjikfOO173/NbBwAgpxHOmbJwYXK9sNBfHQCAnEc4Z8rDD7sls08BAOqJcM6Uu+928zWfeqrvSgAAOY5wzoQ33nDLiy7yWwcAIBII50w44wy3/OEPvZYBAIgGwrm+rJW2b3frhx3mtxYAQCQQzvWVuKRNqxkAkCGEc3394hduedddfusAAEQG4VwfpaXSnDluvU0bv7UAACKDcK6Pd95xy3vv9VoGACBaCOf6+MtfpGbNpNGjfVcCAIgQwrmuiorcDFSnnio1aeK7GgBAhBDOdfXXv0pffy2ddZbvSgAAEUM419Vtt7nlccf5rQMAEDmEc11YK82bJw0cKDVs6LsaAEDEEM518eSTbnnKKX7rAABEEuFcF7NmueUVV/itAwAQSYRzXdx/v1u2b++3DgBAJBHOtbVtm1sOGOC3DgBAZBHOtTVxolv+6Ed+6wAARBbhXFuJ+82nnuq3DgBAZBHOtfWf/0iDBklt2/quBAAQUYRzbWzZ4ia76NvXdyUAgAgjnGvjmWfc8uij/dYBAIg0wrk2Lr/cLc84w28dAIBII5zTZW1yvWVLf3UAACKPcE7X/PluOWaM3zoAAJFHOKfruefc8vTT/dYBAIg8wjld99zjlgMH+q0DABB5hHO6tmxxy/x8v3UAACKPcE7HJ5+45Qkn+K0DABALhHM6pk1zy5//3G8dAIBYIJzTMXWq1Ly5NGKE70oAADFAOKdjwgTpiCOkPE4XACB4pM2ebNggLV9eeRASAAACRDjvycyZbnnBBX7rAADEBuG8J3/+s1see6zfOgAAsUE478nTT7tlq1Z+6wAAxAbhXJPSUrccOdJvHQCAWCGca/LRR2550kl+6wAAxArhXJOpU93ysMP81gEAiBXCuSYffijts4/Us6fvSgAAMUI41+SDD6ShQ31XAQCIGcK5OuvXSwsXSoce6rsSAEDMEM7V+fBDt+R+MwAgywjn6kybJhkjDR7suxIAQMwQztX58EOpb183GxUAAFlEOFdn2jTpkEN8VwEAiCHCOZWVK6U1a6RBg3xXAgCIIcI5lTfecMvDD/dbBwAglgjnVG6+2S1pOQMAPCjwXUAodewoNWjgvgAAyDJazrsrK5PmzZNOPtl3JQCAmCKcd7dggbR1K5e0AQDeEM67e+klt2TwEQCAJ4GFszEm3xjTJKj3D8zTT7vlAQf4rQMAEFtBtpxHSJpvjFlY/rV/gMfKnMJC6Zhj6AwGAPAmyHA2ko611vYs//o8wGNlxo4d0scfM00kAMCroO8532yM+cgY85IxplHAx6q/2bOlkhKG7QQAeBVkOH8m6TJJgyU1kXTm7i8wxow2xkw3xkxfu3ZtgKWkaeZMtzz4YL91AABiLbBwttZ+Za3dYq21kt6V1CbFax611g6x1g5p165dUKWkb+pUqX17qUsX35UAAGIsyN7ax1XY7C3plaCOlTHTprn7zcb4rgQAEGNBDt/ZxRhzr6Q1kp6x1n4R4LHqb8MG6fPPpfPO810JACDmAgtna+3jQb13IKZNc8t+/fzWAQCIPUYIS/j0U7ccNsxvHQCA2COcE2bPdp3B9t7bdyUAgJgjnBNmz5YGDvRdBQAAhLMkN/DIp59KBx3kuxIAAAhnSdL8+VJRkdS/v+9KAAAgnCW58bQlwhkAEAqEsyTNnSvl5Un758bEWQCAaCOcJRfOPXtKjRv7rgQAAMJZkjRjhjRggO8qAACQRDi7YTu/+II5nAEAoUE4L17slj17+q0DAIByhPOiRW7ZrZvfOgAAKEc4z5vnpojs1ct3JQAASCKc3TSRXbpIhYW+KwEAQBLh7B6jYppIAECIxDucrXX3nHv39l0JAADfiHc4r1olbdtGT20AQKjEO5znznVLLmsDAEIk3uH8ySduSTgDAEIk3uE8d67Urp37AgAgJAhnWs0AgJCJbzhbSzgDAEIpvuG8fLm0ZQvhDAAInfiGMz21AQAhFd9w/vRTtyScAQAhE99wXrRI2msvqU0b35UAAFBJfMN58WKpe3ffVQAAUEV8w3nZMmm//XxXAQBAFfEMZ2tdOHft6rsSAACqiGc4r10r7dhByxkAEErxDOfFi92yRw+/dQAAkEI8w3nePLdkqkgAQAjFM5znz5cKCmg5AwBCKZ7hvHCh6wxWUOC7EgAAqohvOHNJGwAQUvELZ2ulBQukXr18VwIAQErxC+f1691sVIwOBgAIqfiF84oVbtm5s986AACoRvzCedkyt2QAEgBASMUvnJcscctu3fzWAQBANeIXzgsWSK1aMVUkACC04hfOiceojPFdCQAAKcU3nAEACKl4hXNxsesQRjgDAEIsXuG8apVUWkpPbQBAqMUrnFeudMt99vFbBwAANYhXOC9f7padOvmtAwCAGsQrnJcudcuuXX1WAQBAjeIVzqtWSYWFUosWvisBAKBa8Qvnjh15xhkAEGrxDGcAAEKMcAYAIGQIZwAAQiY+4bx1q7RlC+EMAAi9+ITzqlVuSTgDAEIufuHM6GAAgJCLXzjTcgYAhBzhDABAyMQrnBs2lFq39l0JAAA1ilc4d+jA6GAAgNCLVzhzSRsAkAPiE84rVxLOAICcEJ9wXrWKx6gAADkhHuG8c6f09de0nAEAOSEe4bx6tVsSzgCAHBB4OBtj9jLGnBH0cWrEM84AgBySjZbzDZLOy8Jxqkc4AwBySKDhbIwZKmlBkMdIC+EMAMghgYWzMaZAUh/VEM7GmNHGmOnGmOlr164NqhQXznl5Urt2wR0DAIAMCbLlfKakv9f0Amvto9baIdbaIe2CDM6VK6X27aX8/OCOAQBAhhQE+N7nSBopqa2kbsaYW6y1vwrweNXjGWcAQA4JLJyttSdLkjHmeEkXegtmyYXzvvt6OzwAALURdIewXpJOkNTXGDM8yGPViHG1AQA5JMjL2rLWLpD0kyCPsUclJdLatYQzACBnRH+EsK++kqwlnAEAOSP64cwzzgCAHEM4AwAQMtEP55Ur3ZJHqQAAOSL64bxqlWSMG4QEAIAcEI9wbttWatDAdyUAAKQlmuFcViaNGSN9+inPOAMAck6gzzl7s3ixdOed0qJFhDMAIOdEs+Xcs6ebhWq//QhnAEDOiWY4S1LLltL27dLq1YQzACCnRDecmzaVli6VSksJZwBATol2OC9c6NZ5xhkAkEOiG87NmkkLFrh1Ws4AgBwS3XBu2tQ9UiURzgCAnBLtcE4gnAEAOSS64bzXXm7ZsaPUuLHfWgAAqIXohnOvXm7Zu7ffOgAAqKXohnOPHm7ZqpXfOgAAqKXohvMJJ0jf/rZ0662+KwEAoFaiOba2JLVpI02a5LsKAABqLbotZwAAchThDABAyBDOAACEDOEMAEDIEM4AAIQM4QwAQMgQzgAAhAzhDABAyBDOAACEDOEMAEDIEM4AAIQM4QwAQMgQzgAAhAzhDABAyBDOAACEDOEMAEDIEM4AAIQM4QwAQMgYa63vGiRJxpi1kpZl+G3bSlqX4feEw7kNDuc2OJzbYHF+a28/a2273XeGJpyDYIyZbq0d4ruOKOLcBodzGxzObbA4v5nDZW0AAEKGcAYAIGSiHs6P+i4gwji3weHcBodzGyzOb4ZE+p4zAAC5KOotZwAAcg7hjEqMMfnGmCa+64giY0xPY0yB7zqiKnF++QwjCiJ3WdsY01jSDZJWShou6XJr7Ta/VeUOY8yRksZL2lW+6yRJZ6vC+ZRUqt3Ocap9nHfHGLO/pBMl3Sipu6SdSuP8pbsv7uc5xfkdoKqf4aXi/NaKMeYwSXdLai3pbUk/k/Rz8bnNiiiG8zWSFlhr/2GM+ZWktdbaB3zXlSuMMd+WtMpaO698u8r5lNQwnX2cd8cY08hau8sYs1TSQEkXqo7nNNW+uJ/nFOd3oCp8hstfw+e4lowxl0saJ3eF9UNJL0mayec2O6J4WftwSQvL12dJ6umxllx1szHmI2PMS5K+parnM9U55rxXw1q7a7dd6Z4/znMaUpxfqcJn2BjTSJzfunjCWltcfn4XSRokPrdZE8Vw3kvJy1mbJTX1WEsu+kzSZZIGS2oi6WRVPZ+pzjHnPX3pnj/Oc93s/hk+U5zfWkv80WOM6Sxpk6Tm4nObNVEM522SWpSvNxLjvNaKtfYra+0W6+53vCspX1XPZ6pzzHlPX7rnj/NcByk+w23E+a2T8j48V5R/8bnNoiiG81RJ+5ev95L0jr9Sco8x5rgKm70l3aKq5zPVOea8py/d88d5roMUn+FXxPmtNWNMA0lXSbrDWrtDfG6zKoqPdTws6XfGmEJJBdbaN30XlGO6GGPulbRG0jOSpmm382mM+SCdfd5+g5AxxuRLOk2uBTdS7rzeVJdzynmuKsX5bWyMOVHln2Fr7RfGmCr/X+D87tGjkr4jabQxJk/SvyV153ObHZHrrQ0AyDxjjLEERtYQzgAAhEwU7zkDAJDTCGcAAEKGcAYAIGQIZwAAQoZwBrBHxpgW5Y/TAMgC/rEBqFH5ZCjLlBzhCUDAojgICRBbxphhctMl/kPSF5LaSyq21t5Y1/e01r5jjNmUoRIBpIFwBiLEWjvFGPOFpOeste9LkjFmqOeyANQS4QxEmDHmNEnLjDGTJT0pNxzjYEmjrLXvG2POK3/pfpLyrbW/Nsa0lJvVqZWkQ621R5e/5lRjzP+Wr59RzVSNADKAcAai6VxjzHckDbHWnmKMsZIWWWvPNsaMlBvn+GJJJ1hrz5EkY8wkY8w7ki6QdL+19iNjzKAK7znLWvsnY8xUSQfLTWYAIAB0CAOi6XFr7a8l/aR828rdg5akKZKaSRogaWeFn/lIbuagQyV9LUnW2pkVvr+6fPmV3LR/AAJCOAMRZq1dYow5tXwzcaVsmKSnJX0u6ZAKL28h6T1JiyUdVcPbmkzXCaAyJr4AIsQYc7Ckv8pN77dUUmO5e8cHlb9kplwoP2qttcaY28pf85GkUmvtC8aY/SU9Imm+pDmSJkuaJOkSa+2LxpiJkiZba2/L3m8GxAvhDMRA+b3kH1prl3ouBUAauKwNAEDIEM5AxBljekvqKOlY37UASA+XtYGIM8YYyz90IKcQzgAAhAyXtQEACBnCGQCAkCGcAQAIGcIZAICQIZwBAAiZ/wfZY3qtkLRsSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.arange(len(W_1_hist)), W_1_hist, 'r',\n",
    "         np.arange(len(W_2_hist)), W_2_hist, 'b',\n",
    "         np.arange(len(W_3_hist)), W_3_hist, 'k')\n",
    "plt.legend(['W_1','W_2', 'W_3'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Weights norm')\n",
    "#plt.ylim(-.0, .06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 0.4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, 'r', np.arange(len(val_loss)), val_loss, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.ylim(-.0,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to go back from normalization in order to have a better feeling of the results and avoid division for very low numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_range_norm(x_norm, x_min, x_max):\n",
    "    \"\"\" Inverse of normalization in range [0, 1] \"\"\"\n",
    "    x = x_min +  x_norm * (x_max - x_min)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of accuracy\n",
    "\n",
    "We need to define some metrics to evaluate the goodness of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pe(pred, true):\n",
    "    \"\"\" Calculate percent error\"\"\"\n",
    "    return abs((pred - true) / true) * 100\n",
    "\n",
    "def r2(pred, true):\n",
    "    \"\"\" Calculate the coefficient of determination, i.e. R^2\"\"\"\n",
    "    # https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "    ss_tot = np.sum((true - np.mean(true, axis=0)) ** 2, axis=0)  \n",
    "    ss_res = np.sum((true - pred) ** 2, axis=0)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "def raae(pred, true):\n",
    "    \"\"\" Calculate the relative averaged absolute error\"\"\"\n",
    "    sigma = np.std(true, axis=0)\n",
    "    num = np.sum(abs(true - pred), axis=0)\n",
    "    return num / (true.shape[0] * sigma)\n",
    "\n",
    "def rmae(pred, true):\n",
    "    \"\"\" Calculate the relative maximum absolute error\"\"\"\n",
    "    sigma = np.std(true, axis=0)\n",
    "    num = np.max(abs(pred - true), axis=0)\n",
    "    return num / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the network's weights saved. This is required since, if the training is stopped by the early stopping class, the actual weights are associated to the overfitted network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(input_folder + '/weights_NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 13.30%\n",
      "\n",
      "The coefficient of determination is: 0.934\n",
      "\n",
      "The relative averaged absolute error is: 0.176\n",
      "\n",
      "The relative maximum absolute error is: 1.009\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "model.eval()\n",
    "train_x = Variable(torch.from_numpy(X_train.astype('float32')))\n",
    "train_y = Variable(torch.from_numpy(Y_train.astype('float32')))\n",
    "\n",
    "train_pred = model(train_x)\n",
    "\n",
    "train_y = reverse_range_norm(train_y.detach().numpy(), y_min, y_max)\n",
    "train_pred = reverse_range_norm(train_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "err_perc = pe(train_pred, train_y)\n",
    "r_2 = r2(train_pred, train_y)\n",
    "raae_score = raae(train_pred, train_y)\n",
    "rmae_score = rmae(train_pred, train_y)\n",
    "\n",
    "if flag_list:\n",
    "    for i in range(train_y.shape[0]):\n",
    "        print(\"True : {:+010.2f}, {:+013.2f}  |  Prediction : {:+010.2f}, {:+013.2f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "              format(*train_y[i, :], *train_pred[i, :], *err_perc[i, :]))\n",
    "    \n",
    "print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 17.74%\n",
      "\n",
      "The coefficient of determination is: 0.849\n",
      "\n",
      "The relative averaged absolute error is: 0.263\n",
      "\n",
      "The relative maximum absolute error is: 1.437\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "model.eval()\n",
    "val_x = Variable(torch.from_numpy(X_val.astype('float32')))\n",
    "val_y = Variable(torch.from_numpy(Y_val.astype('float32')))\n",
    "\n",
    "val_pred = model(val_x)\n",
    "\n",
    "val_y = reverse_range_norm(val_y.detach().numpy(), y_min, y_max)\n",
    "val_pred = reverse_range_norm(val_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "err_perc = pe(val_pred, val_y)\n",
    "r_2 = r2(val_pred, val_y)\n",
    "raae_score = raae(val_pred, val_y)\n",
    "rmae_score = rmae(val_pred, val_y)\n",
    "    \n",
    "if flag_list:\n",
    "    for i in range(val_y.shape[0]):\n",
    "        print(\"True : {:+010.2f}, {:+013.2f}  |  Prediction : {:+010.2f}, {:+013.2f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "              format(*val_y[i, :], *val_pred[i, :], *err_perc[i, :]))\n",
    "print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The mean of the percentage error is: 23.85%\n",
      "\n",
      "The coefficient of determination is: 0.734\n",
      "\n",
      "The relative averaged absolute error is: 0.342\n",
      "\n",
      "The relative maximum absolute error is: 2.068\n"
     ]
    }
   ],
   "source": [
    "flag_list = False\n",
    "testing = True\n",
    "\n",
    "if testing:\n",
    "    model.eval()\n",
    "    test_x = Variable(torch.from_numpy(X_test.astype('float32')))\n",
    "    test_y = Variable(torch.from_numpy(Y_test.astype('float32')))\n",
    "\n",
    "    test_pred = model(test_x)\n",
    "    \n",
    "    test_y = reverse_range_norm(test_y.detach().numpy(), y_min, y_max)\n",
    "    test_pred = reverse_range_norm(test_pred.detach().numpy(), y_min, y_max)\n",
    "\n",
    "    err_perc = pe(test_pred, test_y)\n",
    "    r_2 = r2(test_pred, test_y)\n",
    "    raae_score = raae(test_pred, test_y)\n",
    "    rmae_score = rmae(test_pred, test_y)\n",
    "\n",
    "    if flag_list:\n",
    "        for i in range(test_y.shape[0]):\n",
    "            print(\"True : {:.4f}, {:.4f}  |  Prediction : {:.4f}, {:.4f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "                  format(*test_y[i, :], *test_pred[i, :], *err_perc[i, :]))\n",
    "    print(\"\\nThe mean of the percentage error is: {:.2f}%\".format(np.mean(err_perc)))\n",
    "    print(\"\\nThe coefficient of determination is: {:.3f}\".format(np.mean(r_2)))\n",
    "    print(\"\\nThe relative averaged absolute error is: {:.3f}\".format(np.mean(raae_score)))\n",
    "    print(\"\\nThe relative maximum absolute error is: {:.3f}\".format(np.mean(rmae_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 Parameter containing:\n",
      "tensor([[-1.0844e-01, -4.9057e-01, -4.6628e-01,  6.9210e-01, -2.3332e-01,\n",
      "          4.2882e-01, -4.1004e-01, -1.8674e-01],\n",
      "        [-1.4891e-01, -3.9336e-01, -9.4734e-02,  5.2422e-02,  1.7628e-01,\n",
      "          2.3287e-01, -8.9333e-03,  3.8331e-01],\n",
      "        [-1.8496e-01,  2.4584e-02,  2.1688e-02,  3.7966e-02,  5.9815e-02,\n",
      "         -5.8522e-01, -5.1259e-01,  1.1237e-01],\n",
      "        [ 1.1920e-03, -5.3885e-03,  5.2022e-03, -2.2404e-03,  2.4978e-02,\n",
      "         -2.0544e-02,  2.3576e-02, -6.3236e-01],\n",
      "        [ 7.2022e-02, -5.1027e-02, -2.5911e-02,  5.7195e-01, -5.0238e-02,\n",
      "          6.3549e-03,  6.9255e-03, -9.6424e-03],\n",
      "        [-8.7163e-02,  1.0295e-01, -1.1280e-01, -2.4809e-01,  6.9280e-02,\n",
      "          3.9446e-01, -5.7644e-01, -1.3134e-01],\n",
      "        [-5.4915e-01,  1.8107e-01,  3.1066e-01, -4.0122e-02,  3.0949e-02,\n",
      "         -7.8460e-02,  1.0198e-02, -1.1879e-01],\n",
      "        [ 1.3140e-02,  5.5321e-03,  1.2650e-02,  7.3111e-01, -3.6382e-02,\n",
      "         -2.8615e-02, -1.0237e-02, -2.4332e-02],\n",
      "        [ 2.7867e-02,  4.3903e-02,  5.0828e-03,  3.1196e-01, -1.5185e-01,\n",
      "          1.3220e-01,  9.0371e-02,  2.8901e-01],\n",
      "        [-2.3288e-01, -1.5710e-01, -2.6328e-01, -2.2125e-01, -1.4394e-01,\n",
      "          5.7349e-01, -3.9494e-01,  5.7261e-01],\n",
      "        [-1.2140e-02, -5.0333e-04,  8.5482e-03, -6.9222e-01,  7.8181e-02,\n",
      "          1.0093e-02, -9.4183e-03,  1.4308e-02],\n",
      "        [ 3.0464e-01,  5.2449e-01, -7.4451e-02, -1.5828e-01, -1.2839e-01,\n",
      "         -2.1625e-01,  7.8812e-02,  4.2669e-02],\n",
      "        [ 2.6085e-01, -1.8634e-01, -1.3076e-01,  1.7110e-01, -7.7619e-01,\n",
      "         -2.2135e-01, -1.7079e-03,  2.9317e-01],\n",
      "        [-1.0480e-02, -3.2760e-02, -4.5917e-03, -1.9695e-03, -8.1825e-03,\n",
      "         -2.2551e-02, -9.3070e-03, -7.8803e-01],\n",
      "        [ 5.1877e-01, -5.1291e-01,  2.3426e-01,  1.1469e-01, -1.7116e-01,\n",
      "          5.6418e-02, -1.8112e-01, -2.4057e-03],\n",
      "        [ 1.5666e-02, -1.8349e-03,  2.1933e-02, -8.1866e-03,  8.8873e-03,\n",
      "         -3.8231e-02, -1.7846e-02,  6.1761e-01],\n",
      "        [-2.4814e-01,  1.0727e-01,  3.6851e-02,  1.5521e-01,  7.0094e-02,\n",
      "         -4.3083e-02,  3.8171e-01, -2.8680e-03],\n",
      "        [ 1.7535e-02, -1.1089e-02, -1.2469e-02,  7.2036e-01,  2.1808e-02,\n",
      "         -4.3915e-03, -3.3395e-02, -2.1680e-02],\n",
      "        [ 4.4016e-02, -9.6052e-03,  4.1551e-03,  8.8652e-01, -1.1954e-02,\n",
      "         -1.9361e-02, -2.2673e-02, -6.6510e-04],\n",
      "        [-1.7163e-01, -2.4287e-02, -7.4443e-01,  2.3707e-02, -4.6964e-02,\n",
      "          1.4123e-01, -1.6278e-01, -3.3166e-03],\n",
      "        [ 1.2856e-01, -1.3241e-01,  1.7343e-01, -4.4937e-01,  5.0444e-01,\n",
      "         -3.5566e-01,  3.8307e-02, -1.9195e-01],\n",
      "        [-5.3341e-03,  1.4626e-02, -2.4461e-02,  4.6804e-02,  1.8781e-02,\n",
      "         -4.5524e-02,  6.2318e-02, -1.7648e+00],\n",
      "        [-7.3661e-02, -1.4683e-01, -5.2092e-01,  3.4310e-02, -4.1813e-03,\n",
      "          1.7898e-01,  2.5288e-01,  3.1542e-01],\n",
      "        [-7.0800e-01, -1.8801e-01,  3.3306e-01,  2.6298e-02, -1.8875e-01,\n",
      "         -2.2749e-03,  4.2050e-02, -1.2990e-01],\n",
      "        [ 6.7781e-03,  2.1143e-03,  1.5522e-02, -1.0262e-02,  2.0047e-02,\n",
      "         -4.0300e-02, -1.5258e-02,  7.2497e-01],\n",
      "        [ 3.5062e-04, -7.4771e-03,  1.3226e-02,  8.7713e-01,  4.4254e-02,\n",
      "          1.3752e-02, -3.5190e-03, -2.0116e-01],\n",
      "        [ 2.7619e-01, -1.4189e-01,  2.5657e-01,  2.7069e-01, -4.8554e-01,\n",
      "          1.8988e-01, -5.6985e-02,  1.7531e-01],\n",
      "        [ 3.5023e-02,  8.9313e-02, -1.2721e-01,  1.3398e-01,  1.8522e-01,\n",
      "         -1.7377e-01, -3.2339e-01,  2.6656e-01],\n",
      "        [-1.7246e-02, -9.1714e-03,  7.8256e-03, -5.8404e-02,  5.5100e-03,\n",
      "          1.3484e-03, -2.5005e-03, -1.3705e+00],\n",
      "        [ 2.4685e-01, -2.0182e-02, -1.3812e-01,  1.3410e-01, -8.5874e-01,\n",
      "         -1.6122e-03,  3.3256e-01, -1.5802e-01],\n",
      "        [-5.7846e-01, -3.5988e-01,  3.7208e-02, -7.2953e-01, -6.3505e-02,\n",
      "         -1.1108e-03, -1.5298e-01, -2.0105e-01],\n",
      "        [-7.1637e-02,  4.2091e-02,  2.5132e-02, -5.2019e-01,  8.1150e-02,\n",
      "          1.0033e-03,  2.4940e-02,  6.1611e-01],\n",
      "        [ 4.2385e-05,  2.6666e-05,  3.4214e-05, -1.9625e-05, -3.6069e-05,\n",
      "         -2.6943e-05,  2.3318e-05, -2.1698e-05],\n",
      "        [-5.6526e-02,  3.4733e-02,  6.7802e-03, -1.8144e-01, -1.2912e-02,\n",
      "         -2.5097e-02,  2.9677e-02,  9.8578e-01],\n",
      "        [-7.7475e-06,  5.9301e-06, -1.8844e-06,  1.0501e-05,  2.2919e-05,\n",
      "          1.1559e-05,  4.7148e-06, -1.2189e-05],\n",
      "        [-1.4510e-01,  9.7375e-02,  1.7491e-01, -1.0011e-01, -1.0493e+00,\n",
      "          8.9976e-02,  4.2141e-01,  6.3543e-02],\n",
      "        [ 2.3625e-01,  2.0107e-01,  1.5814e-02, -8.8970e-01,  1.8992e-02,\n",
      "         -1.4729e-02,  2.4035e-02, -4.9901e-01],\n",
      "        [ 1.3358e-02, -1.0489e-02, -4.4734e-03, -1.0966e-02, -1.2194e-02,\n",
      "         -1.4186e-02,  7.7175e-03, -6.7784e-01],\n",
      "        [ 4.4299e-02, -1.2579e-01, -8.2714e-02,  1.1046e+00, -9.9921e-02,\n",
      "         -1.1693e-02, -1.0037e-01, -9.6645e-02],\n",
      "        [-3.9751e-01,  3.4425e-01, -1.5810e-01, -9.1850e-01,  6.2328e-01,\n",
      "          1.5106e-04, -3.1949e-01,  9.9856e-02],\n",
      "        [-1.0390e-01, -1.1792e-03, -1.9893e-01, -7.0709e-02,  5.8567e-02,\n",
      "          3.2614e-01,  2.0391e-01, -8.0878e-02],\n",
      "        [-3.3268e-01,  1.2249e-01, -2.7809e-02,  1.1352e-01,  3.5619e-01,\n",
      "         -1.6145e-01,  1.4630e-01, -5.9135e-03],\n",
      "        [ 2.2202e-01,  3.6265e-01, -1.2008e-01, -2.1518e-01, -2.5427e-01,\n",
      "         -5.1642e-02, -1.2405e-01,  3.8521e-01],\n",
      "        [ 1.1141e-01, -2.4619e-01, -8.2764e-01, -2.4855e-01,  4.2161e-01,\n",
      "          3.6868e-01, -3.6010e-02, -3.3215e-01],\n",
      "        [ 5.1238e-02, -2.5498e-02, -3.8415e-02,  9.7346e-01, -4.9586e-02,\n",
      "         -4.3489e-02, -4.6563e-02,  5.6522e-02],\n",
      "        [ 1.0806e-01, -4.6786e-02,  1.1394e-02, -8.7110e-01,  1.6838e-02,\n",
      "         -2.7872e-02, -4.1876e-02, -4.1077e-01],\n",
      "        [ 4.8652e-03,  4.2394e-03,  8.6441e-03, -7.9896e-03,  8.6882e-03,\n",
      "         -2.8322e-02, -4.9992e-03,  8.3308e-01],\n",
      "        [-2.9328e-02, -2.9241e-02,  7.1670e-03, -1.9381e+00,  4.9141e-02,\n",
      "          2.9948e-02,  2.4500e-02,  1.0333e-01],\n",
      "        [ 1.6407e-02,  3.6978e-02,  4.1935e-02,  6.0886e-01,  4.6733e-02,\n",
      "         -6.5504e-02,  4.5734e-02, -4.7528e-02],\n",
      "        [ 7.3859e-02,  4.3628e-03,  1.3842e-02, -6.3777e-01,  1.2565e-02,\n",
      "         -1.0053e-02, -7.6583e-03, -1.3661e-02],\n",
      "        [ 3.4180e-02,  1.2163e-03,  3.8031e-02, -1.2720e+00,  4.3714e-04,\n",
      "         -1.4935e-01, -4.1294e-02, -9.7191e-02],\n",
      "        [ 4.8948e-01, -2.5928e-01, -1.0032e-01,  2.0506e-01,  1.0472e-02,\n",
      "         -1.2401e-01,  1.9718e-01,  1.8678e-01],\n",
      "        [-1.2365e-01,  5.6998e-02,  2.2850e-02,  6.7332e-01,  2.1596e-02,\n",
      "          7.4934e-02, -4.2624e-04, -1.4507e-02],\n",
      "        [-5.5676e-02, -4.0404e-02,  7.1155e-02,  2.6401e-01,  1.0655e-01,\n",
      "         -5.5856e-03,  1.0473e-02, -1.3387e-01],\n",
      "        [ 8.2770e-02,  1.1406e-01, -3.0749e-01,  1.2977e-01,  2.2413e-02,\n",
      "          3.0460e-01, -2.0829e-01, -2.1471e-02],\n",
      "        [ 2.4317e-01, -2.6288e-02,  9.2921e-02, -1.0394e+00,  2.3064e-01,\n",
      "         -5.1541e-02,  2.9187e-01, -3.3564e-01],\n",
      "        [-2.2131e-01,  8.3428e-02,  2.0617e-02, -2.7853e-01,  5.1261e-01,\n",
      "         -2.2280e-01, -1.4498e-01,  2.5384e-01],\n",
      "        [ 2.5073e-02, -1.4383e-03,  3.0116e-02,  6.9193e-02,  7.6774e-02,\n",
      "          6.4241e-03, -2.0786e-02,  6.8348e-01],\n",
      "        [-4.3917e-01, -6.0662e-01, -2.1586e-01,  1.6908e-02,  2.9456e-01,\n",
      "          3.1355e-02,  2.7211e-02,  3.6422e-02],\n",
      "        [-1.6864e-01,  2.4105e-01,  1.1299e-01,  1.4254e-02,  1.2980e-01,\n",
      "         -1.9424e-01,  1.9773e-01, -4.3516e-01],\n",
      "        [ 4.6528e-02,  1.5668e-03,  2.5372e-03, -6.7346e-01, -3.3359e-02,\n",
      "          1.2720e-02, -1.2200e-02, -4.1914e-02],\n",
      "        [-5.9338e-02,  2.4411e-03, -2.7623e-03, -1.1176e-02,  1.0936e-02,\n",
      "         -9.7964e-03,  4.2503e-02,  1.0177e+00],\n",
      "        [ 2.7732e-05, -6.4002e-05, -2.5877e-05,  1.0272e-05, -7.6049e-06,\n",
      "          3.9102e-05, -2.6984e-05, -2.0533e-05],\n",
      "        [ 4.9562e-02,  1.0627e-02,  2.5869e-03, -1.5897e+00, -8.3313e-02,\n",
      "          2.2313e-02, -4.3083e-03,  5.4991e-02]], requires_grad=True)\n",
      "b_1 Parameter containing:\n",
      "tensor([-2.9830e-01, -2.3101e-01,  1.6092e-01,  4.0474e-01, -1.5960e-01,\n",
      "         6.2052e-02,  1.4837e-03, -3.8434e-01, -3.3192e-01, -1.6254e-01,\n",
      "         4.1213e-01, -4.0923e-01,  1.7230e-02,  5.5169e-01, -2.7098e-01,\n",
      "        -1.9397e-01, -2.4090e-01, -2.0561e-01, -2.8640e-01,  2.4910e-01,\n",
      "         4.4970e-02,  2.0080e-01, -1.1952e-01,  1.9962e-01, -2.3408e-01,\n",
      "        -5.5205e-01, -3.6108e-01, -1.2196e-01,  2.6996e-01,  9.7083e-02,\n",
      "         8.5317e-01, -1.8584e-01, -7.3052e-06, -5.5606e-01,  1.9652e-06,\n",
      "         1.0020e-03,  3.8815e-01,  4.5674e-01, -8.6975e-01, -6.8611e-02,\n",
      "        -1.4665e-01, -1.4835e-01, -3.4344e-01,  6.8277e-02, -7.3664e-01,\n",
      "         6.5048e-01, -2.6737e-01,  1.7589e-01, -4.2642e-01,  4.0296e-01,\n",
      "         6.3933e-01, -3.1673e-01, -2.5484e-01, -5.8375e-02, -1.0743e-01,\n",
      "         2.1586e-01, -1.3436e-01, -4.6901e-01,  1.1224e-01, -1.9496e-01,\n",
      "         4.6646e-01, -8.1822e-01, -3.6351e-05,  2.3458e-01],\n",
      "       requires_grad=True)\n",
      "W_2 Parameter containing:\n",
      "tensor([[-1.6548e-01, -6.6943e-03, -4.8915e-03,  ..., -2.8771e-02,\n",
      "         -6.9771e-05, -1.5098e-01],\n",
      "        [-3.8894e-03, -1.0169e-04, -1.8631e-01,  ...,  2.3320e-02,\n",
      "          1.1817e-04, -5.7920e-03],\n",
      "        [-2.0205e-01,  1.6208e-02,  6.2865e-04,  ..., -4.2851e-02,\n",
      "         -1.5836e-04, -2.9667e-06],\n",
      "        ...,\n",
      "        [-2.6094e-05,  5.4215e-02,  3.3715e-05,  ..., -6.0024e-02,\n",
      "          8.5578e-05,  7.4026e-02],\n",
      "        [-2.0628e-01,  5.4677e-03,  4.8458e-03,  ..., -1.0183e-02,\n",
      "          1.4416e-04, -5.7903e-04],\n",
      "        [-1.5925e-01,  3.8437e-04, -5.5289e-04,  ..., -2.9123e-03,\n",
      "         -8.7245e-06, -2.7485e-02]], requires_grad=True)\n",
      "b_2 Parameter containing:\n",
      "tensor([ 2.2950e-02, -2.6891e-02,  4.0276e-02,  4.8653e-02,  4.3052e-02,\n",
      "        -2.9746e-01, -2.7227e-01,  4.2470e-02,  3.6495e-02,  3.5921e-02,\n",
      "         5.1349e-02, -9.1147e-02,  3.7366e-02,  5.4128e-02,  3.2487e-02,\n",
      "         1.1712e-02,  7.1657e-02,  5.3352e-02,  5.0999e-02,  4.1305e-02,\n",
      "         5.3762e-02,  4.0014e-02,  5.1557e-02,  3.4891e-02,  4.6329e-02,\n",
      "        -2.6054e-06,  3.8427e-02,  4.3749e-02, -7.3502e-02,  5.3622e-02,\n",
      "         2.8461e-02, -3.9258e-01,  2.8655e-02,  3.4366e-02, -1.1554e-01,\n",
      "         6.7380e-02,  6.4993e-02,  1.5199e-02,  4.0237e-02,  7.9818e-02,\n",
      "         2.7146e-02,  5.6265e-02,  2.7220e-02,  2.0307e-02,  4.4149e-02,\n",
      "        -3.5721e-01, -4.5842e-03, -4.3727e-02, -3.3942e-01, -3.3610e-01,\n",
      "         5.1826e-02, -7.5576e-02,  1.5953e-02,  6.0863e-02, -2.3765e-05,\n",
      "         5.4804e-02,  1.8235e-02,  3.6759e-02,  6.5866e-02,  4.3525e-02,\n",
      "         3.5013e-02, -7.7510e-02,  2.7580e-02,  2.4215e-02],\n",
      "       requires_grad=True)\n",
      "W_4 Parameter containing:\n",
      "tensor([[ 1.1174e+00, -1.2804e+00,  7.1869e-01,  1.5016e-03,  1.5423e-01,\n",
      "         -3.4153e-02,  6.0086e-01,  4.5325e-01,  9.6072e-01,  1.0047e+00,\n",
      "         -2.5054e-01, -9.8434e-01, -1.7475e-01, -2.7004e-01,  6.2269e-01,\n",
      "         -5.4778e-01,  1.7729e-01, -2.4910e-01,  8.9739e-04,  6.2594e-01,\n",
      "         -1.2208e-04,  4.1719e-01, -1.9787e-01, -7.1724e-01, -8.0016e-03,\n",
      "          4.2922e-07,  4.9071e-03,  1.2287e-03,  1.6404e+00, -5.3671e-01,\n",
      "          6.8886e-01, -1.6415e-02,  7.7774e-01,  6.6143e-01, -4.6174e-01,\n",
      "         -2.9532e-01, -2.3051e-01,  2.1284e-01, -3.6043e-01,  8.3158e-04,\n",
      "         -1.2151e+00, -7.2907e-03, -9.5414e-01,  2.7816e-01,  4.5871e-01,\n",
      "         -1.3690e-01,  8.2323e-01,  1.0333e+00,  4.8539e-04, -2.8621e-04,\n",
      "          5.6992e-04, -2.2515e-01,  3.3342e-02, -3.6171e-02,  4.7728e-05,\n",
      "         -3.8187e-01,  7.8315e-01,  1.1392e+00,  1.2305e-05,  3.9273e-01,\n",
      "          3.0737e-01,  1.5044e+00,  6.8008e-01,  3.3979e-01],\n",
      "        [-8.6497e-02,  1.5491e-04, -2.1583e-03,  1.1183e+00,  1.0600e+00,\n",
      "         -1.0954e+00, -1.0768e+00,  7.8895e-01, -2.2402e-03, -3.2689e-03,\n",
      "          7.4318e-01, -3.3273e-02,  5.0663e-01,  2.8413e-01, -7.5255e-03,\n",
      "          2.9968e-04, -1.0261e+00,  1.1233e+00,  7.5640e-01, -4.4834e-04,\n",
      "          8.5600e-01,  4.9844e-01,  8.0127e-01, -2.7428e-01,  1.1274e+00,\n",
      "          2.9058e-05,  6.2457e-01,  1.3109e+00, -4.7677e-03,  5.5979e-01,\n",
      "         -2.8742e-02, -7.4169e-01, -5.1534e-03, -7.3787e-02, -9.1456e-01,\n",
      "         -8.1353e-01,  1.2102e+00,  9.3138e-01, -1.3036e+00, -1.3388e+00,\n",
      "          1.4447e-01,  8.2264e-01, -1.6829e-01, -1.8200e-03,  9.2742e-01,\n",
      "         -6.5874e-01, -1.7550e-02, -1.4267e-01, -9.5550e-01, -6.9321e-01,\n",
      "          7.4830e-01, -1.4162e-01,  1.1841e+00,  1.1724e+00, -7.5009e-05,\n",
      "          3.6151e-01,  1.2875e-01, -3.5416e-01,  1.3502e+00,  7.5137e-01,\n",
      "         -1.8742e-04, -2.8613e-03, -2.2592e-02, -2.4821e-02]],\n",
      "       requires_grad=True)\n",
      "b_4 Parameter containing:\n",
      "tensor([0.3689, 0.3456], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
