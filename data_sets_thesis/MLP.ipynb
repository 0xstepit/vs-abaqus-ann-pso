{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLQLStKwyzgs"
   },
   "source": [
    "# **METAMODELING WITH ARTIFICIAL NEURAL NETWORK**\n",
    "\n",
    "In this notebook, we will use the results of Abaqus analyses in order to build an Artificial Neural Network (ANN) of the Finite Element (FE) analysis solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HiM0koqnW-b"
   },
   "outputs": [],
   "source": [
    "# Install latest Tensorflow build\n",
    "#!pip install -q tf-nightly-2.0-preview\n",
    "from tensorflow import summary\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Orwif6dIvF4t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Bwj567NSvHv1",
    "outputId": "e91823c3-87c5-47fe-cca9-cd505741e7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrenn in /opt/anaconda3/lib/python3.7/site-packages (0.1)\r\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from pyrenn) (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Pyrenn\n",
    "! pip install pyrenn\n",
    "import pyrenn as prn\n",
    "\n",
    "# Matplotlib spec\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']}) # Palatino font\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jqtrSiFqdBTe",
    "outputId": "1cd691d5-fc75-4420-b6e5-d331a6ee9c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMkgB8U2dS1A"
   },
   "source": [
    "When this notebook has been generated the result of the previous line of code is: _'1.5.1+cu101'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZDSnO7jyzg8"
   },
   "source": [
    "We fix the seed in order to obtain reproducible results.\n",
    "\n",
    "__N.B.__ : Reproducible results are obtained every time the runtime is restarded and runned. If you run multiple time the same cell the results will not be reporducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SytMvTAE22lL"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed=seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxRgA_ioyzhA"
   },
   "source": [
    "## **Data preprocessing**\n",
    "\n",
    "We start by importing some information about the model used to generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "0FZ6R8aeyzhB",
    "outputId": "e6f59f12-68cb-4eb0-dd90-990424b0c8c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Radius</th>\n",
       "      <th>MaxCurvature</th>\n",
       "      <th>MeshSize</th>\n",
       "      <th>Plies</th>\n",
       "      <th>EffectivePlies</th>\n",
       "      <th>Symmetric</th>\n",
       "      <th>Balanced</th>\n",
       "      <th>AnglesFunction</th>\n",
       "      <th>LoadCase</th>\n",
       "      <th>Train</th>\n",
       "      <th>Val</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>705</td>\n",
       "      <td>300</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>harmlin</td>\n",
       "      <td>axial</td>\n",
       "      <td>81</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Height  Radius  MaxCurvature  MeshSize  Plies  EffectivePlies  \\\n",
       "Value     705     300      0.001575         5      8               2   \n",
       "\n",
       "       Symmetric  Balanced AnglesFunction LoadCase  Train  Val  Test  \n",
       "Value       True      True        harmlin    axial     81   27    27  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify parameter to choose the output folder to consider\n",
    "stacking_sequence = 'symmetric'\n",
    "data_set = 'large1.65'\n",
    "load_case = 'torsion'\n",
    "fiber_path = 'harmlin'\n",
    "\n",
    "# Check if notebook running in Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# Model info folder\n",
    "input_folder = './'\n",
    "\n",
    "info = pd.read_csv(input_folder + 'model_info.csv', sep=\";\")\n",
    "info.index = ['Value']\n",
    "eff_plies = int(info['EffectivePlies'].values)\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['Train', 'Val', 'Test']\n",
    "for set in sets:\n",
    "    if set not in info.keys():\n",
    "        sets.remove(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIBWzRwLyzhG"
   },
   "source": [
    "At this point we have to import the data set containing the input and output of the FE analysis. The data is stored in a dataframe in which the upper part is associated to the training set and the lower part to the test set. The precise number of upper row belonging to the train set is indicated in the info above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "niD2Q60oyzhG",
    "outputId": "6046f3fb-ed80-4099-aa94-7eb2dd87de49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude1</th>\n",
       "      <th>PhaseShift1</th>\n",
       "      <th>Omega1</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Amplitude2</th>\n",
       "      <th>PhaseShift2</th>\n",
       "      <th>Omega2</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Buckling</th>\n",
       "      <th>Stiffness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.08</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.39</td>\n",
       "      <td>38.52</td>\n",
       "      <td>14.60</td>\n",
       "      <td>76.85</td>\n",
       "      <td>1.27</td>\n",
       "      <td>13.29</td>\n",
       "      <td>235.051</td>\n",
       "      <td>281175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.39</td>\n",
       "      <td>85.65</td>\n",
       "      <td>1.72</td>\n",
       "      <td>7.86</td>\n",
       "      <td>46.78</td>\n",
       "      <td>69.20</td>\n",
       "      <td>0.66</td>\n",
       "      <td>27.25</td>\n",
       "      <td>239.225</td>\n",
       "      <td>388831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.29</td>\n",
       "      <td>45.21</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8.63</td>\n",
       "      <td>87.30</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.56</td>\n",
       "      <td>190.754</td>\n",
       "      <td>465806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.79</td>\n",
       "      <td>62.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.35</td>\n",
       "      <td>20.34</td>\n",
       "      <td>16.69</td>\n",
       "      <td>1.16</td>\n",
       "      <td>19.37</td>\n",
       "      <td>182.235</td>\n",
       "      <td>448824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.92</td>\n",
       "      <td>28.67</td>\n",
       "      <td>0.07</td>\n",
       "      <td>26.28</td>\n",
       "      <td>57.23</td>\n",
       "      <td>84.57</td>\n",
       "      <td>0.48</td>\n",
       "      <td>20.18</td>\n",
       "      <td>248.555</td>\n",
       "      <td>365717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>73.83</td>\n",
       "      <td>10.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>15.26</td>\n",
       "      <td>19.30</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1.81</td>\n",
       "      <td>7.24</td>\n",
       "      <td>225.731</td>\n",
       "      <td>421545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>33.20</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.23</td>\n",
       "      <td>36.61</td>\n",
       "      <td>94.59</td>\n",
       "      <td>58.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>26.93</td>\n",
       "      <td>225.708</td>\n",
       "      <td>194835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>33.40</td>\n",
       "      <td>23.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>7.64</td>\n",
       "      <td>42.03</td>\n",
       "      <td>76.21</td>\n",
       "      <td>0.85</td>\n",
       "      <td>17.11</td>\n",
       "      <td>178.640</td>\n",
       "      <td>452326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>23.20</td>\n",
       "      <td>25.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>32.97</td>\n",
       "      <td>84.96</td>\n",
       "      <td>32.20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9.29</td>\n",
       "      <td>243.416</td>\n",
       "      <td>322473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>11.04</td>\n",
       "      <td>71.92</td>\n",
       "      <td>0.43</td>\n",
       "      <td>34.01</td>\n",
       "      <td>65.61</td>\n",
       "      <td>71.46</td>\n",
       "      <td>1.15</td>\n",
       "      <td>37.22</td>\n",
       "      <td>242.012</td>\n",
       "      <td>157904.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Amplitude1  PhaseShift1  Omega1  Beta1  Amplitude2  PhaseShift2  Omega2  \\\n",
       "0         65.08        20.60    0.39  38.52       14.60        76.85    1.27   \n",
       "1         24.39        85.65    1.72   7.86       46.78        69.20    0.66   \n",
       "2         19.29        45.21    1.29   8.63       87.30        76.00    0.53   \n",
       "3         65.79        62.30    0.25   2.35       20.34        16.69    1.16   \n",
       "4         13.92        28.67    0.07  26.28       57.23        84.57    0.48   \n",
       "..          ...          ...     ...    ...         ...          ...     ...   \n",
       "130       73.83        10.10    0.30  15.26       19.30        48.92    1.81   \n",
       "131       33.20         6.00    1.23  36.61       94.59        58.90    0.23   \n",
       "132       33.40        23.11    0.11   7.64       42.03        76.21    0.85   \n",
       "133       23.20        25.50    0.49  32.97       84.96        32.20    0.18   \n",
       "134       11.04        71.92    0.43  34.01       65.61        71.46    1.15   \n",
       "\n",
       "     Beta2  Buckling  Stiffness  \n",
       "0    13.29   235.051   281175.0  \n",
       "1    27.25   239.225   388831.0  \n",
       "2     2.56   190.754   465806.0  \n",
       "3    19.37   182.235   448824.0  \n",
       "4    20.18   248.555   365717.0  \n",
       "..     ...       ...        ...  \n",
       "130   7.24   225.731   421545.0  \n",
       "131  26.93   225.708   194835.0  \n",
       "132  17.11   178.640   452326.0  \n",
       "133   9.29   243.416   322473.0  \n",
       "134  37.22   242.012   157904.0  \n",
       "\n",
       "[135 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data folder\n",
    "data_folder = './'\n",
    "\n",
    "data_orig = pd.read_csv(data_folder + '/data.csv', sep=';')\n",
    "data = data_orig.drop(columns='Stiffness')\n",
    "data_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9U4RPrnyzhP"
   },
   "source": [
    "The most important step to perform before training our model is the normalization of the variables. Different strategies are possible at this end, among which 2 are the most used:\n",
    "\n",
    "* Range normalization: converts all the values to the range $[0, 1]$\n",
    "\n",
    "* Standard score normalization: forces the variables to have $0$ mean and $1$ standard deviation\n",
    "\n",
    "We will try both to see the effect on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZVjkHziyzhQ"
   },
   "outputs": [],
   "source": [
    "def range_norm(x):\n",
    "    \"\"\"normalization in range [0, 1]\"\"\"\n",
    "    x_min = np.min(x, axis=0)\n",
    "    x_max = np.max(x, axis=0)\n",
    "    x_norm = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    return x_norm\n",
    "\n",
    "def std_norm(x):\n",
    "    \"\"\"normalization with zero mean and unitary standard deviation\"\"\"\n",
    "    m = np.mean(x, axis=0)\n",
    "    s = np.std(x, axis=0)\n",
    "    x_norm = (x - m) / s\n",
    "    \n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "ORbqtnh-yzhR",
    "outputId": "d18315cf-cbc9-40f5-f987-26f6485bba5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude1</th>\n",
       "      <th>PhaseShift1</th>\n",
       "      <th>Omega1</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Amplitude2</th>\n",
       "      <th>PhaseShift2</th>\n",
       "      <th>Omega2</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Buckling</th>\n",
       "      <th>Stiffness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>135.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503193</td>\n",
       "      <td>0.501657</td>\n",
       "      <td>0.342395</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0.500112</td>\n",
       "      <td>0.502561</td>\n",
       "      <td>0.366686</td>\n",
       "      <td>0.502270</td>\n",
       "      <td>0.495412</td>\n",
       "      <td>0.510102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.293347</td>\n",
       "      <td>0.293968</td>\n",
       "      <td>0.256763</td>\n",
       "      <td>0.293973</td>\n",
       "      <td>0.295692</td>\n",
       "      <td>0.293923</td>\n",
       "      <td>0.262649</td>\n",
       "      <td>0.292343</td>\n",
       "      <td>0.216112</td>\n",
       "      <td>0.258279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.257680</td>\n",
       "      <td>0.250901</td>\n",
       "      <td>0.147208</td>\n",
       "      <td>0.248591</td>\n",
       "      <td>0.243842</td>\n",
       "      <td>0.251185</td>\n",
       "      <td>0.154450</td>\n",
       "      <td>0.256666</td>\n",
       "      <td>0.369555</td>\n",
       "      <td>0.285821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.503545</td>\n",
       "      <td>0.498198</td>\n",
       "      <td>0.274112</td>\n",
       "      <td>0.500563</td>\n",
       "      <td>0.501305</td>\n",
       "      <td>0.511515</td>\n",
       "      <td>0.324607</td>\n",
       "      <td>0.502129</td>\n",
       "      <td>0.505081</td>\n",
       "      <td>0.523821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750760</td>\n",
       "      <td>0.753660</td>\n",
       "      <td>0.497462</td>\n",
       "      <td>0.745436</td>\n",
       "      <td>0.748326</td>\n",
       "      <td>0.757507</td>\n",
       "      <td>0.528796</td>\n",
       "      <td>0.750168</td>\n",
       "      <td>0.633420</td>\n",
       "      <td>0.712391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amplitude1  PhaseShift1      Omega1       Beta1  Amplitude2  \\\n",
       "count  135.000000   135.000000  135.000000  135.000000  135.000000   \n",
       "mean     0.503193     0.501657    0.342395    0.497512    0.500112   \n",
       "std      0.293347     0.293968    0.256763    0.293973    0.295692   \n",
       "min      0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.257680     0.250901    0.147208    0.248591    0.243842   \n",
       "50%      0.503545     0.498198    0.274112    0.500563    0.501305   \n",
       "75%      0.750760     0.753660    0.497462    0.745436    0.748326   \n",
       "max      1.000000     1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       PhaseShift2      Omega2       Beta2    Buckling   Stiffness  \n",
       "count   135.000000  135.000000  135.000000  135.000000  135.000000  \n",
       "mean      0.502561    0.366686    0.502270    0.495412    0.510102  \n",
       "std       0.293923    0.262649    0.292343    0.216112    0.258279  \n",
       "min       0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%       0.251185    0.154450    0.256666    0.369555    0.285821  \n",
       "50%       0.511515    0.324607    0.502129    0.505081    0.523821  \n",
       "75%       0.757507    0.528796    0.750168    0.633420    0.712391  \n",
       "max       1.000000    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm = range_norm(data_orig)\n",
    "data_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL3mX63XyzhT"
   },
   "source": [
    "Now we can split the data into training and test set. The two sets have been generate independently during the DOE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "uy149NwNyzhT",
    "outputId": "224f9a13-bed4-4c54-f05e-a4eee762b812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             - - - - - - - \n",
      "            |Problem info:|\n",
      "             - - - - - - -  \n",
      "\n",
      "X_train : (81, 8)  |  Y_train : (81, 2)\n",
      "X_val   : (27, 8)  |  Y_val   : (27, 2)\n",
      "X_test  : (27, 8)  |  Y_test  : (27, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_norm.drop(['Buckling', 'Stiffness'], axis=1).values\n",
    "Y = data_norm[['Buckling','Stiffness']].values\n",
    "\n",
    "# Train set\n",
    "train_smp = int(info['Train'].values)\n",
    "X_train = X[:train_smp, :]\n",
    "Y_train = Y[:train_smp]\n",
    "last = np.copy(train_smp)\n",
    "\n",
    "# Validation set\n",
    "if 'Val' in sets:\n",
    "    val_smp = int(info['Val'].values)\n",
    "    X_val = X[last:last+val_smp, :]\n",
    "    Y_val = Y[last:last+val_smp]\n",
    "    last += val_smp\n",
    "\n",
    "# Test set\n",
    "if 'Test' in sets:\n",
    "    test_smp = int(info['Test'].values)\n",
    "    X_test = X[last:last+test_smp, :]\n",
    "    Y_test = Y[last:last+test_smp]\n",
    "\n",
    "print('             - - - - - - - ')\n",
    "print('            |Problem info:|')\n",
    "print('             - - - - - - -  \\n')\n",
    "print(\"X_train : {}  |  Y_train : {}\".format(X_train.shape, Y_train.shape))\n",
    "print(\"X_val   : {}  |  Y_val   : {}\".format(X_val.shape, Y_val.shape))\n",
    "print(\"X_test  : {}  |  Y_test  : {} \\n\".format(X_test.shape, Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ngXLJgnf3zs"
   },
   "source": [
    "At this point we can generate the iterable data sets for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3haXjC9f3UA"
   },
   "outputs": [],
   "source": [
    "# just for the training set\n",
    "batch = 32\n",
    "\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "    \n",
    "train_dataset = Data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch, shuffle=True, num_workers=0, worker_init_fn=_init_fn)\n",
    "\n",
    "val_dataset = Data.TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(Y_val).float())\n",
    "val_loader = Data.DataLoader(dataset=val_dataset, batch_size=X_val.shape[0], shuffle=True, num_workers=0,  worker_init_fn=_init_fn)\n",
    "\n",
    "test_dataset = Data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(Y_test).float())\n",
    "test_loader = Data.DataLoader(dataset=test_dataset, batch_size=X_test.shape[0], shuffle=True, num_workers=0, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7O7AIIoyzhU"
   },
   "source": [
    "## **Neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vyx3cytYfs7E"
   },
   "source": [
    "First define network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aMiO2DUfrIK"
   },
   "outputs": [],
   "source": [
    "class MLPNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(MLPNN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(H, D_in)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Second hidden layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Third hidden layer\n",
    "        self.W_3 = Parameter(init.xavier_normal_(torch.Tensor(H, H)))\n",
    "        self.b_3 = Parameter(init.constant_(torch.Tensor(H), 0))\n",
    "        # Output layer\n",
    "        self.W_4 = Parameter(init.xavier_normal_(torch.Tensor(D_out, H)))\n",
    "        self.b_4 = Parameter(init.constant_(torch.Tensor(D_out), 0))\n",
    "        \n",
    "        # define activation function in constructor\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        self.activation_3 = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation_1(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.activation_2(x)\n",
    "        x = F.linear(x, self.W_3, self.b_3)\n",
    "        x = self.activation_3(x)\n",
    "        pred = F.linear(x, self.W_4, self.b_4)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda function to generate mini-batches\n",
    "get_batch = lambda i, size: range(i * size, (i + 1) * size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ibkHXkYKmXs6",
    "outputId": "3a2927df-9996-474a-eee5-3ec0d6f12263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 | Train loss : 0.04220 | Val loss : 0.09654\n",
      "Iteration : 100 | Train loss : 0.00837 | Val loss : 0.02631\n",
      "Iteration : 200 | Train loss : 0.00355 | Val loss : 0.02465\n",
      "Iteration : 300 | Train loss : 0.00747 | Val loss : 0.02560\n",
      "Iteration : 400 | Train loss : 0.00439 | Val loss : 0.02708\n",
      "Iteration : 500 | Train loss : 0.00572 | Val loss : 0.02715\n",
      "Iteration : 600 | Train loss : 0.00840 | Val loss : 0.02824\n",
      "Iteration : 700 | Train loss : 0.00595 | Val loss : 0.03039\n",
      "Iteration : 800 | Train loss : 0.00625 | Val loss : 0.02844\n",
      "Iteration : 900 | Train loss : 0.00595 | Val loss : 0.03126\n",
      "Iteration : 1000 | Train loss : 0.00408 | Val loss : 0.03026\n",
      "Iteration : 1100 | Train loss : 0.00573 | Val loss : 0.03032\n",
      "Iteration : 1200 | Train loss : 0.00438 | Val loss : 0.02997\n",
      "Iteration : 1300 | Train loss : 0.00395 | Val loss : 0.02993\n",
      "Iteration : 1400 | Train loss : 0.00604 | Val loss : 0.02894\n",
      "Iteration : 1500 | Train loss : 0.00429 | Val loss : 0.03053\n",
      "Iteration : 1600 | Train loss : 0.00445 | Val loss : 0.02918\n",
      "Iteration : 1700 | Train loss : 0.00486 | Val loss : 0.02937\n",
      "Iteration : 1800 | Train loss : 0.00396 | Val loss : 0.02858\n",
      "Iteration : 1900 | Train loss : 0.00476 | Val loss : 0.02918\n",
      "Iteration : 2000 | Train loss : 0.00390 | Val loss : 0.02885\n",
      "Iteration : 2100 | Train loss : 0.00462 | Val loss : 0.02845\n",
      "Iteration : 2200 | Train loss : 0.00391 | Val loss : 0.02812\n",
      "Iteration : 2300 | Train loss : 0.00578 | Val loss : 0.02777\n",
      "Iteration : 2400 | Train loss : 0.00386 | Val loss : 0.02974\n",
      "Iteration : 2500 | Train loss : 0.00323 | Val loss : 0.02783\n",
      "Iteration : 2600 | Train loss : 0.00486 | Val loss : 0.02751\n",
      "Iteration : 2700 | Train loss : 0.00409 | Val loss : 0.02990\n",
      "Iteration : 2800 | Train loss : 0.00428 | Val loss : 0.02900\n",
      "Iteration : 2900 | Train loss : 0.00350 | Val loss : 0.02887\n",
      "Iteration : 3000 | Train loss : 0.00482 | Val loss : 0.02864\n",
      "Iteration : 3100 | Train loss : 0.00497 | Val loss : 0.02948\n",
      "Iteration : 3200 | Train loss : 0.00418 | Val loss : 0.02826\n",
      "Iteration : 3300 | Train loss : 0.00394 | Val loss : 0.02885\n",
      "Iteration : 3400 | Train loss : 0.00389 | Val loss : 0.02874\n",
      "Iteration : 3500 | Train loss : 0.00476 | Val loss : 0.02945\n",
      "Iteration : 3600 | Train loss : 0.00371 | Val loss : 0.02834\n",
      "Iteration : 3700 | Train loss : 0.00413 | Val loss : 0.02922\n",
      "Iteration : 3800 | Train loss : 0.00511 | Val loss : 0.02796\n",
      "Iteration : 3900 | Train loss : 0.00565 | Val loss : 0.02818\n",
      "Iteration : 4000 | Train loss : 0.00303 | Val loss : 0.02848\n",
      "Iteration : 4100 | Train loss : 0.00426 | Val loss : 0.02843\n",
      "Iteration : 4200 | Train loss : 0.00285 | Val loss : 0.02753\n",
      "Iteration : 4300 | Train loss : 0.00506 | Val loss : 0.02801\n",
      "Iteration : 4400 | Train loss : 0.00321 | Val loss : 0.02772\n",
      "Iteration : 4500 | Train loss : 0.00413 | Val loss : 0.02808\n",
      "Iteration : 4600 | Train loss : 0.00463 | Val loss : 0.02816\n",
      "Iteration : 4700 | Train loss : 0.00485 | Val loss : 0.02809\n",
      "Iteration : 4800 | Train loss : 0.00429 | Val loss : 0.02788\n",
      "Iteration : 4900 | Train loss : 0.00678 | Val loss : 0.02798\n",
      "Iteration : 5000 | Train loss : 0.00336 | Val loss : 0.02849\n",
      "Iteration : 5100 | Train loss : 0.00477 | Val loss : 0.02778\n",
      "Iteration : 5200 | Train loss : 0.00465 | Val loss : 0.02835\n",
      "Iteration : 5300 | Train loss : 0.00414 | Val loss : 0.02812\n",
      "Iteration : 5400 | Train loss : 0.00486 | Val loss : 0.02768\n",
      "Iteration : 5500 | Train loss : 0.00324 | Val loss : 0.02793\n",
      "Iteration : 5600 | Train loss : 0.00482 | Val loss : 0.02763\n",
      "Iteration : 5700 | Train loss : 0.00605 | Val loss : 0.02877\n",
      "Iteration : 5800 | Train loss : 0.00362 | Val loss : 0.02879\n",
      "Iteration : 5900 | Train loss : 0.00366 | Val loss : 0.02746\n",
      "Iteration : 6000 | Train loss : 0.00394 | Val loss : 0.02856\n",
      "Iteration : 6100 | Train loss : 0.00518 | Val loss : 0.02731\n",
      "Iteration : 6200 | Train loss : 0.00332 | Val loss : 0.02710\n",
      "Iteration : 6300 | Train loss : 0.00423 | Val loss : 0.02680\n",
      "Iteration : 6400 | Train loss : 0.00702 | Val loss : 0.02841\n",
      "Iteration : 6500 | Train loss : 0.00600 | Val loss : 0.02758\n",
      "Iteration : 6600 | Train loss : 0.00398 | Val loss : 0.02836\n",
      "Iteration : 6700 | Train loss : 0.00448 | Val loss : 0.02686\n",
      "Iteration : 6800 | Train loss : 0.00457 | Val loss : 0.02867\n",
      "Iteration : 6900 | Train loss : 0.00367 | Val loss : 0.02870\n",
      "Iteration : 7000 | Train loss : 0.00423 | Val loss : 0.02815\n",
      "Iteration : 7100 | Train loss : 0.00481 | Val loss : 0.02727\n",
      "Iteration : 7200 | Train loss : 0.00485 | Val loss : 0.02724\n",
      "Iteration : 7300 | Train loss : 0.00616 | Val loss : 0.02829\n",
      "Iteration : 7400 | Train loss : 0.00433 | Val loss : 0.02691\n",
      "Iteration : 7500 | Train loss : 0.00540 | Val loss : 0.02743\n",
      "Iteration : 7600 | Train loss : 0.00520 | Val loss : 0.02768\n",
      "Iteration : 7700 | Train loss : 0.00401 | Val loss : 0.02809\n",
      "Iteration : 7800 | Train loss : 0.00384 | Val loss : 0.02682\n",
      "Iteration : 7900 | Train loss : 0.00692 | Val loss : 0.02786\n",
      "Iteration : 8000 | Train loss : 0.00467 | Val loss : 0.02823\n",
      "Iteration : 8100 | Train loss : 0.00325 | Val loss : 0.02795\n",
      "Iteration : 8200 | Train loss : 0.00814 | Val loss : 0.02731\n",
      "Iteration : 8300 | Train loss : 0.00295 | Val loss : 0.02820\n",
      "Iteration : 8400 | Train loss : 0.00234 | Val loss : 0.02800\n",
      "Iteration : 8500 | Train loss : 0.00493 | Val loss : 0.02845\n",
      "Iteration : 8600 | Train loss : 0.00445 | Val loss : 0.02812\n",
      "Iteration : 8700 | Train loss : 0.00513 | Val loss : 0.02828\n",
      "Iteration : 8800 | Train loss : 0.00505 | Val loss : 0.02773\n",
      "Iteration : 8900 | Train loss : 0.00378 | Val loss : 0.02767\n",
      "Iteration : 9000 | Train loss : 0.00464 | Val loss : 0.02877\n",
      "Iteration : 9100 | Train loss : 0.00392 | Val loss : 0.02863\n",
      "Iteration : 9200 | Train loss : 0.00458 | Val loss : 0.02751\n",
      "Iteration : 9300 | Train loss : 0.00290 | Val loss : 0.02815\n",
      "Iteration : 9400 | Train loss : 0.00688 | Val loss : 0.02749\n",
      "Iteration : 9500 | Train loss : 0.00349 | Val loss : 0.02792\n",
      "Iteration : 9600 | Train loss : 0.00578 | Val loss : 0.02786\n",
      "Iteration : 9700 | Train loss : 0.00560 | Val loss : 0.02816\n",
      "Iteration : 9800 | Train loss : 0.00546 | Val loss : 0.02798\n",
      "Iteration : 9900 | Train loss : 0.00502 | Val loss : 0.02769\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[1]\n",
    "n_y = Y_train.shape[1]\n",
    "D_in, H, D_out = n_x, 4, n_y\n",
    "\n",
    "modelMLP = MLPNN(D_in, H, D_out)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-2\n",
    "\n",
    "# Flag True if you are in the network optimization process\n",
    "is_optimizing = True\n",
    "\n",
    "# If weight_NN exists and we are not in optimization mode just load\n",
    "# network weights and evaluate the model.\n",
    "if(os.path.isfile('net_weights/weights_NN') and is_optimizing==False):\n",
    "    modelMLP.load_state_dict(torch.load('net_weights/weights_NN'))\n",
    "    print(modelMLP.eval())\n",
    "else:\n",
    "    criterion = nn.MSELoss(reduction='mean') \n",
    "    optimizer = optim.Adam(modelMLP.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    \n",
    "    # Initialize loss lists.\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    test_loss = []\n",
    "    losses = []\n",
    "    curr_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        modelMLP.train()\n",
    "        for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            y_pred = modelMLP(batch_x)\n",
    "            \n",
    "            \n",
    "            batch_loss = criterion(y_pred, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += batch_loss\n",
    "            \n",
    "        train_loss.append(batch_loss.item() / step)\n",
    "\n",
    "        modelMLP.eval()\n",
    "        \n",
    "        for _, (val_x, val_y) in enumerate(val_loader):\n",
    "            val_pred = modelMLP(val_x)\n",
    "            loss_val = criterion(val_pred, val_y)\n",
    "            \n",
    "        val_loss.append(loss_val.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Iteration : {} | Train loss : {:.5f} | Val loss : {:.5f}\".format(epoch, train_loss[epoch], val_loss[epoch]))\n",
    "\n",
    "    torch.save(modelMLP.state_dict(), 'weights_NN')\n",
    "\n",
    "tb.add_graph(modelMLP, batch_x)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 0.2)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(np.arange(epochs), train_loss, 'r', np.arange(epochs), val_loss, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')\n",
    "plt.ylim(-.0, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : 0.6952, 0.6050  |  Prediction : 0.5091, 0.6306  |  Error : 0026.77%, -004.22%\n",
      "True : 0.7150, 0.6589  |  Prediction : 0.5231, 0.6586  |  Error : 0026.84%, 0000.05%\n",
      "True : 0.0608, 0.8413  |  Prediction : 0.1165, 0.8364  |  Error : -091.68%, 0000.58%\n",
      "True : 0.3848, 0.7040  |  Prediction : 0.2339, 0.8264  |  Error : 0039.22%, -017.39%\n",
      "True : 0.1405, 0.8262  |  Prediction : 0.3894, 0.7837  |  Error : -177.14%, 0005.14%\n",
      "True : 0.4991, 0.2266  |  Prediction : 0.5337, 0.2596  |  Error : -006.94%, -014.59%\n",
      "True : 0.4369, 0.6837  |  Prediction : 0.5098, 0.5826  |  Error : -016.67%, 0014.78%\n",
      "True : 0.2431, 0.6512  |  Prediction : 0.2136, 0.7444  |  Error : 0012.12%, -014.30%\n",
      "True : 0.6432, 0.2436  |  Prediction : 0.5322, 0.2034  |  Error : 0017.26%, 0016.51%\n",
      "True : 0.4795, 0.2499  |  Prediction : 0.5651, 0.2846  |  Error : -017.84%, -013.90%\n",
      "True : 0.5915, 0.2097  |  Prediction : 0.5999, 0.2321  |  Error : -001.42%, -010.68%\n",
      "True : 0.6205, 0.7150  |  Prediction : 0.2957, 0.8303  |  Error : 0052.34%, -016.12%\n",
      "True : 0.4993, 0.7546  |  Prediction : 0.2503, 0.7221  |  Error : 0049.86%, 0004.30%\n",
      "True : 0.6711, 0.3590  |  Prediction : 0.6632, 0.4093  |  Error : 0001.17%, -014.02%\n",
      "True : 0.0295, 0.9467  |  Prediction : 0.1174, 0.9146  |  Error : -297.92%, 0003.39%\n",
      "True : 0.6233, 0.1405  |  Prediction : 0.5481, 0.1785  |  Error : 0012.07%, -027.01%\n",
      "True : 0.6588, 0.4069  |  Prediction : 0.4990, 0.2552  |  Error : 0024.26%, 0037.29%\n",
      "True : 0.6456, 0.4225  |  Prediction : 0.5721, 0.4583  |  Error : 0011.38%, -008.45%\n",
      "True : 0.3138, 0.8853  |  Prediction : 0.3456, 0.8428  |  Error : -010.12%, 0004.80%\n",
      "True : 0.7180, 0.2829  |  Prediction : 0.5310, 0.2196  |  Error : 0026.05%, 0022.37%\n",
      "True : 0.6204, 0.6272  |  Prediction : 0.4479, 0.5962  |  Error : 0027.80%, 0004.95%\n",
      "True : 0.6340, 0.5238  |  Prediction : 0.6135, 0.4850  |  Error : 0003.23%, 0007.41%\n",
      "True : 0.5091, 0.1811  |  Prediction : 0.5370, 0.1664  |  Error : -005.48%, 0008.11%\n",
      "True : 0.8394, 0.3306  |  Prediction : 0.6269, 0.3571  |  Error : 0025.32%, -008.01%\n",
      "True : 0.4299, 0.0875  |  Prediction : 0.4753, 0.0822  |  Error : -010.57%, 0006.08%\n",
      "True : 0.7094, 0.5861  |  Prediction : 0.5727, 0.4788  |  Error : 0019.27%, 0018.30%\n",
      "True : 0.1592, 0.8399  |  Prediction : 0.5503, 0.7598  |  Error : -245.61%, 0009.54%\n"
     ]
    }
   ],
   "source": [
    "modelMLP.eval()\n",
    "for _, (test_x, test_y) in enumerate(test_loader):\n",
    "    test_pred = modelMLP(test_x)\n",
    "    for i in range(test_y.shape[0]):\n",
    "        err_perc = (test_y.numpy() - test_pred.detach().numpy()) / test_y.numpy() * 100\n",
    "        print(\"True : {:.4f}, {:.4f}  |  Prediction : {:.4f}, {:.4f}  |  Error : {:07.2f}%, {:07.2f}%\".\n",
    "              format(*test_y[i, :].numpy(), *test_pred[i, :].detach().numpy(), *err_perc[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "vxcyjr6IeO3z",
    "outputId": "a373c975-c00c-4223-8db6-7592c3dcf9f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 258), started 4:47:33 ago. (Use '!kill 258' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Cyx2vNti_z4"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=30, delta=1e-3, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        if self.val_loss_min > val_loss:\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
